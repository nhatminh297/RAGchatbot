{"docstore/metadata": {"d7c28a5e-1ee5-4157-ab97-a7d10fe7c67d": {"doc_hash": "65a5e39505467dc45e309b0ed88262ca9105245aa28727cee137bbf7289458e0"}, "64d6aa37-a002-4df6-82e3-ee5910394882": {"doc_hash": "28a6d7ede2e7d1bcd42ed68127203a971db829af565c4bba16069b433a10593c"}, "854a6d16-58de-4d7a-aee7-7635248d27aa": {"doc_hash": "a7ac0bf5edd712d9dfbf27bf89ea56b4c957b61be31c0bde0a799b83640ff2a4"}, "1029a61c-2640-4a49-a92e-99e9c556d49c": {"doc_hash": "a051a5255fa8e09fd348d535b09a7d8924cd06fc434657489a5483326cdc97cd"}, "95ed2d51-3fda-4ea4-9944-06c76222cd84": {"doc_hash": "2edae7f2c7b4eff2e3af43a0be0a20c99b005366c0600293ff72b64da22c4ede"}, "a96193f0-62f8-4cea-9912-c1ca68bbde35": {"doc_hash": "ed607e271a55c1740f6e210b1fd63d480dad127bbf55611d3657469f35c0f231"}, "e4c628c7-ca19-4dc1-9998-cfe454609fc6": {"doc_hash": "8f657c9f9c759dc3633d09770e0baed92fcf370906cd8e5c7a42d62057141c7c"}, "2d476f61-3318-4457-94da-6e2b9bd0b99f": {"doc_hash": "b67afda9c9d49137e40416b034543b3a6c3c447963cf0270135de11647090645"}, "0f92f5a6-5161-4e35-8312-d0db9bedd44b": {"doc_hash": "ad34664ebb585a98175b3d0bb6820a1a99840ee60645f16fb4bd8eee75a66996"}, "005d8e3c-6310-4efe-9c8b-7340b5c694b2": {"doc_hash": "6a9495f2abbcca1331af9a88f3fbfbecabbdd14f5f8dc693fc4e7443d28bade9"}, "632c852f-4d75-4dbc-891d-e0a716ad5eae": {"doc_hash": "6535b734a01de0350abbd37593cbc6c7bdcdaaf8b85fcc4546f55bd910644b05"}, "150f5b22-5a80-4207-8350-1eaddb100c9a": {"doc_hash": "2d430c282e10e887995642dcc2375c1144e530f566a9283d876b7f0395f1b6e1"}, "8dec473f-31c5-49d3-839f-ce5f7c07fe11": {"doc_hash": "23d0328596c5f33c29530d4be033f9881a33d13ae9c5195caec45ada988511cc"}, "3fabe1d8-2051-4c24-b1b7-f23733a37157": {"doc_hash": "519035b1744c6543befbf1f3c5e68498cae603cd56d6917fe29c8df210cc0f1b"}, "7f516e90-cd79-44a8-b3d8-dd19c38e8b76": {"doc_hash": "abdb4002b24af90f5abf3daec5b57fa75a90d4fe33b52d214ae47a9f0f1a2036"}, "d9cd26f9-6644-480b-bc99-2555f5ffd85b": {"doc_hash": "983c357f10de5f1b5c4f5acb869f4660fc12649530af47381ad107f1ca744389"}, "ef8df65c-8dd0-443a-8043-bf67c5145bd8": {"doc_hash": "55f4e62d022cd366628f687d70f7b64cf0b537e4cdf7a0c25f249ae54366c84e"}, "5805841a-ec3b-48c2-87f1-5b1ce684f33f": {"doc_hash": "1443a1aeeb80c7a1a7200972d91dbc1a4abfbb595f722ad48f7947e4f46be8b0"}, "39c1f1f1-472c-4b16-a991-cc1c3bb8d6aa": {"doc_hash": "9ce7d5ae83c114c03d9179890795b8e7ca726d9b0a59708ff32f2c1a64eaaf41"}, "3c0e1de1-74fc-4a2d-b9c3-d487f376b61a": {"doc_hash": "6ab2842a83e7ab507ece6a70c63289faebac19a5de7e0cb1c3805446f04dba9e"}, "6c5e725e-cbfb-4cd8-aff8-cb4a63642430": {"doc_hash": "b668a32a39e330bd4731e739945d89f08b0185fed58f185e0f030d7ee387218d"}, "b297e4dd-a7ad-4ae3-83f2-42f7787d97f1": {"doc_hash": "4e3d3bcb31972ac7993e33448706f2be0f1d8310fe4d014fc525ea669bf439f1", "ref_doc_id": "d7c28a5e-1ee5-4157-ab97-a7d10fe7c67d"}, "44634e3a-ee59-4da6-99aa-a7b5d8462f57": {"doc_hash": "888425af6cba73daa7510e35b7c7615400962e929bf25ae836ef7184a0e6d522", "ref_doc_id": "64d6aa37-a002-4df6-82e3-ee5910394882"}, "761db376-1c8a-4c8b-892a-4797097865a0": {"doc_hash": "33025f42f451b55c9565ccda253fe98c5aeb1f579349c63c0ca70510cee27a3c", "ref_doc_id": "854a6d16-58de-4d7a-aee7-7635248d27aa"}, "f02a89e9-925d-4362-811b-b099132a1991": {"doc_hash": "082c8510d02cf90a9da82f1da2c9d475b8d4815ff7d3658e140e9b7ea56fd5b6", "ref_doc_id": "854a6d16-58de-4d7a-aee7-7635248d27aa"}, "a6a52e9a-a0f9-45e3-bbf0-8887defb6a27": {"doc_hash": "8eebeaf4ca036e8f1a984ba43bd9951e48804cc7c88925be61341e982da70079", "ref_doc_id": "1029a61c-2640-4a49-a92e-99e9c556d49c"}, "d9aa1cd6-4740-4415-83c1-f3bc4a2cdac9": {"doc_hash": "fcc17dd5f33facd475964d8eb9d7a0237f675de758e141cbc8ed7b74edca019f", "ref_doc_id": "95ed2d51-3fda-4ea4-9944-06c76222cd84"}, "008fcbbb-cbcb-421d-9957-52e91105e0e1": {"doc_hash": "40bc77c8641d4005bec45aa85af7bff20bf2236addecabd3ee337e47c96eb105", "ref_doc_id": "95ed2d51-3fda-4ea4-9944-06c76222cd84"}, "df3747a6-5577-4f22-ab6e-93249f9196c9": {"doc_hash": "45be1c9e67f82a8cbc59480dfd5911b8e03a03ba5db1710ba1a2ec9ac9f60bf7", "ref_doc_id": "a96193f0-62f8-4cea-9912-c1ca68bbde35"}, "4ff91f68-87ba-48ad-b71a-74c612831349": {"doc_hash": "a0e54f55366b26a9a2a49c9cc825be799b5d70170899516f79544fc58739da21", "ref_doc_id": "e4c628c7-ca19-4dc1-9998-cfe454609fc6"}, "0301cf35-5310-41fb-9e50-38a997bae8bd": {"doc_hash": "04ee1c82a4ad7a6980b58545f63b892b5c247e983d4ff9568a09b29465522e07", "ref_doc_id": "e4c628c7-ca19-4dc1-9998-cfe454609fc6"}, "f49cf013-1e95-4223-a037-b18e00217122": {"doc_hash": "70c3bf09e5fd4d430568b2ca1940d698eafa1642a66127adfcddf3b0b3a5cfbf", "ref_doc_id": "2d476f61-3318-4457-94da-6e2b9bd0b99f"}, "24fb5653-3fd9-4504-8102-422cf45cc53b": {"doc_hash": "98d8b30f4ef3a39bbc23bbcc05f12161e41080ec1b7c0a38456ea3fd306282b8", "ref_doc_id": "0f92f5a6-5161-4e35-8312-d0db9bedd44b"}, "f09c8d4a-3da8-406a-8937-968ab04e29cf": {"doc_hash": "4024b0af8ef26271101fef8a0f50eff8f888c7b10e1160a5b89a7662bdb8bde9", "ref_doc_id": "005d8e3c-6310-4efe-9c8b-7340b5c694b2"}, "f17c3f6f-9f76-4639-9360-3c02d3746a1f": {"doc_hash": "516406e6e43d105a921aaed2d99769ed7d737a4f8fb7bb952fc11c0f92785af8", "ref_doc_id": "005d8e3c-6310-4efe-9c8b-7340b5c694b2"}, "1b4601ea-4c43-44de-a582-9aa1bf3f44ee": {"doc_hash": "ba89653ea265575edf7a3a0a87d0fe4062c40895d39052df6d31c699d86d7c54", "ref_doc_id": "632c852f-4d75-4dbc-891d-e0a716ad5eae"}, "d0ffdba7-1d58-482b-ad4c-a04324cb0b21": {"doc_hash": "32c664dc11aad152f3b545cef285c8be7ba537dc533fa7458409e765d8628bcd", "ref_doc_id": "150f5b22-5a80-4207-8350-1eaddb100c9a"}, "1c83538c-ce07-4e6d-ac4b-d2b6b9ea1da4": {"doc_hash": "d40ad9daa5b6df14c0d81cb112c7b890b0794a3a742189bb83a5cf65f35ae9df", "ref_doc_id": "150f5b22-5a80-4207-8350-1eaddb100c9a"}, "a203c43d-323d-4b3b-905c-0dbb3b930e00": {"doc_hash": "199810334106660e9d02de51f0ca575b0eb133844916101cb8f10474c2cc212a", "ref_doc_id": "8dec473f-31c5-49d3-839f-ce5f7c07fe11"}, "3790c5d1-f39b-49cc-9670-b2ac1569cfa0": {"doc_hash": "748bb091b313df0dc352a8e88e410ae5162125c208894832d437c9566cac9035", "ref_doc_id": "8dec473f-31c5-49d3-839f-ce5f7c07fe11"}, "33d3bdec-e51b-4444-94a4-5357484ca7da": {"doc_hash": "4e8e1a2d1956c685bc7e6bfc4da925f13261b15e457a2186cecbf9ab08777704", "ref_doc_id": "3fabe1d8-2051-4c24-b1b7-f23733a37157"}, "67fba76f-7e3f-4009-80b8-34b46d8711d5": {"doc_hash": "24b5fe0794f8f47a7dd27bbec5dd37430e0de1677f9d9980f3586beb7ea5b176", "ref_doc_id": "3fabe1d8-2051-4c24-b1b7-f23733a37157"}, "3f35adba-2271-4efa-931b-92d446739686": {"doc_hash": "5a3d8ba7560625918f92e097a4375972da17d31459ec71c9d4f0f13e99840d1e", "ref_doc_id": "7f516e90-cd79-44a8-b3d8-dd19c38e8b76"}, "d0ac5b09-f2be-4444-83e9-b1576540ea7c": {"doc_hash": "1f34ba352433a3a1ff9ee0b54f20674d4a341d8587d99b5067031c37553609a8", "ref_doc_id": "d9cd26f9-6644-480b-bc99-2555f5ffd85b"}, "87d26acb-31f5-4a7f-b78e-630dd475f278": {"doc_hash": "6c5bfd1cb51f7aa95b9520a65e5261fdd2e625a3137c493fc4bf6bdda63cf7c7", "ref_doc_id": "ef8df65c-8dd0-443a-8043-bf67c5145bd8"}, "7d57d056-c6a8-4365-9efa-921d81fc177d": {"doc_hash": "0ad0d212a93a03fd63d45bb46646a0013f0fda295341b50685fa11cff35b4da5", "ref_doc_id": "5805841a-ec3b-48c2-87f1-5b1ce684f33f"}, "7ae0d0de-4ce5-4b2c-91fc-2b2a6351f9ea": {"doc_hash": "35f47f55f86767fcbbfd5383a099f2718e281ee7b373e7601bd347da4dd775e5", "ref_doc_id": "39c1f1f1-472c-4b16-a991-cc1c3bb8d6aa"}, "ac9c3b39-9e51-4e86-9919-7e2082d48ab7": {"doc_hash": "2901a4827997127e2c1cda84c06170c4dfbc60f8f67f737b54c7f39f93023f12", "ref_doc_id": "3c0e1de1-74fc-4a2d-b9c3-d487f376b61a"}, "1d2fa2a3-bad8-4734-8e50-7d39bc445ecc": {"doc_hash": "0ad20db2b06e69ec954330a1fa84627e0a334c90b3b05884e045b1d4f3999980", "ref_doc_id": "6c5e725e-cbfb-4cd8-aff8-cb4a63642430"}}, "docstore/data": {"b297e4dd-a7ad-4ae3-83f2-42f7787d97f1": {"__data__": {"id_": "b297e4dd-a7ad-4ae3-83f2-42f7787d97f1", "embedding": null, "metadata": {"page_label": "1", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7c28a5e-1ee5-4157-ab97-a7d10fe7c67d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "65a5e39505467dc45e309b0ed88262ca9105245aa28727cee137bbf7289458e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44634e3a-ee59-4da6-99aa-a7b5d8462f57", "node_type": "1", "metadata": {}, "hash": "888425af6cba73daa7510e35b7c7615400962e929bf25ae836ef7184a0e6d522", "class_name": "RelatedNodeInfo"}}, "hash": "4e3d3bcb31972ac7993e33448706f2be0f1d8310fe4d014fc525ea669bf439f1", "text": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem. Our framework adopts a two-stage pipeline.\nIn the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\nTypical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\nThe research of BIR is still in its primary stage, thus requiring more explanations of its current state.\nAccording to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). They all have achieved remarkable progress, but also have apparent limitations.\nBSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023", "start_char_idx": 0, "end_char_idx": 3397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44634e3a-ee59-4da6-99aa-a7b5d8462f57": {"__data__": {"id_": "44634e3a-ee59-4da6-99aa-a7b5d8462f57", "embedding": null, "metadata": {"page_label": "2", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64d6aa37-a002-4df6-82e3-ee5910394882", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "28a6d7ede2e7d1bcd42ed68127203a971db829af565c4bba16069b433a10593c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b297e4dd-a7ad-4ae3-83f2-42f7787d97f1", "node_type": "1", "metadata": {"page_label": "1", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "4e3d3bcb31972ac7993e33448706f2be0f1d8310fe4d014fc525ea669bf439f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "761db376-1c8a-4c8b-892a-4797097865a0", "node_type": "1", "metadata": {}, "hash": "33025f42f451b55c9565ccda253fe98c5aeb1f579349c63c0ca70510cee27a3c", "class_name": "RelatedNodeInfo"}}, "hash": "888425af6cba73daa7510e35b7c7615400962e929bf25ae836ef7184a0e6d522", "text": "(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\nFigure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). ( Zoom in for best view )\nrealistic details. Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. The second group ZIR is a newly emerged direction.\nRepresentative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\nWith a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. However, the problem setting of ZIR is not in accordance with BIR. Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. The third group is BFR, which focuses on human face restoration.\nState-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. Nevertheless, BFR is only a sub-domain of BIR. It usually\n2", "start_char_idx": 0, "end_char_idx": 1910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "761db376-1c8a-4c8b-892a-4797097865a0": {"__data__": {"id_": "761db376-1c8a-4c8b-892a-4797097865a0", "embedding": null, "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "854a6d16-58de-4d7a-aee7-7635248d27aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "a7ac0bf5edd712d9dfbf27bf89ea56b4c957b61be31c0bde0a799b83640ff2a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44634e3a-ee59-4da6-99aa-a7b5d8462f57", "node_type": "1", "metadata": {"page_label": "2", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "888425af6cba73daa7510e35b7c7615400962e929bf25ae836ef7184a0e6d522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f02a89e9-925d-4362-811b-b099132a1991", "node_type": "1", "metadata": {}, "hash": "082c8510d02cf90a9da82f1da2c9d475b8d4815ff7d3658e140e9b7ea56fd5b6", "class_name": "RelatedNodeInfo"}}, "hash": "33025f42f451b55c9565ccda253fe98c5aeb1f579349c63c0ca70510cee27a3c", "text": "assumes a fixed input size and restricted image space, thus cannot be applied to general images.\nAccording to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. Therefore,\nwe desire a new BIR method to overcome these limitations.\nIn this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. We\nalso make dedicated designs to realize these strategies. First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. This helps DiffBIR to handle diverse and extreme degradation\ncases. Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\nWithout this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\nEquipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). We can observe the\ndifferences of these methods in some aspects. For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\nExtensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n2 Related Work\nBlind Image Super-Resolution. Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.", "start_char_idx": 0, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f02a89e9-925d-4362-811b-b099132a1991": {"__data__": {"id_": "f02a89e9-925d-4362-811b-b099132a1991", "embedding": null, "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "854a6d16-58de-4d7a-aee7-7635248d27aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "a7ac0bf5edd712d9dfbf27bf89ea56b4c957b61be31c0bde0a799b83640ff2a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "761db376-1c8a-4c8b-892a-4797097865a0", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "33025f42f451b55c9565ccda253fe98c5aeb1f579349c63c0ca70510cee27a3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6a52e9a-a0f9-45e3-bbf0-8887defb6a27", "node_type": "1", "metadata": {}, "hash": "8eebeaf4ca036e8f1a984ba43bd9951e48804cc7c88925be61341e982da70079", "class_name": "RelatedNodeInfo"}}, "hash": "082c8510d02cf90a9da82f1da2c9d475b8d4815ff7d3658e140e9b7ea56fd5b6", "text": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n2 Related Work\nBlind Image Super-Resolution. Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. \u00d74/\u00d78), which is limited for BIR problem.\nZero-shot Image Restoration. ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\nDDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n3", "start_char_idx": 3626, "end_char_idx": 5192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6a52e9a-a0f9-45e3-bbf0-8887defb6a27": {"__data__": {"id_": "a6a52e9a-a0f9-45e3-bbf0-8887defb6a27", "embedding": null, "metadata": {"page_label": "4", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1029a61c-2640-4a49-a92e-99e9c556d49c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "a051a5255fa8e09fd348d535b09a7d8924cd06fc434657489a5483326cdc97cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f02a89e9-925d-4362-811b-b099132a1991", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "082c8510d02cf90a9da82f1da2c9d475b8d4815ff7d3658e140e9b7ea56fd5b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9aa1cd6-4740-4415-83c1-f3bc4a2cdac9", "node_type": "1", "metadata": {}, "hash": "fcc17dd5f33facd475964d8eb9d7a0237f675de758e141cbc8ed7b74edca019f", "class_name": "RelatedNodeInfo"}}, "hash": "8eebeaf4ca036e8f1a984ba43bd9951e48804cc7c88925be61341e982da70079", "text": "Meanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\nBlind Face Restoration. As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. Early attempts utilize geometric priors (e.g.\nfacial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\nState-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. The overall\nframework is illustrated in Figure 2. Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\nThen, the generative prior is leveraged for producing realistic restoration results (Section 3.2). In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\nIreg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n3.1 Pretraining for Degradation Removal\nDegradation Model. BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\nAmong all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4", "start_char_idx": 0, "end_char_idx": 4107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9aa1cd6-4740-4415-83c1-f3bc4a2cdac9": {"__data__": {"id_": "d9aa1cd6-4740-4415-83c1-f3bc4a2cdac9", "embedding": null, "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95ed2d51-3fda-4ea4-9944-06c76222cd84", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "2edae7f2c7b4eff2e3af43a0be0a20c99b005366c0600293ff72b64da22c4ede", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6a52e9a-a0f9-45e3-bbf0-8887defb6a27", "node_type": "1", "metadata": {"page_label": "4", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "8eebeaf4ca036e8f1a984ba43bd9951e48804cc7c88925be61341e982da70079", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "008fcbbb-cbcb-421d-9957-52e91105e0e1", "node_type": "1", "metadata": {}, "hash": "40bc77c8641d4005bec45aa85af7bff20bf2236addecabd3ee337e47c96eb105", "class_name": "RelatedNodeInfo"}}, "hash": "fcc17dd5f33facd475964d8eb9d7a0237f675de758e141cbc8ed7b74edca019f", "text": "twice. Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\nRestoration Module. To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). We modify SwinIR [ 36] as our restoration module. Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. We optimize the parameters of the restoration module by minimizing the L2pixel loss. The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. The diffusion and denoising processes are performed in the latent space. In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\nA network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\nLAControlNet. Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. In particular, the decoder receives the features from\nencoder and fuses them in different scales. Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.", "start_char_idx": 0, "end_char_idx": 4045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "008fcbbb-cbcb-421d-9957-52e91105e0e1": {"__data__": {"id_": "008fcbbb-cbcb-421d-9957-52e91105e0e1", "embedding": null, "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95ed2d51-3fda-4ea4-9944-06c76222cd84", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "2edae7f2c7b4eff2e3af43a0be0a20c99b005366c0600293ff72b64da22c4ede", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9aa1cd6-4740-4415-83c1-f3bc4a2cdac9", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "fcc17dd5f33facd475964d8eb9d7a0237f675de758e141cbc8ed7b74edca019f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df3747a6-5577-4f22-ab6e-93249f9196c9", "node_type": "1", "metadata": {}, "hash": "45be1c9e67f82a8cbc59480dfd5911b8e03a03ba5db1710ba1a2ec9ac9f60bf7", "class_name": "RelatedNodeInfo"}}, "hash": "40bc77c8641d4005bec45aa85af7bff20bf2236addecabd3ee337e47c96eb105", "text": "LAControlNet. Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. In particular, the decoder receives the features from\nencoder and fuses them in different scales. Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. The outputs of the\nparallel module are added to the original UNet decoder. Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. During finetuning, the parallel module and these\n5", "start_char_idx": 3273, "end_char_idx": 4649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df3747a6-5577-4f22-ab6e-93249f9196c9": {"__data__": {"id_": "df3747a6-5577-4f22-ab6e-93249f9196c9", "embedding": null, "metadata": {"page_label": "6", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a96193f0-62f8-4cea-9912-c1ca68bbde35", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "ed607e271a55c1740f6e210b1fd63d480dad127bbf55611d3657469f35c0f231", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "008fcbbb-cbcb-421d-9957-52e91105e0e1", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "40bc77c8641d4005bec45aa85af7bff20bf2236addecabd3ee337e47c96eb105", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ff91f68-87ba-48ad-b71a-74c612831349", "node_type": "1", "metadata": {}, "hash": "a0e54f55366b26a9a2a49c9cc825be799b5d70170899516f79544fc58739da21", "class_name": "RelatedNodeInfo"}}, "hash": "45be1c9e67f82a8cbc59480dfd5911b8e03a03ba5db1710ba1a2ec9ac9f60bf7", "text": "1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\nWe aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. (4)\nThe obtained result in this stage is denoted as Idiff. To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. In this\nwork, the diffusion and denoising processes are based on the latent space. Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\nAlgorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6", "start_char_idx": 0, "end_char_idx": 3433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ff91f68-87ba-48ad-b71a-74c612831349": {"__data__": {"id_": "4ff91f68-87ba-48ad-b71a-74c612831349", "embedding": null, "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4c628c7-ca19-4dc1-9998-cfe454609fc6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "8f657c9f9c759dc3633d09770e0baed92fcf370906cd8e5c7a42d62057141c7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df3747a6-5577-4f22-ab6e-93249f9196c9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "45be1c9e67f82a8cbc59480dfd5911b8e03a03ba5db1710ba1a2ec9ac9f60bf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0301cf35-5310-41fb-9e50-38a997bae8bd", "node_type": "1", "metadata": {}, "hash": "04ee1c82a4ad7a6980b58545f63b892b5c247e983d4ff9568a09b29465522e07", "class_name": "RelatedNodeInfo"}}, "hash": "a0e54f55366b26a9a2a49c9cc825be799b5d70170899516f79544fc58739da21", "text": "4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\nFor a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\nFor BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\nImplementation. The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). The head number is set to 6 and the window\nsize is set to 8. We train the restoration module with a batch size of 96 for 150k iterations. We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. We use Adam [ 28] optimizer and set the learning rate to 10\u22124. The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\nMetrics . Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. We also\ndeploy a user study for a more thorough comparison.\n4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\nBSR on real-world dataset. We provide the quantitative comparison on real-world datasets in Table\n1. It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. The visual comparison results are presented in Figure 3. It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. More\nvisualization results can be found in Figure 11 and Figure 12.\nTable 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. Red and blue indicate the best and second best performance.", "start_char_idx": 0, "end_char_idx": 3719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0301cf35-5310-41fb-9e50-38a997bae8bd": {"__data__": {"id_": "0301cf35-5310-41fb-9e50-38a997bae8bd", "embedding": null, "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4c628c7-ca19-4dc1-9998-cfe454609fc6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "8f657c9f9c759dc3633d09770e0baed92fcf370906cd8e5c7a42d62057141c7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ff91f68-87ba-48ad-b71a-74c612831349", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "a0e54f55366b26a9a2a49c9cc825be799b5d70170899516f79544fc58739da21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f49cf013-1e95-4223-a037-b18e00217122", "node_type": "1", "metadata": {}, "hash": "70c3bf09e5fd4d430568b2ca1940d698eafa1642a66127adfcddf3b0b3a5cfbf", "class_name": "RelatedNodeInfo"}}, "hash": "04ee1c82a4ad7a6980b58545f63b892b5c247e983d4ff9568a09b29465522e07", "text": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. The visual comparison results are presented in Figure 3. It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. More\nvisualization results can be found in Figure 11 and Figure 12.\nTable 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. Red and blue indicate the best and second best performance. The top 3 results are marked as gray .\nDataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n7", "start_char_idx": 2898, "end_char_idx": 4413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f49cf013-1e95-4223-a037-b18e00217122": {"__data__": {"id_": "f49cf013-1e95-4223-a037-b18e00217122", "embedding": null, "metadata": {"page_label": "8", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d476f61-3318-4457-94da-6e2b9bd0b99f", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "b67afda9c9d49137e40416b034543b3a6c3c447963cf0270135de11647090645", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0301cf35-5310-41fb-9e50-38a997bae8bd", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "04ee1c82a4ad7a6980b58545f63b892b5c247e983d4ff9568a09b29465522e07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24fb5653-3fd9-4504-8102-422cf45cc53b", "node_type": "1", "metadata": {}, "hash": "98d8b30f4ef3a39bbc23bbcc05f12161e41080ec1b7c0a38456ea3fd306282b8", "class_name": "RelatedNodeInfo"}}, "hash": "70c3bf09e5fd4d430568b2ca1940d698eafa1642a66127adfcddf3b0b3a5cfbf", "text": "Figure 3: Visual comparison on real-world datasets with upsampling scale factor of 4.( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. To be more exact, better result\nobtains higher score. 31 users are recruited to\nconduct this user study under detailed instruc-\ntion. The distribution of scores obtained by each\nmethod is shown in Figure 4. It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\nBFR on both synthetic and real-world datasets. We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\nThis reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. It can be seen from the second example that only DiffBIR\n8", "start_char_idx": 0, "end_char_idx": 2085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24fb5653-3fd9-4504-8102-422cf45cc53b": {"__data__": {"id_": "24fb5653-3fd9-4504-8102-422cf45cc53b", "embedding": null, "metadata": {"page_label": "9", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f92f5a6-5161-4e35-8312-d0db9bedd44b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "ad34664ebb585a98175b3d0bb6820a1a99840ee60645f16fb4bd8eee75a66996", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f49cf013-1e95-4223-a037-b18e00217122", "node_type": "1", "metadata": {"page_label": "8", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "70c3bf09e5fd4d430568b2ca1940d698eafa1642a66127adfcddf3b0b3a5cfbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f09c8d4a-3da8-406a-8937-968ab04e29cf", "node_type": "1", "metadata": {}, "hash": "4024b0af8ef26271101fef8a0f50eff8f888c7b10e1160a5b89a7662bdb8bde9", "class_name": "RelatedNodeInfo"}}, "hash": "98d8b30f4ef3a39bbc23bbcc05f12161e41080ec1b7c0a38456ea3fd306282b8", "text": "can successfully recover the occluded left eye. Figure 6 presents a visual comparison on real-world\ndataset. It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). More visualization results can be found in Figure 9 and Figure 10.\nFigure 5: Qualitative comparison of different BFR methods on synthetic datasets.( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets.( Zoom in for best view )\n9", "start_char_idx": 0, "end_char_idx": 710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f09c8d4a-3da8-406a-8937-968ab04e29cf": {"__data__": {"id_": "f09c8d4a-3da8-406a-8937-968ab04e29cf", "embedding": null, "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "005d8e3c-6310-4efe-9c8b-7340b5c694b2", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "6a9495f2abbcca1331af9a88f3fbfbecabbdd14f5f8dc693fc4e7443d28bade9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24fb5653-3fd9-4504-8102-422cf45cc53b", "node_type": "1", "metadata": {"page_label": "9", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "98d8b30f4ef3a39bbc23bbcc05f12161e41080ec1b7c0a38456ea3fd306282b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f17c3f6f-9f76-4639-9360-3c02d3746a1f", "node_type": "1", "metadata": {}, "hash": "516406e6e43d105a921aaed2d99769ed7d737a4f8fb7bb952fc11c0f92785af8", "class_name": "RelatedNodeInfo"}}, "hash": "4024b0af8ef26271101fef8a0f50eff8f888c7b10e1160a5b89a7662bdb8bde9", "text": "Table 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. Red\nand blue indicate the best and second best performance. The top 3 results are marked as gray .\nSynthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. In this part, we investigate the significance of our\nproposed two-stage pipeline. Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). The\nvisual comparison is presented in Figure 7(a). As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. This demonstrates that the\nrestoration module contributes to preserving fidelity. The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. This indicates that the RM is indispensable in degradation removal.\nLQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet.( Zoom in for best view )\nTable 3: The effectiveness of restoration module. The best results are denoted as Red.\nFace GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. Next, we illustrate the necessity of finetuning the\nlatent diffusion model. Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\nHowever, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).", "start_char_idx": 0, "end_char_idx": 3050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f17c3f6f-9f76-4639-9360-3c02d3746a1f": {"__data__": {"id_": "f17c3f6f-9f76-4639-9360-3c02d3746a1f", "embedding": null, "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "005d8e3c-6310-4efe-9c8b-7340b5c694b2", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "6a9495f2abbcca1331af9a88f3fbfbecabbdd14f5f8dc693fc4e7443d28bade9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f09c8d4a-3da8-406a-8937-968ab04e29cf", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "4024b0af8ef26271101fef8a0f50eff8f888c7b10e1160a5b89a7662bdb8bde9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b4601ea-4c43-44de-a582-9aa1bf3f44ee", "node_type": "1", "metadata": {}, "hash": "ba89653ea265575edf7a3a0a87d0fe4062c40895d39052df6d31c699d86d7c54", "class_name": "RelatedNodeInfo"}}, "hash": "516406e6e43d105a921aaed2d99769ed7d737a4f8fb7bb952fc11c0f92785af8", "text": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model. Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\nHowever, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\nThe Effectiveness of LAControlNet. Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\nAs shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10", "start_char_idx": 2570, "end_char_idx": 3652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b4601ea-4c43-44de-a582-9aa1bf3f44ee": {"__data__": {"id_": "1b4601ea-4c43-44de-a582-9aa1bf3f44ee", "embedding": null, "metadata": {"page_label": "11", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "632c852f-4d75-4dbc-891d-e0a716ad5eae", "node_type": "4", "metadata": {"page_label": "11", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "6535b734a01de0350abbd37593cbc6c7bdcdaaf8b85fcc4546f55bd910644b05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f17c3f6f-9f76-4639-9360-3c02d3746a1f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "516406e6e43d105a921aaed2d99769ed7d737a4f8fb7bb952fc11c0f92785af8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0ffdba7-1d58-482b-ad4c-a04324cb0b21", "node_type": "1", "metadata": {}, "hash": "32c664dc11aad152f3b545cef285c8be7ba537dc533fa7458409e765d8628bcd", "class_name": "RelatedNodeInfo"}}, "hash": "ba89653ea265575edf7a3a0a87d0fe4062c40895d39052df6d31c699d86d7c54", "text": "regularization on color consistency during training. One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\nNevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\nThe Flexibility of Controllable Module. Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. The visualization result is shown in Figure 8. Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\nFigure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg.(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\nExtensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\nReferences\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural\nimages. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative\nmodels. In International Conference on Machine Learning , pages 537\u2013546. PMLR, 2017.\n11", "start_char_idx": 0, "end_char_idx": 2494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0ffdba7-1d58-482b-ad4c-a04324cb0b21": {"__data__": {"id_": "d0ffdba7-1d58-482b-ad4c-a04324cb0b21", "embedding": null, "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "150f5b22-5a80-4207-8350-1eaddb100c9a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "2d430c282e10e887995642dcc2375c1144e530f566a9283d876b7f0395f1b6e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b4601ea-4c43-44de-a582-9aa1bf3f44ee", "node_type": "1", "metadata": {"page_label": "11", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "ba89653ea265575edf7a3a0a87d0fe4062c40895d39052df6d31c699d86d7c54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c83538c-ce07-4e6d-ac4b-d2b6b9ea1da4", "node_type": "1", "metadata": {}, "hash": "d40ad9daa5b6df14c0d81cb112c7b890b0794a3a742189bb83a5cf65f35ae9df", "class_name": "RelatedNodeInfo"}}, "hash": "32c664dc11aad152f3b545cef285c8be7ba537dc533fa7458409e765d8628bcd", "text": "[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image\nsuper-resolution: A new benchmark and a new model. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative latent\nbank for large-factor image super-resolution. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. Progressive\nsemantic-aware style transformation for blind face restoration. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\nReal-world blind super-resolution via feature matching with implicit high-resolution priors. In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image\nsuper-resolution transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. Cornell University - arXiv , 2017.\n[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. Intermediate layer optimization for\ninverse problems using deep generative models. arXiv preprint arXiv:2102.07364 , 2021.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\nIeee, 2009.\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n[13] Berk Dogan, Shuhang Gu, and Radu Timofte. Exemplar guided face image super-resolution without facial\nlandmarks. Computer Vision and Pattern Recognition , 2019.\n[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for\nimage super-resolution. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. Springer, 2014.\n[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\nGenerative diffusion prior for unified image restoration and enhancement. arXiv preprint arXiv:2304.01247 ,\n2023.\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks.", "start_char_idx": 0, "end_char_idx": 3392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c83538c-ce07-4e6d-ac4b-d2b6b9ea1da4": {"__data__": {"id_": "1c83538c-ce07-4e6d-ac4b-d2b6b9ea1da4", "embedding": null, "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "150f5b22-5a80-4207-8350-1eaddb100c9a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "2d430c282e10e887995642dcc2375c1144e530f566a9283d876b7f0395f1b6e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0ffdba7-1d58-482b-ad4c-a04324cb0b21", "node_type": "1", "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "32c664dc11aad152f3b545cef285c8be7ba537dc533fa7458409e765d8628bcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a203c43d-323d-4b3b-905c-0dbb3b930e00", "node_type": "1", "metadata": {}, "hash": "199810334106660e9d02de51f0ca575b0eb133844916101cb8f10474c2cc212a", "class_name": "RelatedNodeInfo"}}, "hash": "d40ad9daa5b6df14c0d81cb112c7b890b0794a3a742189bb83a5cf65f35ae9df", "text": "Springer, 2014.\n[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\nGenerative diffusion prior for unified image restoration and enhancement. arXiv preprint arXiv:2304.01247 ,\n2023.\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):139\u2013\n144, 2020.\n[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. Springer, 2022.\n[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information\nprocessing systems , 30, 2017.\n12", "start_char_idx": 2773, "end_char_idx": 4283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a203c43d-323d-4b3b-905c-0dbb3b930e00": {"__data__": {"id_": "a203c43d-323d-4b3b-905c-0dbb3b930e00", "embedding": null, "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dec473f-31c5-49d3-839f-ce5f7c07fe11", "node_type": "4", "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "23d0328596c5f33c29530d4be033f9881a33d13ae9c5195caec45ada988511cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c83538c-ce07-4e6d-ac4b-d2b6b9ea1da4", "node_type": "1", "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "d40ad9daa5b6df14c0d81cb112c7b890b0794a3a742189bb83a5cf65f35ae9df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3790c5d1-f39b-49cc-9670-b2ac1569cfa0", "node_type": "1", "metadata": {}, "hash": "748bb091b313df0dc352a8e88e410ae5162125c208894832d437c9566cac9035", "class_name": "RelatedNodeInfo"}}, "hash": "199810334106660e9d02de51f0ca575b0eb133844916101cb8f10474c2cc212a", "text": "[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional\nadversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution\nvia kernel estimation and noise injection. In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models.\narXiv preprint arXiv:2201.11793 , 2022.\n[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. Progressive face super-resolution via\nattention to facial landmark. arXiv preprint arXiv:1908.08239 , 2019.\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[29] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,\n2013.\n[30] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,\n2013.\n[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-\nresolution using a generative adversarial network. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face\nrestoration via deep multi-scale component dictionaries. Springer International Publishing eBooks , 2020.\n[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. Learning warped\nguidance for blind face restoration. Cornell University - arXiv , 2018.\n[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. Learning dual memory\ndictionaries for blind face restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image\nrestoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Blind image super-resolution: A survey and\nbeyond.", "start_char_idx": 0, "end_char_idx": 3401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3790c5d1-f39b-49cc-9670-b2ac1569cfa0": {"__data__": {"id_": "3790c5d1-f39b-49cc-9670-b2ac1569cfa0", "embedding": null, "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dec473f-31c5-49d3-839f-ce5f7c07fe11", "node_type": "4", "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "23d0328596c5f33c29530d4be033f9881a33d13ae9c5195caec45ada988511cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a203c43d-323d-4b3b-905c-0dbb3b930e00", "node_type": "1", "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "199810334106660e9d02de51f0ca575b0eb133844916101cb8f10474c2cc212a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33d3bdec-e51b-4444-94a4-5357484ca7da", "node_type": "1", "metadata": {}, "hash": "4e8e1a2d1956c685bc7e6bfc4da925f13261b15e457a2186cecbf9ab08777704", "class_name": "RelatedNodeInfo"}}, "hash": "748bb091b313df0dc352a8e88e410ae5162125c208894832d437c9566cac9035", "text": "Learning warped\nguidance for blind face restoration. Cornell University - arXiv , 2018.\n[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. Learning dual memory\ndictionaries for blind face restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image\nrestoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Blind image super-resolution: A survey and\nbeyond. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n13", "start_char_idx": 2750, "end_char_idx": 4215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33d3bdec-e51b-4444-94a4-5357484ca7da": {"__data__": {"id_": "33d3bdec-e51b-4444-94a4-5357484ca7da", "embedding": null, "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fabe1d8-2051-4c24-b1b7-f23733a37157", "node_type": "4", "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "519035b1744c6543befbf1f3c5e68498cae603cd56d6917fe29c8df210cc0f1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3790c5d1-f39b-49cc-9670-b2ac1569cfa0", "node_type": "1", "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "748bb091b313df0dc352a8e88e410ae5162125c208894832d437c9566cac9035", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67fba76f-7e3f-4009-80b8-34b46d8711d5", "node_type": "1", "metadata": {}, "hash": "24b5fe0794f8f47a7dd27bbec5dd37430e0de1677f9d9980f3586beb7ea5b176", "class_name": "RelatedNodeInfo"}}, "hash": "4e8e1a2d1956c685bc7e6bfc4da925f13261b15e457a2186cecbf9ab08777704", "text": "[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for\ngenerative adversarial networks. arXiv preprint arXiv:1802.05957 , 2018.\n[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453 , 2023.\n[43] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In\nInternational Conference on Machine Learning , pages 8162\u20138171. PMLR, 2021.\n[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep\ngenerative prior for versatile image restoration and manipulation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. Springer, 2015.\n[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial\nnetworks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n[51] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.\n[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 , 2020.\n[53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems , 30, 2017.\n[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with\ngenerative facial prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.", "start_char_idx": 0, "end_char_idx": 3413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67fba76f-7e3f-4009-80b8-34b46d8711d5": {"__data__": {"id_": "67fba76f-7e3f-4009-80b8-34b46d8711d5", "embedding": null, "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fabe1d8-2051-4c24-b1b7-f23733a37157", "node_type": "4", "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "519035b1744c6543befbf1f3c5e68498cae603cd56d6917fe29c8df210cc0f1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33d3bdec-e51b-4444-94a4-5357484ca7da", "node_type": "1", "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "4e8e1a2d1956c685bc7e6bfc4da925f13261b15e457a2186cecbf9ab08777704", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f35adba-2271-4efa-931b-92d446739686", "node_type": "1", "metadata": {}, "hash": "5a3d8ba7560625918f92e097a4375972da17d31459ec71c9d4f0f13e99840d1e", "class_name": "RelatedNodeInfo"}}, "hash": "24b5fe0794f8f47a7dd27bbec5dd37430e0de1677f9d9980f3586beb7ea5b176", "text": "Curran Associates, Inc., 2019.\n[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 , 2020.\n[53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems , 30, 2017.\n[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with\ngenerative facial prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\nEsrgan: Enhanced super-resolution generative adversarial networks. In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space\nmodel. arXiv preprint arXiv:2212.00490 , 2022.\n[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer:\nA general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n14", "start_char_idx": 2777, "end_char_idx": 4348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f35adba-2271-4efa-931b-92d446739686": {"__data__": {"id_": "3f35adba-2271-4efa-931b-92d446739686", "embedding": null, "metadata": {"page_label": "15", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f516e90-cd79-44a8-b3d8-dd19c38e8b76", "node_type": "4", "metadata": {"page_label": "15", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "abdb4002b24af90f5abf3daec5b57fa75a90d4fe33b52d214ae47a9f0f1a2036", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67fba76f-7e3f-4009-80b8-34b46d8711d5", "node_type": "1", "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "24b5fe0794f8f47a7dd27bbec5dd37430e0de1677f9d9980f3586beb7ea5b176", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0ac5b09-f2be-4444-83e9-b1576540ea7c", "node_type": "1", "metadata": {}, "hash": "1f34ba352433a3a1ff9ee0b54f20674d4a341d8587d99b5067031c37553609a8", "class_name": "RelatedNodeInfo"}}, "hash": "5a3d8ba7560625918f92e097a4375972da17d31459ec71c9d4f0f13e99840d1e", "text": "[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan prior embedded network for blind face\nrestoration in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. Face super-resolution\nguided by facial component heatmaps. In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model\nfor deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n[66] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543 , 2023.\n[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration\nwith codebook lookup transformer. Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n15", "start_char_idx": 0, "end_char_idx": 2393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0ac5b09-f2be-4444-83e9-b1576540ea7c": {"__data__": {"id_": "d0ac5b09-f2be-4444-83e9-b1576540ea7c", "embedding": null, "metadata": {"page_label": "16", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9cd26f9-6644-480b-bc99-2555f5ffd85b", "node_type": "4", "metadata": {"page_label": "16", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "983c357f10de5f1b5c4f5acb869f4660fc12649530af47381ad107f1ca744389", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f35adba-2271-4efa-931b-92d446739686", "node_type": "1", "metadata": {"page_label": "15", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "5a3d8ba7560625918f92e097a4375972da17d31459ec71c9d4f0f13e99840d1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87d26acb-31f5-4a7f-b78e-630dd475f278", "node_type": "1", "metadata": {}, "hash": "6c5bfd1cb51f7aa95b9520a65e5261fdd2e625a3137c493fc4bf6bdda63cf7c7", "class_name": "RelatedNodeInfo"}}, "hash": "1f34ba352433a3a1ff9ee0b54f20674d4a341d8587d99b5067031c37553609a8", "text": "A Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. Resize. We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. Noise. We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\nMoreover, we combine the degradation settings adopted in blind face restoration. Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. In this way, the generation module is trained to remedy the information loss within a wide\nrange.\nB More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset.( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset.( Zoom in for best view )\n16", "start_char_idx": 0, "end_char_idx": 1967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87d26acb-31f5-4a7f-b78e-630dd475f278": {"__data__": {"id_": "87d26acb-31f5-4a7f-b78e-630dd475f278", "embedding": null, "metadata": {"page_label": "17", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef8df65c-8dd0-443a-8043-bf67c5145bd8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "55f4e62d022cd366628f687d70f7b64cf0b537e4cdf7a0c25f249ae54366c84e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0ac5b09-f2be-4444-83e9-b1576540ea7c", "node_type": "1", "metadata": {"page_label": "16", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "1f34ba352433a3a1ff9ee0b54f20674d4a341d8587d99b5067031c37553609a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d57d056-c6a8-4365-9efa-921d81fc177d", "node_type": "1", "metadata": {}, "hash": "0ad0d212a93a03fd63d45bb46646a0013f0fda295341b50685fa11cff35b4da5", "class_name": "RelatedNodeInfo"}}, "hash": "6c5bfd1cb51f7aa95b9520a65e5261fdd2e625a3137c493fc4bf6bdda63cf7c7", "text": "C More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset.( Zoom in for best view )\n17", "start_char_idx": 0, "end_char_idx": 199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d57d056-c6a8-4365-9efa-921d81fc177d": {"__data__": {"id_": "7d57d056-c6a8-4365-9efa-921d81fc177d", "embedding": null, "metadata": {"page_label": "18", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5805841a-ec3b-48c2-87f1-5b1ce684f33f", "node_type": "4", "metadata": {"page_label": "18", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "1443a1aeeb80c7a1a7200972d91dbc1a4abfbb595f722ad48f7947e4f46be8b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87d26acb-31f5-4a7f-b78e-630dd475f278", "node_type": "1", "metadata": {"page_label": "17", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "6c5bfd1cb51f7aa95b9520a65e5261fdd2e625a3137c493fc4bf6bdda63cf7c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ae0d0de-4ce5-4b2c-91fc-2b2a6351f9ea", "node_type": "1", "metadata": {}, "hash": "35f47f55f86767fcbbfd5383a099f2718e281ee7b373e7601bd347da4dd775e5", "class_name": "RelatedNodeInfo"}}, "hash": "0ad0d212a93a03fd63d45bb46646a0013f0fda295341b50685fa11cff35b4da5", "text": "(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset.( Zoom in for best view )\n18", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ae0d0de-4ce5-4b2c-91fc-2b2a6351f9ea": {"__data__": {"id_": "7ae0d0de-4ce5-4b2c-91fc-2b2a6351f9ea", "embedding": null, "metadata": {"page_label": "19", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39c1f1f1-472c-4b16-a991-cc1c3bb8d6aa", "node_type": "4", "metadata": {"page_label": "19", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "9ce7d5ae83c114c03d9179890795b8e7ca726d9b0a59708ff32f2c1a64eaaf41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d57d056-c6a8-4365-9efa-921d81fc177d", "node_type": "1", "metadata": {"page_label": "18", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "0ad0d212a93a03fd63d45bb46646a0013f0fda295341b50685fa11cff35b4da5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac9c3b39-9e51-4e86-9919-7e2082d48ab7", "node_type": "1", "metadata": {}, "hash": "2901a4827997127e2c1cda84c06170c4dfbc60f8f67f737b54c7f39f93023f12", "class_name": "RelatedNodeInfo"}}, "hash": "35f47f55f86767fcbbfd5383a099f2718e281ee7b373e7601bd347da4dd775e5", "text": "(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset.( Zoom in for best view )\n19", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac9c3b39-9e51-4e86-9919-7e2082d48ab7": {"__data__": {"id_": "ac9c3b39-9e51-4e86-9919-7e2082d48ab7", "embedding": null, "metadata": {"page_label": "20", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c0e1de1-74fc-4a2d-b9c3-d487f376b61a", "node_type": "4", "metadata": {"page_label": "20", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "6ab2842a83e7ab507ece6a70c63289faebac19a5de7e0cb1c3805446f04dba9e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ae0d0de-4ce5-4b2c-91fc-2b2a6351f9ea", "node_type": "1", "metadata": {"page_label": "19", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "35f47f55f86767fcbbfd5383a099f2718e281ee7b373e7601bd347da4dd775e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d2fa2a3-bad8-4734-8e50-7d39bc445ecc", "node_type": "1", "metadata": {}, "hash": "0ad20db2b06e69ec954330a1fa84627e0a334c90b3b05884e045b1d4f3999980", "class_name": "RelatedNodeInfo"}}, "hash": "2901a4827997127e2c1cda84c06170c4dfbc60f8f67f737b54c7f39f93023f12", "text": "LQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.( Zoom in for best view )\n20", "start_char_idx": 0, "end_char_idx": 233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d2fa2a3-bad8-4734-8e50-7d39bc445ecc": {"__data__": {"id_": "1d2fa2a3-bad8-4734-8e50-7d39bc445ecc", "embedding": null, "metadata": {"page_label": "21", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c5e725e-cbfb-4cd8-aff8-cb4a63642430", "node_type": "4", "metadata": {"page_label": "21", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "b668a32a39e330bd4731e739945d89f08b0185fed58f185e0f030d7ee387218d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac9c3b39-9e51-4e86-9919-7e2082d48ab7", "node_type": "1", "metadata": {"page_label": "20", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}, "hash": "2901a4827997127e2c1cda84c06170c4dfbc60f8f67f737b54c7f39f93023f12", "class_name": "RelatedNodeInfo"}}, "hash": "0ad20db2b06e69ec954330a1fa84627e0a334c90b3b05884e045b1d4f3999980", "text": "LQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.( Zoom in for best view )\n21", "start_char_idx": 0, "end_char_idx": 234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"d7c28a5e-1ee5-4157-ab97-a7d10fe7c67d": {"node_ids": ["b297e4dd-a7ad-4ae3-83f2-42f7787d97f1"], "metadata": {"page_label": "1", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "64d6aa37-a002-4df6-82e3-ee5910394882": {"node_ids": ["44634e3a-ee59-4da6-99aa-a7b5d8462f57"], "metadata": {"page_label": "2", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "854a6d16-58de-4d7a-aee7-7635248d27aa": {"node_ids": ["761db376-1c8a-4c8b-892a-4797097865a0", "f02a89e9-925d-4362-811b-b099132a1991"], "metadata": {"page_label": "3", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "1029a61c-2640-4a49-a92e-99e9c556d49c": {"node_ids": ["a6a52e9a-a0f9-45e3-bbf0-8887defb6a27"], "metadata": {"page_label": "4", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "95ed2d51-3fda-4ea4-9944-06c76222cd84": {"node_ids": ["d9aa1cd6-4740-4415-83c1-f3bc4a2cdac9", "008fcbbb-cbcb-421d-9957-52e91105e0e1"], "metadata": {"page_label": "5", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "a96193f0-62f8-4cea-9912-c1ca68bbde35": {"node_ids": ["df3747a6-5577-4f22-ab6e-93249f9196c9"], "metadata": {"page_label": "6", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "e4c628c7-ca19-4dc1-9998-cfe454609fc6": {"node_ids": ["4ff91f68-87ba-48ad-b71a-74c612831349", "0301cf35-5310-41fb-9e50-38a997bae8bd"], "metadata": {"page_label": "7", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "2d476f61-3318-4457-94da-6e2b9bd0b99f": {"node_ids": ["f49cf013-1e95-4223-a037-b18e00217122"], "metadata": {"page_label": "8", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "0f92f5a6-5161-4e35-8312-d0db9bedd44b": {"node_ids": ["24fb5653-3fd9-4504-8102-422cf45cc53b"], "metadata": {"page_label": "9", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "005d8e3c-6310-4efe-9c8b-7340b5c694b2": {"node_ids": ["f09c8d4a-3da8-406a-8937-968ab04e29cf", "f17c3f6f-9f76-4639-9360-3c02d3746a1f"], "metadata": {"page_label": "10", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "632c852f-4d75-4dbc-891d-e0a716ad5eae": {"node_ids": ["1b4601ea-4c43-44de-a582-9aa1bf3f44ee"], "metadata": {"page_label": "11", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "150f5b22-5a80-4207-8350-1eaddb100c9a": {"node_ids": ["d0ffdba7-1d58-482b-ad4c-a04324cb0b21", "1c83538c-ce07-4e6d-ac4b-d2b6b9ea1da4"], "metadata": {"page_label": "12", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "8dec473f-31c5-49d3-839f-ce5f7c07fe11": {"node_ids": ["a203c43d-323d-4b3b-905c-0dbb3b930e00", "3790c5d1-f39b-49cc-9670-b2ac1569cfa0"], "metadata": {"page_label": "13", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "3fabe1d8-2051-4c24-b1b7-f23733a37157": {"node_ids": ["33d3bdec-e51b-4444-94a4-5357484ca7da", "67fba76f-7e3f-4009-80b8-34b46d8711d5"], "metadata": {"page_label": "14", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "7f516e90-cd79-44a8-b3d8-dd19c38e8b76": {"node_ids": ["3f35adba-2271-4efa-931b-92d446739686"], "metadata": {"page_label": "15", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "d9cd26f9-6644-480b-bc99-2555f5ffd85b": {"node_ids": ["d0ac5b09-f2be-4444-83e9-b1576540ea7c"], "metadata": {"page_label": "16", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "ef8df65c-8dd0-443a-8043-bf67c5145bd8": {"node_ids": ["87d26acb-31f5-4a7f-b78e-630dd475f278"], "metadata": {"page_label": "17", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "5805841a-ec3b-48c2-87f1-5b1ce684f33f": {"node_ids": ["7d57d056-c6a8-4365-9efa-921d81fc177d"], "metadata": {"page_label": "18", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "39c1f1f1-472c-4b16-a991-cc1c3bb8d6aa": {"node_ids": ["7ae0d0de-4ce5-4b2c-91fc-2b2a6351f9ea"], "metadata": {"page_label": "19", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "3c0e1de1-74fc-4a2d-b9c3-d487f376b61a": {"node_ids": ["ac9c3b39-9e51-4e86-9919-7e2082d48ab7"], "metadata": {"page_label": "20", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}, "6c5e725e-cbfb-4cd8-aff8-cb4a63642430": {"node_ids": ["1d2fa2a3-bad8-4734-8e50-7d39bc445ecc"], "metadata": {"page_label": "21", "file_name": "DIFFBIR.pdf", "file_path": "docs\\DIFFBIR.pdf", "file_type": "application/pdf", "file_size": 45978433, "creation_date": "2023-12-03", "last_modified_date": "2023-12-03", "last_accessed_date": "2023-12-03"}}}}