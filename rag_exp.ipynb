{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install unstructured rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader('docs')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics', metadata={'source': 'docs\\\\2022.acl-long.450.pdf', 'page': 0}),\n",
       " Document(page_content='Volume 1: Long Papers , pages 6507 - 6522', metadata={'source': 'docs\\\\2022.acl-long.450.pdf', 'page': 0}),\n",
       " Document(page_content='May 22-27, 2022 c⃝2022 Association for Computational Linguistics', metadata={'source': 'docs\\\\2022.acl-long.450.pdf', 'page': 0}),\n",
       " Document(page_content='MemSum: Extractive Summarization of Long Documents Using', metadata={'source': 'docs\\\\2022.acl-long.450.pdf', 'page': 0}),\n",
       " Document(page_content='Multi-Step Episodic Markov Decision Processes\\nNianlong Gu\\nInstitute of Neuroinformatics,', metadata={'source': 'docs\\\\2022.acl-long.450.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 501, which is longer than the specified 200\n",
      "Created a chunk of size 322, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 244, which is longer than the specified 200\n",
      "Created a chunk of size 260, which is longer than the specified 200\n",
      "Created a chunk of size 305, which is longer than the specified 200\n",
      "Created a chunk of size 550, which is longer than the specified 200\n",
      "Created a chunk of size 267, which is longer than the specified 200\n",
      "Created a chunk of size 321, which is longer than the specified 200\n",
      "Created a chunk of size 250, which is longer than the specified 200\n",
      "Created a chunk of size 293, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 205, which is longer than the specified 200\n",
      "Created a chunk of size 241, which is longer than the specified 200\n",
      "Created a chunk of size 270, which is longer than the specified 200\n",
      "Created a chunk of size 227, which is longer than the specified 200\n",
      "Created a chunk of size 507, which is longer than the specified 200\n",
      "Created a chunk of size 399, which is longer than the specified 200\n",
      "Created a chunk of size 407, which is longer than the specified 200\n",
      "Created a chunk of size 346, which is longer than the specified 200\n",
      "Created a chunk of size 359, which is longer than the specified 200\n",
      "Created a chunk of size 216, which is longer than the specified 200\n",
      "Created a chunk of size 235, which is longer than the specified 200\n",
      "Created a chunk of size 244, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 323, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 355, which is longer than the specified 200\n",
      "Created a chunk of size 467, which is longer than the specified 200\n",
      "Created a chunk of size 252, which is longer than the specified 200\n",
      "Created a chunk of size 371, which is longer than the specified 200\n",
      "Created a chunk of size 287, which is longer than the specified 200\n",
      "Created a chunk of size 274, which is longer than the specified 200\n",
      "Created a chunk of size 761, which is longer than the specified 200\n",
      "Created a chunk of size 447, which is longer than the specified 200\n",
      "Created a chunk of size 445, which is longer than the specified 200\n",
      "Created a chunk of size 329, which is longer than the specified 200\n",
      "Created a chunk of size 390, which is longer than the specified 200\n",
      "Created a chunk of size 372, which is longer than the specified 200\n",
      "Created a chunk of size 219, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 320, which is longer than the specified 200\n",
      "Created a chunk of size 693, which is longer than the specified 200\n",
      "Created a chunk of size 285, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 348, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 248, which is longer than the specified 200\n",
      "Created a chunk of size 269, which is longer than the specified 200\n",
      "Created a chunk of size 223, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 280, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 271, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 217, which is longer than the specified 200\n",
      "Created a chunk of size 216, which is longer than the specified 200\n",
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 217, which is longer than the specified 200\n",
      "Created a chunk of size 240, which is longer than the specified 200\n",
      "Created a chunk of size 504, which is longer than the specified 200\n",
      "Created a chunk of size 227, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 301, which is longer than the specified 200\n",
      "Created a chunk of size 271, which is longer than the specified 200\n",
      "Created a chunk of size 311, which is longer than the specified 200\n",
      "Created a chunk of size 271, which is longer than the specified 200\n",
      "Created a chunk of size 239, which is longer than the specified 200\n",
      "Created a chunk of size 264, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 224, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 375, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 240, which is longer than the specified 200\n",
      "Created a chunk of size 246, which is longer than the specified 200\n",
      "Created a chunk of size 267, which is longer than the specified 200\n",
      "Created a chunk of size 555, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 357, which is longer than the specified 200\n",
      "Created a chunk of size 348, which is longer than the specified 200\n",
      "Created a chunk of size 344, which is longer than the specified 200\n",
      "Created a chunk of size 535, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 222, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 240, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 242, which is longer than the specified 200\n",
      "Created a chunk of size 225, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 290, which is longer than the specified 200\n",
      "Created a chunk of size 433, which is longer than the specified 200\n",
      "Created a chunk of size 405, which is longer than the specified 200\n",
      "Created a chunk of size 519, which is longer than the specified 200\n",
      "Created a chunk of size 273, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 235, which is longer than the specified 200\n",
      "Created a chunk of size 340, which is longer than the specified 200\n",
      "Created a chunk of size 213, which is longer than the specified 200\n",
      "Created a chunk of size 317, which is longer than the specified 200\n",
      "Created a chunk of size 288, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 246, which is longer than the specified 200\n",
      "Created a chunk of size 661, which is longer than the specified 200\n",
      "Created a chunk of size 201, which is longer than the specified 200\n",
      "Created a chunk of size 519, which is longer than the specified 200\n",
      "Created a chunk of size 366, which is longer than the specified 200\n",
      "Created a chunk of size 249, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 260, which is longer than the specified 200\n",
      "Created a chunk of size 305, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 222, which is longer than the specified 200\n",
      "Created a chunk of size 284, which is longer than the specified 200\n",
      "Created a chunk of size 220, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 293, which is longer than the specified 200\n",
      "Created a chunk of size 236, which is longer than the specified 200\n",
      "Created a chunk of size 332, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 224, which is longer than the specified 200\n",
      "Created a chunk of size 235, which is longer than the specified 200\n",
      "Created a chunk of size 291, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n",
      "Created a chunk of size 221, which is longer than the specified 200\n",
      "Created a chunk of size 369, which is longer than the specified 200\n",
      "Created a chunk of size 331, which is longer than the specified 200\n",
      "Created a chunk of size 234, which is longer than the specified 200\n",
      "Created a chunk of size 258, which is longer than the specified 200\n",
      "Created a chunk of size 201, which is longer than the specified 200\n",
      "Created a chunk of size 217, which is longer than the specified 200\n",
      "Created a chunk of size 229, which is longer than the specified 200\n",
      "Created a chunk of size 267, which is longer than the specified 200\n",
      "Created a chunk of size 204, which is longer than the specified 200\n",
      "Created a chunk of size 293, which is longer than the specified 200\n",
      "Created a chunk of size 339, which is longer than the specified 200\n",
      "Created a chunk of size 349, which is longer than the specified 200\n",
      "Created a chunk of size 219, which is longer than the specified 200\n",
      "Created a chunk of size 538, which is longer than the specified 200\n",
      "Created a chunk of size 250, which is longer than the specified 200\n",
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 724, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 1166, which is longer than the specified 200\n",
      "Created a chunk of size 745, which is longer than the specified 200\n",
      "Created a chunk of size 373, which is longer than the specified 200\n",
      "Created a chunk of size 608, which is longer than the specified 200\n",
      "Created a chunk of size 504, which is longer than the specified 200\n",
      "Created a chunk of size 618, which is longer than the specified 200\n",
      "Created a chunk of size 491, which is longer than the specified 200\n",
      "Created a chunk of size 310, which is longer than the specified 200\n",
      "Created a chunk of size 613, which is longer than the specified 200\n",
      "Created a chunk of size 538, which is longer than the specified 200\n",
      "Created a chunk of size 598, which is longer than the specified 200\n",
      "Created a chunk of size 386, which is longer than the specified 200\n",
      "Created a chunk of size 537, which is longer than the specified 200\n",
      "Created a chunk of size 421, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n",
      "Created a chunk of size 278, which is longer than the specified 200\n",
      "Created a chunk of size 428, which is longer than the specified 200\n",
      "Created a chunk of size 235, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 349, which is longer than the specified 200\n",
      "Created a chunk of size 468, which is longer than the specified 200\n",
      "Created a chunk of size 527, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 369, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 337, which is longer than the specified 200\n",
      "Created a chunk of size 357, which is longer than the specified 200\n",
      "Created a chunk of size 499, which is longer than the specified 200\n",
      "Created a chunk of size 642, which is longer than the specified 200\n",
      "Created a chunk of size 338, which is longer than the specified 200\n",
      "Created a chunk of size 362, which is longer than the specified 200\n",
      "Created a chunk of size 501, which is longer than the specified 200\n",
      "Created a chunk of size 269, which is longer than the specified 200\n",
      "Created a chunk of size 476, which is longer than the specified 200\n",
      "Created a chunk of size 399, which is longer than the specified 200\n",
      "Created a chunk of size 305, which is longer than the specified 200\n",
      "Created a chunk of size 499, which is longer than the specified 200\n",
      "Created a chunk of size 510, which is longer than the specified 200\n",
      "Created a chunk of size 1321, which is longer than the specified 200\n",
      "Created a chunk of size 224, which is longer than the specified 200\n",
      "Created a chunk of size 768, which is longer than the specified 200\n",
      "Created a chunk of size 406, which is longer than the specified 200\n",
      "Created a chunk of size 326, which is longer than the specified 200\n",
      "Created a chunk of size 353, which is longer than the specified 200\n",
      "Created a chunk of size 1180, which is longer than the specified 200\n",
      "Created a chunk of size 426, which is longer than the specified 200\n",
      "Created a chunk of size 1047, which is longer than the specified 200\n",
      "Created a chunk of size 1364, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(chunk_size=200)\n",
    "\n",
    "docs = nltk_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='When MemSum iteratively selects sentences\\ninto the summary, it considers a broad infor-\\nmation set that would intuitively also be used\\nby humans in this task: 1) the text content of\\nthe sentence, 2) the global text context of the\\nrest of the document, and 3) the extraction his-\\ntory consisting of the set of sentences that have\\nalready been extracted.', metadata={'source': 'docs\\\\2022.acl-long.450.pdf', 'page': 0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All, Embed4All\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "gpt4all_embd = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_db = Chroma.from_documents(docs, gpt4all_embd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "faiss_db = FAISS.from_documents(docs, gpt4all_embd)\n",
    "faiss_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local(\"faiss_index\", gpt4all_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=text_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"what is RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which RAG does not require.\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" # replace dots with your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The content the chunk is from.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the document\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Papers and lecture slides\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    chroma_db,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is RAG?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2', metadata={'page': 1, 'source': 'docs\\\\RAG.pdf'}),\n",
       " Document(page_content='Our RAG models achieve state-of-the-art results\\non open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [ 24].', metadata={'page': 1, 'source': 'docs\\\\RAG.pdf'}),\n",
       " Document(page_content='This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.', metadata={'page': 8, 'source': 'docs\\\\RAG.pdf'}),\n",
       " Document(page_content='Acknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models.', metadata={'page': 9, 'source': 'docs\\\\RAG.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tricks: compression\n",
    "\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text. \n",
    "\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=chroma_db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"similar concerns as for GPT-2 [ 50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54].\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "\"RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about RAG?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=chroma_db.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"similar concerns as for GPT-2 [ 50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54].\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"An interactive demo of RAG models can be found at https://huggingface.co/rag/\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "RAG-Token performs better than RAG-Sequence on Jeopardy question generation\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about RAG?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
