{"docstore/metadata": {"887e3cc1-9b34-47b2-888e-bbef4955dd81": {"doc_hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0"}, "e8274ee9-ba00-42ce-bb26-4d8187390e23": {"doc_hash": "b6dd2cf3b75ea5f63ed4fad1470af35dc31ccbad0f3ceeffde882fcc52d055df", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cef4feef-1d4b-4a19-8f2d-eddf42571fc0": {"doc_hash": "0a45aaa647f277fd604d73da5fa5bacfcef8782df36ccdae0027010218123223", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4a160cad-319d-457e-a464-c290c923a7aa": {"doc_hash": "522e79aac68382e248f7bb36c6e5921a3b6aa959bac544d7ebf8565043525cd7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c5096675-ddbc-4845-b44f-0c7f13dce6af": {"doc_hash": "fcde081cc0677e8a63f81125cd860caed5f5099a0b021af6a35af2c60d82d707", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ca42b2d9-b8a6-4e5e-88ca-8eee1aede0c0": {"doc_hash": "3d6167eb6ee259a58a3b5f465414e29f99956137edf9d9147d26da38f021753a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f451bfee-e8cc-4a42-ba43-c071a99f1a39": {"doc_hash": "d2fb2cafdf766ace779d634ae240c7c0437c2d5dc70d8f93949ea1fc614a231f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "90935691-b8d0-40a5-a6bf-2d230b2e5984": {"doc_hash": "357794f70b22dabdd2c971abdad570797c8dbc6cb20d099ee2352d03d0bb7d2b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1b83f7f3-d247-4137-8680-c85e508db105": {"doc_hash": "53aef2457583747866e3e695d1a9d8e8f4a4906327e248a6ac171b7a8d65759d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4557d954-490a-4380-8fb3-1c75ff766294": {"doc_hash": "037ff031bdaed18a0511ecd8468f5574b8d4b7e5488c1cb15809e5fd5aea3d82", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0d061be7-7b18-46da-af7a-4ef12412709b": {"doc_hash": "d6946040cb1ea16d923278170a079e0483c8ae8c500bfe8ecbaf264178495cb0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "81d44862-df30-410c-95c3-3c7d8cf56f26": {"doc_hash": "225d055ed31a706ad14c6abd4492c2b875b0bb76d32a76e1b1f016e9a6d6bd58", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "878f74d8-f2eb-4a3e-9200-6047c5dc8e3f": {"doc_hash": "db4a3708b5a0f89e484aa58c8b112c94989ecd1a4d8b011d6dc1daf3400fb2f2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3c80912f-8ad6-4a0c-9c7f-0bff0150fc32": {"doc_hash": "f139509e4652a7ed204309a5952b02047d261d9a6445bea0e3fd8c2add419856", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b15c81e7-8222-42d7-b86f-9adb2ae16a23": {"doc_hash": "95d12158034c627df66e7165f2a95ebba55b4260585edf680a3a70a45858a53a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0ce69935-20c6-448c-bf4e-163ea822990d": {"doc_hash": "fe943cb290b17e1572a6567a5b2cda3c196a9eb1e2cdf9e2a9de42b85eae980a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ce98d52d-904e-4928-93e9-a8d03ecbddcd": {"doc_hash": "d57b997c11c392856a6842a6d28a57b7040c66d15cf8d1ee52ff67d49164f151", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "97413fd0-d6fd-4277-970d-e5a877e61e2f": {"doc_hash": "287b946518a0d3f3de8c292e70eb950b65b146f77779d5cc6f6ee630d425cd7f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d609659d-5cb6-4ba9-bc98-399080737c58": {"doc_hash": "5907ee7dd59fe1786c67b45e02363daf57ea6fa7a3fff15c74c52fe633e05440", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d6f1e251-e584-4bda-b3d8-75e38b5a46d3": {"doc_hash": "b75fb7a870d8fa630360c7d6276bf00873bafaf5512e13716705adcc4d3043e3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9c0b321f-7758-4ecf-9643-0e364cc6e45a": {"doc_hash": "e27c043f1dceeda49dfeda178ffd139112e5e246537cf4d9b82c5e6d6eea45a4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0ecd85f8-dbaf-4b55-89e9-716765432483": {"doc_hash": "1d0641b1c5f014f359c15705ec68b9b3c6871badffeb00f0486d936360ac07e7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "664f34c4-55ba-4f85-ab87-6bd62c4a68f1": {"doc_hash": "06cd6e844b5d7ed3f5841a56ee3381112ad55b8edbaec3883fde9617deb19bf9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cbab38fa-d062-4e81-9927-37234979f613": {"doc_hash": "452823db6d635acf64d3d30eb8a4f32c041691f6285c0715c33d1f796b6db3f6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "183b765f-6eed-470b-97f3-9117d6259e52": {"doc_hash": "065d5c68670fa7b82ee51d4d65d37abf88cb6b5a094eecb11a2a2f6538cd2403", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "abeaa74d-f8f1-4534-a241-1403569a365d": {"doc_hash": "0c40a7752b13bbb336d9e2c2d15d0ed081ea5d4f91f628a4fc704d4c7cb1063d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0a5d1693-cbd7-4bbc-8344-0c99b4422256": {"doc_hash": "daacda500377ceffeb67384f045cd70eb4e783c15fc6dfd18759cf049bbf703d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5cde80f3-7689-45fe-a5a2-0f45ac271e0b": {"doc_hash": "0efeec3879bf6255612fdda21f68639b7350696a1765623510415a77ba510896", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "260c04c9-89e8-4ed3-8031-ced614c6c6b3": {"doc_hash": "2f4fa73607dbc3da6cd07a9a092c3c12cacf7f24998a9c944a452a05981bc2d6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c0217ff6-b34e-419b-8cf7-7d8251b47bb7": {"doc_hash": "86b98acf07cf8c93681944e3635bfff346c43d6f147a0c7b35764f627acd0227", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "254b2434-cdc3-4bb2-8694-b99f3485c28c": {"doc_hash": "50b95cbd796c585232c6498992d9859dc2239b2c59e3575595be8ca4ffa6bd7e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "379ad591-952e-4f83-bc9b-07f1c2508045": {"doc_hash": "bea172b2808b4ea589abd4f656ce311a8295b8b336b838738fcda392aad6f1d6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "daf2519e-01f9-4177-b2c1-eb77cba5c722": {"doc_hash": "2d9a4b7eec9e9a9532d4b75a8eaa600afe904cbd8e5b49a2ce5e9fb090d999c7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "52be0b8c-ee5b-4aef-a516-2ab212d4aa55": {"doc_hash": "1eecd19f71b599c6802837dd932d8e8dcdd66d31cae636131585a170fcf39c4f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6327bac8-7c0d-4084-83cb-1f71dbfb0d87": {"doc_hash": "d965c42643bec9c3c10d0029e5b3f59f03684b6623e3ec151786ff6bdfc521ff", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "aeee6eeb-44a0-4739-88f2-8e1edef5dc7b": {"doc_hash": "1382fb6033d1a9e96261457b86f5468917cf9903a36ffc571000bb248507a62a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2773337b-1970-4419-a4a4-41afe0252c8c": {"doc_hash": "039b58c4f960df2097d021365682c8106a58e9b81fc4522445c2575bb5d54968", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7658497d-6e31-48a6-a62b-f2ec2a7c5ca9": {"doc_hash": "ee17b7c4f108c74d9b4bd9233e6b524f517d30972e71cad5474d7defceded71e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ac532669-0517-4549-bafb-a9586ffb2de4": {"doc_hash": "bda0b3ea77009f36c008a6c48ffa91dc37276fc0d14db059e4695f5894bfa8b1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "41b6e169-cbbc-47a7-9722-c28ba70d5afa": {"doc_hash": "45d9e310e3c950c2caebc95169e11db3d60bc074910c2c7da0e402d5044db8bd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3d13f11f-b9cc-46ad-8d01-152018f095f3": {"doc_hash": "acde9f441bb5dd348e121a2328a94d05843b2ff55cdafb26bd1470d19dac8e8d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ffb5a522-16c7-4b7a-8769-9d2eafe99a9b": {"doc_hash": "ecc151e7a1d2e99c500d45ae6b19d9b4485eccc087442b86f972c9befad6b835", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8eb0c6bd-c0bf-4c08-8d93-c32405ae5c8b": {"doc_hash": "44c7ad8abb56f862ce2f181f14879c456559c0e38262da523d7c20ec25cefbe7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "aa4abd91-d3be-47cf-8d6a-83ac560d7751": {"doc_hash": "6d8ff0369507b4e366bc9027019de5f8f1b0e87786600bc19c83839d6590a8f3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dc4b1c60-0e08-4821-b3ae-31fd4d27191e": {"doc_hash": "810077761cbbfc1976bfc412470d705a5bb5d8882ff515ef2c253a4486029458", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bb327243-c8a4-4c59-b415-25c9ae61b3d9": {"doc_hash": "a4de15acf03936ee08401bccf12785f6f9638e5ba56e10c2660d16892f5f12d7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7ada15e1-1706-4b99-8de7-bfeb93ac7f24": {"doc_hash": "eaddd98796c6e97f8069d8c302344310682a12a85db78176620e4f3eb4ff4778", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e8d4dc0c-3e90-4cf1-8ada-36bab89b747b": {"doc_hash": "1e98729cead8c0298d77b94a4b97338ff56695578e196f4330788a876bbbe250", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bb707d36-c930-45dc-b610-aeef6228e73a": {"doc_hash": "1695b8aa9ad01c9fa4a8299bcf5dc98ed751af6a969db80c9eb9e5346b1408f1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "51e316c2-47fd-4b28-a986-2a8d1a32f0e8": {"doc_hash": "6acd7610e1c83370891b664effa213cef4918ee99848df40d8cf0e9a5f73ca4e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8e223b2f-e38c-4ca5-a0fe-e486da202ed4": {"doc_hash": "e3567ec0980eeb470c2c87c96b65fa6c24e1729ca9d830c9924bb917f91cb27f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "790146e6-db42-4ac4-a7c1-7dd8773931fc": {"doc_hash": "299e54aae61bdc5054153303ccc5305f273f2c9f4f241743ac1ee2556edc3ab2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "84516289-b954-44cc-92cc-e080c0196b4e": {"doc_hash": "90488b359c793ad54796880cdcaf12a16511882a85d6dd1ee9d08be4e462a658", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3df342d2-23b1-492d-97a5-de874e9371ec": {"doc_hash": "79fc1ce1640a069d5b2a7fa84fb7e5c9889644a7440d5f4dfa115af6ea3c8664", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c872629b-d5b5-4cc3-9c87-954a38042f3d": {"doc_hash": "244c046efb62d0118b250e80aaa43e373a61270eef7a651d10fd8a6398b444e7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "57a41937-44b4-43dc-ab96-cf11ab655018": {"doc_hash": "d449bac1a7874bdb4d1046cb3bc788cd6b6637ac1d42b3b7e1d5be66bcbd31ee", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "47fe2c58-b375-49d0-ba3d-fad724367224": {"doc_hash": "6104fcd7c126c586d7c945bbf840496346c0e899155c306a1fbc7581b2ee4930", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c28f0c5f-fad2-4470-860e-27fede132336": {"doc_hash": "22452a7f3f9d1d4bb38a383122b8cea1098cb548283d6fc257c3e319dde92b11", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2c2cc220-673d-4eb2-a2fb-327889bce696": {"doc_hash": "1ea4a51add90ab6846a5b186792ff4320fff195c417e952f9b20e64ceefd47e0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3384bcd0-4a73-42af-bead-9bbfecf42bf3": {"doc_hash": "8513d41a75ff226ae7391b23aee85318d8bae186cae0314fe2eb47e163eb40db", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "642380f3-ed24-47a1-a6ca-8912e4eaf7f1": {"doc_hash": "95f299b2f418aee27308f7430569f8d053501d661c24d20186c40fdc2ad7275a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6962488e-43aa-44de-8451-7c7ccc73d9e8": {"doc_hash": "7f8f31c7a4961dfcb9437ed0b4ebc346dc44d98946c8903cacf82216d0a768a7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "884f7d0a-de8d-43aa-ae8c-7965db26117f": {"doc_hash": "2cc88731e73a7db5d7eb9b18ae30a46dc0a219d256c3e2e43894848ac8f576ab", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a6719bd5-f89c-401c-8fbb-3d60bc3ba0e6": {"doc_hash": "5765de64b4648ef23a9d29bde2d6d4fb0a846fa029514af3e4771fcbde10b823", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "94fad515-7751-4b16-8ed5-d439effc9231": {"doc_hash": "31bd9f589287c94f9c343aa47f438c75f399f7eab32d89c1b603661d91516500", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "97dcdd9f-eec4-42bb-b926-8d4dcd350397": {"doc_hash": "c38620bd52e88ca12d8025a7f9f6c18324db030244ff147f8c82d4b3263e064f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bc0b8576-da5d-432c-8d75-c4f75f1f13f5": {"doc_hash": "ba33bfa39ed21d1ebe51c257911611c61fa257d40f910e6e3ef811e9f3e75331", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "36282957-700b-4bd4-95a3-f76d9c2413f5": {"doc_hash": "16f1ad43cf59ec0ece6139873e2ed8aff29926aa07b7c857858458e1415c33e2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "92c1a58f-bbe1-49ad-99da-c5301979a82d": {"doc_hash": "86059099ac489e4a5c8b5d2ef0a8ab1eeeab937315a0103e1728c300c6e8330d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c0571fe7-563e-409a-a4e4-aabc59474fa7": {"doc_hash": "60d973a8da08a972a4ce1194070b43015a32780493accd52450e96d464e783f2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7ed9eb4f-7b31-4c86-bc67-1e695dfb0657": {"doc_hash": "249c25890cfb5c849687a4481651fbe69c38048e80e9290e26e6adf18b47162a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1f02c0ba-3d10-4b27-9769-6e2e4bcb0736": {"doc_hash": "a068a0ead4c10af8ec095639e55b2641621bac3f5d65c4dda447bf0bc243cd1b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d08cfaa1-3159-4bcc-8be2-918c79e8ad0c": {"doc_hash": "08acdf09573724df630a782dfff6aa3647f91658c8a594c74eabe6425039dc8a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7fc2e7cf-ace6-4932-8343-71fc9495ae9b": {"doc_hash": "1fe04213645abb6f692a64d54394e0458c4eaf8a9e47d26c38fd0fadb5ff9805", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0dc339dd-6e81-4547-a747-c52c74c30495": {"doc_hash": "ae4b933d286d2b62778ef24e5901ed3628674414162e49f1e69b67db0db59199", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d79ef500-0789-4b32-a9d5-1e9ed2051e49": {"doc_hash": "ab952f174af59b7e9501453d6db9eba73319d7a6105a7584f79313f644a5c76f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d994b7ba-d5ed-4a88-a96a-bd1150633426": {"doc_hash": "cd2a28bf3e274344af6619bb9b11ed63cc76c05b989819586634af914e1d23e9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c28ad670-06c2-4017-8ec2-6f87ecb6bcf4": {"doc_hash": "f400be3ff643bed02e45f9440c4bbb8527bc1ecf26d5fcc32e2c26912cf3fdc5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ec3aa80a-b79a-4aa5-95a2-a29c72a69324": {"doc_hash": "98d9c2d8049111c5432ffc5d7fecae1d4bba0bf63b667a2b14a75860f2aca824", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f0c841ea-f297-416f-9bf9-4a3d6c4aaf05": {"doc_hash": "23b1ecb0c4a13e82b03fdaf1012f22d83c5090e36f135928e2ceb64b725f9217", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cbf618e2-3e59-4a0b-9ae1-d2bbb16fdf2a": {"doc_hash": "4b4a9d574cfd655dddaae409ec83ee0a989b4b8943d972b55c4b8792889b3cc1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8dfbe85e-6966-4006-9e76-9e38a662161f": {"doc_hash": "41f823a7d7e6f48043d46ec9d2f6f56700e1d2e0013362f6fda467a9300d7898", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d54808d6-2445-491a-ae50-ad8c4c63381c": {"doc_hash": "ccd447bbb5537a61681369f3bccb4743993576fd99c3e48ccee89383ad89dfd8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "31d1dbd3-f2e1-4439-ac0a-6111e722609b": {"doc_hash": "30284e284578d09298f21a31d35ad1ec5fc582359b883ab7592339b0fe865e77", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "108c69ac-2118-46d1-ab50-b69c77fb67e7": {"doc_hash": "dfa79d39f109090505dfa4fdd710190c73b2eab9c571ff1151aa6a6cd839ce34", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9f0966f5-5b83-47bc-b176-0d2310b381b0": {"doc_hash": "d9b3bdb7fc949d446c295b001c91f492d1996a5ba8bc721c75b64d3cd4d07b98", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "58388634-5209-4f2d-a2da-60c4305ecb70": {"doc_hash": "5fd0f80d34b5965f693c2f2a63226606603bc0e74fb7dc987b41e178756703ee", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3198f679-0ea6-438b-b0d6-89935847b6c8": {"doc_hash": "90e6ace5bfee1ea15b740b0448037b3fa8a69170dfee17974e8462cb32417d54", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "277bcec3-4472-4755-a59f-b499fd04c445": {"doc_hash": "c2cc171f262651f8a434367ce6145fe97be93ef81a9314eb8a497eefb91caa51", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "18d0b5ac-36f0-43c0-b443-ddf2813c5eec": {"doc_hash": "198af4f69711ffbbd63a2bc87d4e73c28a966e2873e383c33753e4b3919791f1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3932a3dd-55e3-40f8-ae62-598f2afa79cd": {"doc_hash": "12db07e5609b980d39f65a40c78d73b864a595f04b770206199cf70cf8f24f7a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a5a53aff-a935-4ac9-846a-ead97a8e8e4d": {"doc_hash": "b5dbe4fcc3ce6ff635ab9c0d9a35de526b898aaea326e89c0a92ee1349df66b3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3f2bd9d5-4ee5-4f73-854b-327cc4dbd9c2": {"doc_hash": "b55890db37e3777b060a169f4f9a74c20cdfe9c918acd09fad944f004ebc1fed", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f729335c-213c-4ece-b08a-7e3c7c3aa559": {"doc_hash": "0b10bdaa9becbb0744a2fca015ce2a0ee81b8cef475bce2a1a583f300e46ca17", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "168d5bbf-d925-4e19-a6a6-c4a4d1db67fa": {"doc_hash": "d0351f712778ddf3962bb0df7fbb715b47e779df1ef83efe371c8036d90aabe4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "294f88cf-6923-4b09-9a65-414506bbddef": {"doc_hash": "9fa1ab7082abd0bbe47e1432a929cae07973663533f31890c05bd96d7b5d2efe", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4fdb4e19-0761-47c8-8649-c916fe765000": {"doc_hash": "531e53cfe40d410de63b599ca119f64bcc848cf9b589ca591178a115c901a381", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "103621d0-09fd-4a91-9824-e920671a949c": {"doc_hash": "1ce7d8c0febdc565e2581c55c80284232001b492092655bb4e332ea7a3efa993", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4bb20122-108a-495f-9434-2aca0c87ac3f": {"doc_hash": "68a20ee83c2906c35c32bcf5fce783c59b15f5a6cff28c34466871a82a907860", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "823cd9e1-d006-4be6-b7cb-a14dc1b0006a": {"doc_hash": "fcdfe1b175e25761c405e1066cc48574d89ab1ba7dcd08372d5c8ee50982cd01", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cb8cc6b7-1528-4505-a61d-947cea801c73": {"doc_hash": "344d223020b885b08b4ca2b8dc2f8c902f456bed3ec97b1867fc6477e2727107", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ccb3a3bb-aef2-430f-8d9a-3658fa601db7": {"doc_hash": "91dd8b15111d47c0ca51ef304e637fdfe821d0480b1b9c4fb546c97b9fed4efc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f1e06b3e-05ab-4f2b-adb6-c98cdd3db909": {"doc_hash": "1a885eb0675d7e7b57ce2fa5a5a7f6237efa93a1c00b9e1b25dc0aad4e29cf99", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "eb1c5e7c-462d-4d08-927f-acf5f9ab5c19": {"doc_hash": "977cc86520e32de4a85917fb81e0d69f5bc1809b46bafb84ddc32f46e7dc671a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a11ee045-9d8e-4996-9140-ecf6810450d8": {"doc_hash": "ca48ec6c4cc201bdeb2cfbd7de8b71ed77941cdc83937ca5a920b4e6073f83a8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4dc50bf4-82de-4080-b678-0e360d6fe086": {"doc_hash": "418de679bb608bff01edf3ff99a554a98dc7e56259159bb233271e6b5dbe70a2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3e9f65b0-2963-4492-ba06-2a6f06a5100b": {"doc_hash": "4b92a4050b549546cea1107f4b50053bbf7131e8fac3e95632949cf309936815", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dca689b3-b3a4-4724-9c69-8a9f5095138b": {"doc_hash": "9ad7b74a826b2be2bc3af0710491be2b4e510e57364572e26ec28ba6dac5972a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "27337c73-3249-47b9-a4df-a3c2012f6bf9": {"doc_hash": "2c49e04638effd263369dd62aba2472b1dc363252ac9f0fa97057c6d25a7142a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c02b2f67-ed5e-4583-af70-80b6b64084c6": {"doc_hash": "0d592565b94e14565e997f1dc6003ed7e42128c19357a9d5dd0a55741a94bbba", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "66de200d-fbee-4b88-946d-7cc5a8489ad9": {"doc_hash": "7f790d11b315730d05109c57852fe4960ab6604702e01fd95a7f430865772aca", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "13671ef4-4481-46d9-b05a-e3ad81862097": {"doc_hash": "7366a5044f041b20701a526d2f1daed2d63d7520fd752f95deb0644eb3909a2a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e5ea6a92-abe3-4cb5-a163-620674cb2398": {"doc_hash": "afd35d8e7aee8b579cdb415d99a515f849819d112595a2cf28fcc75eb56b8413", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2509fc2a-c8f5-45b2-a7f3-887211b78c35": {"doc_hash": "0171a55e0ac9ffc69b2071ae9377342c0c73ff6b07c96ebf8234ecb421b74bbc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "75f1e5f6-0ec8-439b-ac40-0956e9fc29da": {"doc_hash": "6047aa083d99ee6d9172ad1a2cef60f7639560f561a9ff99fe3b0086394c37e6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "495e43cd-2597-4f13-adc8-cb97d8c8e7c7": {"doc_hash": "7fe90e2a8a64dbe9c008c039111c0e9246671f317e0e1c332a3f0302c0d64a60", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c3621708-7c95-47a7-8b1b-98581467aa5a": {"doc_hash": "14ac43efd089ba02fca67d95f7a61386cfa317c445080c9a1266399c3ae28565", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "93723c87-5488-4771-9776-6672db8b8e7a": {"doc_hash": "444e60e4bd74506c722cfeda210f10fb2b8fb95b40bf8eb5d260456d2600b30a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a2bc1c73-b4b3-4390-b218-983589b51d27": {"doc_hash": "60acc9eba020044a720e104a0c0c96e203e78f40939202c7e1a8aa1c45c5554f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "02de84cb-85c0-4d9c-85d5-a52d503d661d": {"doc_hash": "b9b0a1a8baa7ec15a51776d3df54f30b01e5e0569a26d1191c13b86dc15ab778", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c5a477a2-3f67-46b7-8ee1-fdeff571a1f9": {"doc_hash": "2f6b6f8b5edab3e6f1f78d2207fb1ca3d09905e37826238f18d25aeb5295abe5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1a6a2fe9-9a10-4ba6-8d07-9abc3d1f27d0": {"doc_hash": "4455e4a650c0d8a96daaa9d62e59e664d8ad2a42bb836e1f9280c6b1262521c2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "46e0dda1-f936-48c3-b22c-ff5212489ded": {"doc_hash": "97e6e6ae8b7644afd5e29536e0f256fb6dfed292991abc91ef4900fbb8000d81", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "42c67bed-fca5-4f55-9b4f-009b25892767": {"doc_hash": "b5e8ede13bacd2564e9656a018e3005f9958fe4fcdd53d03d18e0ed4e5cda63e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3806bbf1-78d0-4360-9b31-0a60f8a10a5f": {"doc_hash": "e8df5ad84bcc9db7eb0100023e36eca5ba4553d877238b2ec67a14d83f774e22", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "678fa503-2bce-40f0-ba88-fc0ccf92a4c2": {"doc_hash": "3e8cd75bc98ddf79e00e9f3697b82adc13ed932d3a7cb7edb6228994c2450e3d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "04ee7a19-562d-43d6-86c1-8548f4b098ee": {"doc_hash": "c5f83ab8ae7b8e82e1d59acaf40328732eeda2635398115bbd7916a4ffb14c11", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "83dbe5a6-94fe-4e7d-8c76-b84a51e03d11": {"doc_hash": "974dc0539be1fed8deb67f3044eb6232b54a936151c99154ec76d3486d2c21b2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "14abff48-9c37-4f8e-aab9-959750f29fd5": {"doc_hash": "8b85c4347580d44b1a6fb2e184008a66750b1f770efb86d92f61d3c8bb22a4dc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1cd2ea3b-2962-42f6-84d8-f5761861bd91": {"doc_hash": "ed3cede6d2b1df909df45959d9bac7f579a40ebef0f18eb4ba5fb39a4fd63418", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "913ee0ee-58e5-4826-a7c6-099db479c482": {"doc_hash": "b214521dbf86bc15c0dc1ac10c47b4e5660978f2c5c49f2b3fc1e86536a244f6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "070c0135-aaa4-4288-b5c9-c893ced04488": {"doc_hash": "ca6734d5befeb1b7dcd8d0732f8ee9962afe7ac3e35c50ca022d934a6577bf88", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "48005c0a-b136-4801-bdf2-5aeecf19e1b7": {"doc_hash": "14113dc4fa5b199335e5cbcf495388551a376f077728bc31f84f5205a73f8641", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6b45a12c-a4d3-4c52-bead-ad68bd0d4db0": {"doc_hash": "e586c37fc347ae3d0bf7ff2c1b158a1f944cbef62c328db1baf0d273399c7a32", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "29a8f5b4-53ea-4370-aa21-12ab943fe281": {"doc_hash": "234c09c92c65bbdfedf7e2d8049fe49b3339dd2fc2f58febbbc4f60890e875be", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0520a598-6f1d-438b-8abc-c8030bd3d7de": {"doc_hash": "3fb3adca96c48e2d2d0c881d629fe02abfea43de996dda21fac641066833d125", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0d782296-551d-4f09-b0eb-a8e7ad1d40f7": {"doc_hash": "c3a3c2df323d240ff12644e8e3dd9342404eee2369d645fb7204324ac34dcbb1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f1d93519-ba46-4f0c-8a3b-fd876e1bfc21": {"doc_hash": "2500a71824679c584b7e4d4fe9385c953ace52998eda3dd63ebb5303954544f7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "629e172c-a952-439d-be4b-3daa3ce21ad4": {"doc_hash": "25b9de0468f6ff156c8e7c878adebfa01cce6015a4f4b92042a729cde850d494", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cc2b3f85-e35a-4544-a3ec-0a0258d53062": {"doc_hash": "33081daf0a23111491f2fc7c0e559fa5d20ea3a492b2a8316e77b567e2e2d169", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fc3c035c-e442-435e-bfca-2cf4c39fc81b": {"doc_hash": "376d79bf19e2ebef179141fdf0e648ebf64f036392826698144841b1296cc252", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b53d5e8c-663e-40f8-a993-28bc575b5264": {"doc_hash": "12d9f637a1ed94e7bbf56fc82e28e6cc8c2080db70841821c3989c4c548d3325", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ba0a66c8-f78a-45af-860b-0d8920f9abcc": {"doc_hash": "11250dda877f774674921eae065b95d93016d3950edd1e3895098d5a3c14c0d3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b7fb0487-861c-4b5c-8777-6298ebd7f042": {"doc_hash": "e6622a4245ee2160f170c23444f3b309ff596548ac1c427051a5793a4135e4cc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "06723684-d28f-4264-ae93-3c45472e303f": {"doc_hash": "1de82da7e46afd787eb1a338ccae5534cd42ee16c89a034fa31ff3eceed44e96", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8f4c2e00-0a66-4bb6-bf3f-bb07d53cfe39": {"doc_hash": "054a3edf9a7cc6e982107fff3a9fced3f2b1365aaab4e07dc214408846687a4c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "45579813-f07c-4402-ab9f-7bc300497375": {"doc_hash": "2c903f835ea477ab8a59d213431ec3b282a32c3e8895f5d6a5d0e169b6a59185", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "04449c97-0212-4bbf-a878-131e565c272e": {"doc_hash": "0699d1a7ecef47603b250767a00e39d4fddea18c5b6dc5a097192a873f2f5c50", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4b497066-45bc-4644-a27f-b19751a365cc": {"doc_hash": "b189a4c7682a271dc8a09c2f5d39987c83a5630beee98a999b7b6bcc3a14e0de", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f790045d-75cb-4878-9105-e96e28c20c25": {"doc_hash": "64ec3cb70773a1c6f1352b3cf84f3edef9895565cefe9bf6f2776cc3b8f4ef2d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "79e81508-68de-4331-9222-ab284c0cb200": {"doc_hash": "7e5d6a0e17925228beb87de5b3ee939372b5612c713b398da949ab7e58bebd8a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0ab2e8b3-e8b5-48ce-8977-857dde8ac794": {"doc_hash": "f6668f2c3b01918593ef42ec3b8c5e159d5387575def4844baa99f974b7474ed", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "36fd7f88-a727-41f6-8342-439255a48fae": {"doc_hash": "b7808ed904b9256eebf9540c53f233996dbd880d4adb564d8f0053ffb8d3cf98", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "96b0fc55-79ab-4d65-bd3c-654c8f71edd5": {"doc_hash": "0983810a383080d442fa26ab4fffc2028adcd9c1c9c89d6f41849dd7b89140dc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "29d78105-794b-4253-9e0b-e5d2aef31361": {"doc_hash": "a21c9dc4033a23e86867ea25ee2b33803c45452ed8c8b3217636d2821fea24d0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4b0b288b-3e05-4deb-87e9-c98f464b41ab": {"doc_hash": "1ce99dacc3557c86b333c79f36349c863666ad75277c6314f34b1cb4807af302", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3307634e-2960-4b3b-927d-cec61ea06947": {"doc_hash": "30061c5cf3ae37dc0d4ec292d985944d7a1082869a29e8b4418eaffc5581266d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3fe55192-eed2-41a1-a063-324650fe1ea6": {"doc_hash": "166852e7fff21dc56ae12e4c40d65b3816ab7160f8a71769149107b065930662", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e6c36773-b7fa-4d13-9be8-0340134b6286": {"doc_hash": "92974d8d3285d85a1a4bfc241b5d3d97a903c6da0c4293bd866a3650fd8bcf20", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d03dd2ca-e113-4f6d-9980-7518e474b2a3": {"doc_hash": "aca38b04f4a14eb97784944c08fb56a5651211d0ca7a78d5bf6e6e963e14d5ff", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "651b2efd-5513-42be-a84d-942792bec463": {"doc_hash": "8af84a4bc934e7e61b353db1cfc9f90ac078f41fe8f0c076a9538e3de5752973", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fac5c673-bd74-487b-8a26-e656f0d28350": {"doc_hash": "34dd1291e9ecf7f035dcc558f5dee5b613a84781c1a113fe3b664c5c9c0fad5d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b0f9908e-5a15-4bfe-9a0d-13c03d44ad48": {"doc_hash": "088c12e5e7dfdcfcded7b5ab6d504f669547cd683dafe60765593cdddc495fd3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6b523d44-2250-4857-a871-2bb78ab13fcb": {"doc_hash": "d6b90823175c1aac4b40a668d2fdb0e98f01b80bba142388138d1a028b64e686", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b56eb80c-01e2-4675-957b-db0cec44214a": {"doc_hash": "ec65db028d7bf11c916ac743093396e067378ce018ac06479f90d1c64ad96296", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "927e2f63-ced6-434e-bfbc-d696d3bfadef": {"doc_hash": "7ef5997bd62eaffbe0da26381ec28c6d2c0d3b57ecacd1f8c30b30fb956b9251", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c618e234-385d-46d5-a104-2aeab6ad1e65": {"doc_hash": "c540bb5d60e88c13a636695b173ebf3deda17beb3c4bc0c7344f1568e79e2b28", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "82bb0c82-4d67-495b-89b8-1d57def2b5bc": {"doc_hash": "9574326ebfa839d78a7f0ffaefaca487e560d67fb5133ff01aa77d027ea39493", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3a1b692b-9874-46d6-938b-65d7b6c83322": {"doc_hash": "6f7e17844d7a314d793b14ba49c6ea63debcd9fd7d2fa00e2c2dfce8cd3f6353", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "71e4855e-6988-4833-aab9-a789477d5bb3": {"doc_hash": "92b911aa3af28316559863d2a506e070fe957af3ab02c400de3fd636e9e85b48", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7345b916-0dc3-4540-868a-17b763463334": {"doc_hash": "89a7c5a0293baeea3179ee0dfe1bf8bfec971909b80bc3277179b1d597acb13b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "31e4fa41-4d6d-416a-b1dc-050441f1938a": {"doc_hash": "cb9c6c973311b487f704eb9856fd7dfa9143df2e4dc288059ed932bf17226c20", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9a1019bb-d966-4d86-bc79-9684f1261dbd": {"doc_hash": "2816f10f85f2d9c154e20c730231415f8d81347b73d9ce06c4bfdceaeff85454", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "34639589-cbbe-44a8-93d1-aee506a563fa": {"doc_hash": "e74d9f6ce93399ff1c74e7dd8d49f9a41d8c2222166f308a50d30f98686ba5f9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e5415ceb-c3d5-4806-8a5b-ecedcec97be2": {"doc_hash": "30f2da50d3f8d5ba5f5aa0ee8929113388730e09e598418a056c626ef430ea08", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "345c5d6f-d714-4ff2-8833-002296696fdb": {"doc_hash": "f5b8a72521750760219ab2389c3293d2fc789f1572c8a04d9f67708928f0dae8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "87934093-e51c-42eb-b07e-51cc0e6a372e": {"doc_hash": "bd7dbf200e8dc33405875006a993c1e27c0a532d0c3899cdaf9b652720441ba2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "563511a7-2bf4-40af-9098-6a3009de89c5": {"doc_hash": "2e9be561dd3c6820ff618744da407dc4ff3d87ebcb3d12ed60bec8e8e8c07964", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0689e590-e22d-4f37-96a5-9a1f2c4f9d20": {"doc_hash": "5610cf2e9162ee34f907b2097939c0c19fd67ed9d69d1a8a5b4a371f1d592a30", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "345ea9a5-3f79-4e90-a55e-12994dde510e": {"doc_hash": "784acd299e409dcc41623ae6fa47dd3f71e6d4f90811b99fdc439efbe9e91899", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b1d86b3a-cc1c-4032-b806-174802bfc5c2": {"doc_hash": "f361896099ed9529a86fb873a1bcf7f074d032ba8be148e896838f0aa63ace42", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "da878723-76b7-4dd1-a7ba-960fac34eb88": {"doc_hash": "2af9804e27117fa84b22df8c15afcb46e5922f46b459fcf60f7ff27744465c2f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4d52bfb7-ed49-460f-856f-2aa1480dbf7f": {"doc_hash": "1df1797fb9dd4ccc4d47a3d2a2d1e6d2221b758286918a1e0f5d71d4fc124e07", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "95852e4c-a347-4e78-82a1-ccd6af86e20e": {"doc_hash": "b7e96b4dc2330b0b810a316d87e69108c27a3d92d632f5b922e9951b8a066406", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "60695295-822d-4ba0-ba1e-6b7cb7a6f1f0": {"doc_hash": "43341ceccb3d09c44f7031472fbf32474d1a175001bd919862c87f70c0b309f2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "847dda23-caf9-49ad-b1ad-116461e72413": {"doc_hash": "b9abcee9d9d17ed134bd8f3c4c71dec935f1e6e8ef98925a59b2e52b8efbbda7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d0da254b-15e7-4689-9f45-9ae50beb6c19": {"doc_hash": "3cb3acf81c5523970c4b813ccb89f4b21ce01511cc6b8067c3c754477af430cd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fee909e6-6eb8-4e74-a8f0-2159f2afca77": {"doc_hash": "bbb822fe285205b5987db021a9a5509cd2563f091f918f356c675ecd1fdbf068", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ee83116d-3f44-48d6-8687-6638fdeec590": {"doc_hash": "3e0dc416cd6fdc4081b8cfc772c0e3afd01230d95e48511cd8fc812c35be82a1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c1eb57a0-43f8-4ab6-8c84-974e7d9f214b": {"doc_hash": "e313fd657a477ac8e81018f0faa583cdd93fba07fa0848fb09639fdf11c98c19", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "69b8b300-58d7-45e2-8f75-4414861b9962": {"doc_hash": "4993ceff765b15e2b3276c62268a062eef293bb7160d220897324238dfa07380", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "94e739f6-b404-442b-96b8-cdfa3a38bfca": {"doc_hash": "793657d95f87e49940ff3ef5d167e1f821fe8933e105dcb81e109ce76085f594", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0e776fe8-5ab0-4ffa-abdf-c4c8001c8174": {"doc_hash": "007c0b9ca25e7aa59a5aab338735d99b0a16b38c978bd1a6bb3dcd73ad145fac", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f8551b4d-1563-4d08-8147-89bd31457d05": {"doc_hash": "e54cf2394785b48f0595682802e2f1fd31e4817ff89f4aa8469cc43ac7e181f2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fa871314-45b4-471d-ba69-713bcec0b24b": {"doc_hash": "fcefe71b0e61d63b55a3eec2e1d93d2e42f4dd73122cb94675c472f765937faa", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3e1ad868-4215-40b4-b059-cc80284ee86f": {"doc_hash": "7e42dd3e7577e38285ee76bb403ecf45c99d2c0d1709de9f782f60ced08c6feb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c2c50208-89c6-4dea-bd2e-df578c65a5c0": {"doc_hash": "2b026103b77ee0216eddbce6fb0de8e00aaad6371dcfdb248423012aa399bfaa", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0978c9ea-ffe5-49a1-ba91-57fe0489d955": {"doc_hash": "10e0b3b80197c7709861924d7e5eb222e65ac743b607f14f02695908394fa07f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cf1be144-1eb0-4032-8f76-aec2553c4733": {"doc_hash": "0425730911b104d883905f3eefc22a97aeba1938fc1a2a87925e232b23911b5d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "64a0fbb3-f32b-4316-96c2-9a17a9f13a06": {"doc_hash": "c996ddb8df531fb94fd104a78612d80c8169760e2e96f8eb98f1bc3e66018b14", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ce38a0d4-3458-4ead-8f4a-dbebc017863a": {"doc_hash": "0d168d6fc53d7820f9fc7c5f4bfdf6db162e25304b59d47f3a689e086f82f325", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e4e38081-d462-4a9d-9019-83a009d29477": {"doc_hash": "65a1e87c48d529d070a8fe3728a251895d310f70bcb48198297064a8d446edb0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "60f8db2b-dc9a-4ea8-9f41-143b7c027685": {"doc_hash": "1c63f78db8f316b952e45d29c7b352b1447e6975b84976e9913d921165e4d46e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a6c6399d-9912-4417-9a88-61005b0fef04": {"doc_hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8f5f4da7-7a40-41d6-931e-9ad2ef9062cc": {"doc_hash": "37ce2e970133ed562edf72699b1f91aeca7b399ef5f4353bb9358071dea04c93", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d50cd03e-a76a-4e3e-a98c-0b1f399ae8f5": {"doc_hash": "483dc75e29288851fce34020880f940b2b39e6ee21d1e740712114607feb6ebb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "109b8a70-88f3-49bc-a908-26e6ad69c290": {"doc_hash": "d67d2cce53d53d9c74463cf9dc7552f88b9be190385ee2dee039697274761bc0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cf9dee5c-8e10-4142-b1e6-3f8509f036dc": {"doc_hash": "2163e2a2175aaa50d5a01cf3390e3d4c7440a3f95c8ada95363a5dbed098d501", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "611eb5a8-c788-4d0d-9560-e0886aa41273": {"doc_hash": "af13ad44f77bfacba3372d2d2c0993b55e85039f460b3b461bfe5ad291ad3d16", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "afabe86c-a91b-42e2-837a-055c1fe4f43a": {"doc_hash": "959c96a1ae81ca0eb01884e22f5ed2c282a54d07503860446ea1b1e45851d270", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "264a7da6-5672-46cc-a9d9-2883e6cbab59": {"doc_hash": "8ccd244e9646d60a223fa3891d58bb1a4d5861df17a124af15dde6ea9d952de5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0a3c8b2e-333d-4985-84b9-c95d5d75658b": {"doc_hash": "ca7c0a44cbf1368b0b2c3d3ae6a0f4d7b3a22a8afda182c7124194fd9d86def2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ae8e34f0-f350-4f1e-ba4e-6e48e8ac8785": {"doc_hash": "6d5646d60da7f4c1a0a98a3a0d372ae367032e51ba628f72cdcdaf9b02c64c9d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ea68df9c-cf46-496d-abf7-aa2a033ae811": {"doc_hash": "14cc4bcd8bee5216c265632cab4257cd5eeb4c972cd6b542959dc30a16c42f8b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5e28c622-61bc-4e76-ab50-d6b305eabb25": {"doc_hash": "c9b23760f72f391d74488867f06c365b32441329dbf10d4caad06c2cd9e15cd9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fcdc9722-a8b3-4f5e-98f0-7ac9a518358a": {"doc_hash": "e1e4395111ab84e746a6a630863a56f1273831d4d3253435f09cf584b1ddb1cd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b2bff15d-6780-40f2-941d-b2efe98e3358": {"doc_hash": "e700f4fcdff8980241b46c56208f13762577b124ba2e3ea38a05a308eac0839a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7b4eee06-f1c4-48a2-b404-796a5a74790f": {"doc_hash": "63babde0a3206b85b854d66cff31e05ac0ef24e3507fe92c6569ab98783a384a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "40b27d67-e31b-4212-b1b7-c835225f8bb3": {"doc_hash": "40eba104d0c51174c5a6ca95ca0fb5f896cb3f7594de404f338a560c1238dedc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "94ad96d7-0a9f-4ac1-b0de-1c5eb0c0e9b3": {"doc_hash": "5d021ebca03b887a801ac51f74612d5e86cd4125970ee05361836de3f02d4835", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3d025fa7-b0f7-4787-b39d-ec299f44664d": {"doc_hash": "5fd8d581b5aa92a295f7cb45affdcebd2822e92d56bba7a143a66948bed38bd3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8936bab0-ae00-43f7-b24e-b54d3fbbb340": {"doc_hash": "20db08b4f27f07a783871a917fbab62108ca62b178a4e05e0995a9bde097df4e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2201a452-7b9d-4dd8-8491-457e36161411": {"doc_hash": "48e41d3743bb8ac90321fb23c52668bb5c8d39639ee477864ca809d149a389ef", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fe3b84ff-a1ca-4325-8237-32d9390e302d": {"doc_hash": "d40522ef844cb7ea9c833b8c61337807cc67d51933351757841f14246314daf4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a3191d42-95b2-4a7a-ae67-f149bee5fd76": {"doc_hash": "f5c5f10e593d374a89e7410aef793117bd15df6ba33639da37c412d3cbaf23d8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "22435e08-5926-4e7e-8367-8ca08d528a88": {"doc_hash": "9f1b0ca33b83b761e2360371a77d11d1be5ad8ef1d9a76e6b96d261572732aa1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0673045b-dbd3-442e-b459-5232f9143787": {"doc_hash": "5fa8e4b852e0eb42c91f63a7a483ba39c10fbde4d424575002e37afe7f5892de", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ea357866-e618-4e1b-bb5f-36d216bba1ca": {"doc_hash": "d06796566811fdd00ce6cdbfec451336adb07860b3f2f20c18615c31882ad4fb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "43bdd6e7-8794-49be-afc9-a85a9be48750": {"doc_hash": "cde76dd04175ec10f286a4ad0cf97627265448df6743f0e5ffb8b2cf7f3a61a3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5bae9b51-7b4f-48ec-9c18-0de7e4606164": {"doc_hash": "f20d94801d511ebd40ac6fa6f6327682f8af34f0707bd08bced4795d332662d2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8afd5ff6-7d40-4bbd-9b77-8f22deff6342": {"doc_hash": "6725e4a0adee6cc59e5d72c317159cbae3bdf14b0032a977a12178cde38d0e5e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f0a1e8a3-324d-463c-afeb-c492ed9daa2f": {"doc_hash": "8e8c2c588dc9edca005f6226a9614def5523ab0cc15509dc3fa20393fb0dc7b0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5e3548b1-1470-47c6-bec4-36389d703f92": {"doc_hash": "e7eaffd1170028a0e34b61a70dfd371b5c07c05771505295e20617413927365a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "da45dcc2-adc2-42dc-8ceb-e72a331052d1": {"doc_hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b37ddbcd-97ea-4547-b4a6-a68f00ef1057": {"doc_hash": "60fd1fae8cb3b307b132cc2d4c0958bd3f3e07e78ae1cbde161dd1843a5440ab", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dfb5de91-db01-4939-bc33-3616592cd6c2": {"doc_hash": "70853cb48b44678a839eaf615c355f5860e4c609c647995ab58d21c92ce79a99", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2e5cefba-4137-4228-bc24-b083b010f990": {"doc_hash": "576676ac6935eeebd43f21898532f22fd47183b1820a7eef5dfb168d69d3e178", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6b281d00-d5f4-4aa5-8ecf-cc1969f2e781": {"doc_hash": "1fc785f470e763c04158be3683264e0694f2dbb1259d0269f240c7c829ca60d7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "25c39189-98a2-4e25-8fd1-d516a41830bd": {"doc_hash": "47dbc5044b14e01f72a75c441f87755b656d125d9edaee8113a7cd340dfd37d0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1a181d98-6a92-4c06-9794-d3230b963753": {"doc_hash": "be57fee454954c13f05603b7e2c3a2952a43d0ca2a154caf25ce60f4e7cb7595", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3dbda129-b751-4b3a-9fe5-e73b0c20b721": {"doc_hash": "6b4b624f24909c99922939744b0813b720e268bbcc75c97b380283e69026e15b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "423c5cf0-a213-4c56-b664-06053a2ebe72": {"doc_hash": "0594554950f3046526d6562f83ae0c8f690ec49b91aa5fb561a16cdc8ca52db7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f76bebaf-e95b-4e88-87ca-1a3c4d44c887": {"doc_hash": "8d639083af966c25ed15f51044d70daf83429968c548ed3d0f4da34568449992", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a865b445-e410-4f5c-9381-4c3ee63d043b": {"doc_hash": "5232ddeadf08a010ec91992fae4b997dffe035800204af740fede47ff1eedd89", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "180a0f06-0e26-46c9-acdc-a6710785befe": {"doc_hash": "1e97b5a84b84480ef2fd784897c1cae416d84325515f0d08521a1ba99fc9d7c3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e8c50c35-4031-4542-a6a1-b6986dccffe8": {"doc_hash": "9b37982ba9fcdd36f4fd7b88228de6d545fa729f65a32c20c323cf943ff20cef", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7a1490e0-96ca-4506-acaf-953269b8d3db": {"doc_hash": "531040073953866a72cf046ebf6fa6e1311fbd0773265792fb7411a165d4cd07", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8c1158d7-90e4-468e-82cb-473f18fe992f": {"doc_hash": "11e2857b4903f1637c56ae67f607f2ec5234346345dbee604fc13a3a399ec87a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9fc0df74-0385-4391-b4f9-0eb20b6f1b64": {"doc_hash": "c26aea36dc6bffc34748af3fa4ece5d51303d1f4513dd6650802d55f457adf4f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "842d6c2c-8db7-4e4b-aa8e-e1f6c557335b": {"doc_hash": "7985fa01290fee918ce91716b78d46ec20d88c77c2b8a7fbdd0e967271440f8d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "57cd9472-f5c5-45a8-9a29-d8d2123ad685": {"doc_hash": "46353d9aa42833381af75d5c224e257e57cde2ee88213b27ad9a21edb1d173ac", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dd3bda7a-cbff-47b5-8188-eb636b7d35ac": {"doc_hash": "6d713a879eedf9b8ff358835bf0091c1158c78db7f990314e85c941c69e3280b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9c81086e-54a6-412c-9b4c-c1e893ee279a": {"doc_hash": "ef2e09588509719b2bbe8958aadfa29b2971c5670f538bb41baf82ba00507d6d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1021aa89-6d09-48da-89ed-d9933d98a680": {"doc_hash": "c61de3fa3ec87595c80f5da52915b4b102e315737514f975b0118f6f6b1badd5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8dbb3559-30eb-4045-bc5d-b26036e0e7fb": {"doc_hash": "69408ed392ba8395e1cb9a998bcea474ac771375f9cd21a734a24117c22ead41", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f894422d-3968-47b3-91b2-e2cb16415f3b": {"doc_hash": "25e7f73e0e5c3806cd5d038b53136406f9ffc73439fd2f8c812d67106eba3a9c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e78df41f-e7b1-4c05-b561-bfdb4dfc2c50": {"doc_hash": "4c6d0b2fa03c2b5975cde39d038779b2a469d4fce31a2afc34c2db9f5bedeaf6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "058156a7-ba76-4939-8605-49d9089f2432": {"doc_hash": "d9a447f174c56bd7152ec908a55a0113d3a96e7de61b92e38b8b217beb20a7b0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "57f06cc8-c3b3-4108-8fdb-fef473001cb0": {"doc_hash": "52d576d59637edd1986727b76988e1596362562f28a20b27a59a2c7c468b367a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2a90fac1-9e61-4a8e-9638-e3d03abaa1b6": {"doc_hash": "a72fc1747e2182af2b950ff621e1520e3249c3e7e2757c6b9e1f6f87a64a6c85", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e7fa000e-ae3f-4d09-96a6-e19138952711": {"doc_hash": "234f3086c6d378f68d6af5064e0336f793038ccf8eb8ee2d698926f56a9bfbf7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "63bf8bc5-309e-4e5e-97e5-cb4698f3f0b4": {"doc_hash": "8cf4b86199ce3416efea9f9128997bbfe74fe5e09ef0702352766cfaca696fac", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bbeefa32-1443-4b83-959e-12764f6a48a7": {"doc_hash": "a8d3d5acfb279edbd865d0857502288f6a132253c53eb45c9031009e7af1587c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c5539238-3d99-4cd0-8356-000c20717f18": {"doc_hash": "ee849f190e8b0c38a0c9d943c28423d51c30172a99331626ce58f0b6197c932d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b01e6783-bcdd-4cb1-95ab-d81319d2e058": {"doc_hash": "715ab7170f6a8cb5c5c6b3391411b919abebbcddaac3ace7a6d0c087bbecb44f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c3ecce72-200e-4c51-bbb3-72403063afea": {"doc_hash": "bd22df58152c8bf65f2948eb14715d85a5e7cbc2d492f89b4139905e44f8739b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "17bd6cbc-9deb-4f49-b774-c62157218fe3": {"doc_hash": "a1d0c0ff7debcd74b9482abf135aa547f90f9f736e8c9ed2cdf24c6004da853d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bf7530db-91fa-4d03-ab84-d2127ebdd1c8": {"doc_hash": "7400563ede7f412689efed1c13e4f435902f52419f4e7ccc40ec9b0d6e748d6a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4691b7b8-6508-4d11-97a9-e132e3bc0359": {"doc_hash": "e7128932285006fd6fc8f114455e66362dd5257c41384b559f290c633a6d9bef", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f615dd0e-f8e5-4440-8a6e-f0384f4f52d0": {"doc_hash": "cda892b79fa1a0c02e26fe318d22ac1f281c596cf8511fac2b6b0b34d4a8cca5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0511131a-c1cc-492d-9989-a243d5878fdf": {"doc_hash": "2c9e92bb417ce644f4e9540ce0c8601e21630c3c900c16f62949199fab4029d0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1499dfec-f332-4037-8fa4-78e5fd12c2a7": {"doc_hash": "1cd0635c7ff15ea72bac0462ebe1128f7d5a1962b9e67a9e409861aa2d916068", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8746bcd0-ba02-4e73-9775-453b211a040a": {"doc_hash": "3ca130eee86c160d1910e5dc3b2917f144c6618c6d8632599164815682a3a441", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0a8fb800-b1db-4b6c-9952-0ddcd1a246e0": {"doc_hash": "792b791001e8869d4798409419c7955f861f7aefec920fbd6a4f4363c736ef5c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "beacdb7c-03e6-4f71-9462-8d7e86c9cdd4": {"doc_hash": "656d52da5b2549de986c8ffe2ffa1f642cca69f5d88456e3122b8f9eb5d97469", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "43c44ed8-29ca-44e8-ac59-be54635cfe89": {"doc_hash": "02ab0ba34cf748fa9d9f94ecbfe5fd831274f664fb446c791ae37e0f3addce51", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a2520686-96e4-4af7-b4db-e38b9a52308e": {"doc_hash": "dabaa13b9f99255db5f336b28ac42b019dcbe74bbfde79e5f614785a5f70ccee", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4c87ffe8-6fb5-469c-9add-7192476da0d3": {"doc_hash": "2cd3808500a00bf286009c3bb7528ea321c53a4f402ca62749790f8a034ea721", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "067a3710-de15-438a-9556-5429e720fc7a": {"doc_hash": "b876e82814cc481fe45f1d2a946020f4d6c1cdca784a60888a41bc78d8b86f8f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "171df040-b833-491e-bb1b-547e6c62a3ef": {"doc_hash": "d731158785815d2a27bb4368f71107c50510cd8b06698c5744af33b7c8cd9fc0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ba348eff-24eb-4092-8f3e-8d1be3f45724": {"doc_hash": "67890edd2596397a2652640fc023266637534178ff3dbf8c8b9603ea126b0586", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0b97be60-e83e-402f-9995-b3778d387316": {"doc_hash": "de76b10f33c8c1aa049ddef4a4d33bc47f8cd22e4f545989d0242bf100a0e511", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f0a51584-18b0-4486-8860-71e75ba39ca5": {"doc_hash": "5041385b0620a29e1d936ae63267ab0bb01ad4f0e4366e290468a5cb0f63396d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "df40838d-7f0f-43ea-86f7-57120e7e3000": {"doc_hash": "d13e98ac7f8cf2b74d5b394e84805e9e767b141df06ea29046582dafbd7d2e43", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "542995bd-ee2f-447e-b444-68c20e2f84f0": {"doc_hash": "aea3e2dc78f45f37f9272a836de1e0635b8e0f9b0aec9c6bbe577f048538f300", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f78bf349-1623-4609-8018-5d562a67df47": {"doc_hash": "b855b6bc09a88bcdf1fce84bc6f2b700c99decd8300789769c7fe038e8973b8c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7f02815e-b849-476c-8aad-e3f34f4a23bb": {"doc_hash": "f39a6f6459d1ec20314d7ec367d2a5eafd24bea0bd2f7841bf7c815410b07c1b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4780c862-03a1-4ba5-8121-01b3c23731a1": {"doc_hash": "e76c63cad29bee4e9c1c9be77c995266f16308d6d91755b4f3cab8dd001e0019", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "86e8c524-ca71-4d9e-b471-d14965af5916": {"doc_hash": "90750e887afc3085e7622891b51727f0bf316e3cad88e8e9e628ac1416781725", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cfbb1950-5a50-46db-8ff7-aaec2905ea27": {"doc_hash": "b121ea0ba2fbab09848550dfbe4f5868889c0f8a89e67431831efa49f62852a2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9f0c6007-01fc-4a1f-a45a-0530a6948c5d": {"doc_hash": "05e6db237f50cbfb08c6b9a93955847e4dd01f9164ceec0aaec9712aa3d78074", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bc6a7f0c-e602-4534-bae7-a17ac229b32a": {"doc_hash": "d44fc9cc77d3e5275147d871c592a67132b1d19d4d41421e9ee9ef0daeb0c447", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2003c2fe-2558-44f8-b4d1-e83e586c839a": {"doc_hash": "018301978f6b9a30b63221ad5f46d99cdba8a0dbc63b21785b25eeb0bc840c5a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "39b7232a-0cdc-4f7d-9ce0-55c058fc8ef6": {"doc_hash": "3208cabbcd218c0a08237f9e85420af22528868e151f22cdb4e25dedc01d59aa", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "481de39b-d027-41c3-b85b-eaadf3dd1940": {"doc_hash": "e40d0e976bbb8a6b814c36f2c89df0f51ecc14be1fa7b9454c8249f23c2d6e66", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3ecd8886-1272-4eec-9501-fb62bd91ef9d": {"doc_hash": "674022e6282d0aad4ed6c848f9d3cfd854df90bab0efcfa32ced569b07f3eb82", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ab8c725a-40f4-425a-bdaa-db1e62da05d5": {"doc_hash": "7b89d87e994ab0f58bb09e0942b0fcd4a7f22bac6ce9eeffd2793a347dedff61", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0ce6abe5-93a3-4c94-9970-0f8a1977241e": {"doc_hash": "aaf0643e543e91f8db46fdb627081393a5940349ba4158e14f0a7ed7ccef690d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9e6842f3-9e9a-4811-9956-01a82c9d9b08": {"doc_hash": "323caae4d6af72a793f3a653acc3a813a49e10835aa67d343c2661a214ed3068", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9ae979fc-107a-42bb-9167-5838cc143aac": {"doc_hash": "241b032e9c642e98bcaadc85c1b6136b2760604af8380562a1b1af7846152b18", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f92ca317-b8bf-4c0d-b273-71978efd65a9": {"doc_hash": "e8cbcb3f2590a99ca28615188a15e3b600e9dbe62b2fbbc94de1244645ea3427", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "89e970b2-a1e6-42e1-9052-3f3e9f191d38": {"doc_hash": "96b48b9c1f7f9dd5f204881c2c993086991654ed5b9e9b8f46353b27fbacead8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5a88373f-7594-4d54-8723-0b289aadae1e": {"doc_hash": "9a1d5c50bdaa4490ad33b87820fb305097bbae711491ae083f366970363583c0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dbee968b-8c24-4dbb-9328-b89cc9ba46d6": {"doc_hash": "490d6b2cba32dba893f99c563279d7d2020c990091f3233bc67575d15e38b9bd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d56bbb8c-0194-4dbe-bae4-33558d95dc57": {"doc_hash": "9231316e85446fb963d8f93977f0f40ebc4a424f6ee4d96fbe5fa1c152ee1da8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ab591579-badf-42ad-be19-510df44dc6a4": {"doc_hash": "9eb360f159fe4b75e04d85131332d1037d2a8af63f943024b85ea355f5e23132", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "29ac1e64-35f1-4c69-a63a-6516ce992690": {"doc_hash": "ddfad7892f372f866c87478d934b3e3fc3e9610ea19ae262cb99ed0782e648d1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "75f64e11-631b-468e-9822-0989d5c9c4c5": {"doc_hash": "158ad385a2a8db3740f2ba473e86a1fbcf9c29f1eaf2c4a13ca8a560e04c98b7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "752a51c8-af7b-4d36-899f-a86fb53b9158": {"doc_hash": "247521667bf7cb1a464210009a2e5985f7523ad90d68acf09765e5052cadd72a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f27634f0-b139-425f-8819-6a333b789fd1": {"doc_hash": "eb859d14df00e6d9a4e9c325e6efdb35284cec62b548bdb9a39536c9c1ca0334", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9ad198db-25a9-40e0-a305-0913364c1249": {"doc_hash": "62ca5bd9410311924d60b907003df025060a20ed8f6230bb125e63215df9aa09", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8c18168d-60e3-4882-b0aa-85c4ecc7728e": {"doc_hash": "bb668ebb78dbbbf76f6d52e047e8c092277ba38b49572db01b7ffe7d02ca04f6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1bca1016-0aa5-4ee2-baa0-9aa22b6d5163": {"doc_hash": "4646d0091f53103d41b762505b6d757fae9c0ab9743c6a53f3a4b171a955a235", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "792ce173-7bf2-4f65-bfb8-73d159878d98": {"doc_hash": "dfbd34730a78c2f1d3359ca3582e0bab313af2756095970923c9656a60eab737", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b0325e13-59cc-4da1-ac86-769fcd2281c6": {"doc_hash": "cae6eab0bf81d999ebea48a8efb95653af583daf3755d5fa87e2794fbb23994a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e170db10-d971-40ae-a928-50b69e24655c": {"doc_hash": "6fdd216062c1fcb90a2a000a5737dec54af301c0e274770d46aa5434fe0801f3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a39d37af-c459-43cd-b017-939502f7f398": {"doc_hash": "89e5748017e90c3fd376ef9befd24f812a7d6377f96a7548939b2d56cc29d092", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c1372150-0ba3-4928-84de-e428a2609731": {"doc_hash": "b70617a86fded2e941e422dc4a51ca1991e1b0d76950899b02f22b8d5c9778b4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "806df6a3-c505-45f5-86bd-60333f9cf57e": {"doc_hash": "4279a00854644d467c7d18c9f49e13a5c696f3bbf475916f78d9b0481bfef070", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "285936fd-25dd-494c-b9ee-ed8bf9a98ec0": {"doc_hash": "dc9a0d7aa60abbeb53dd314bfb305e325721a115a63e53a0f2e477481826be18", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1daa56d6-9690-47e3-8e9c-6aa6c1f21d4c": {"doc_hash": "6e3cb1aeccfad63590721bed8bfe9aa05f9a98c053bf833965009a27cafbb416", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ac7565be-0c0c-4d04-9342-3b262b91e175": {"doc_hash": "1a662a059e7aa46cb1e0bfb2cd71553ab6b2e0b764fd2560f0278bebe995d836", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e1195bcc-cbaf-4627-8892-6f6ee09307c0": {"doc_hash": "76aea4a28a56732b73b9d2f9261f21f6e826e0aadd64b53cf35b3354aeab4613", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "49084dda-ce96-4f45-9014-8719182723e5": {"doc_hash": "2c6eb9f515f398cf30030c6fcfe45076d5d5c8960b5113a7bc4c4743ec455a04", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c8abec02-9ded-4377-8c86-3a6a7747c570": {"doc_hash": "567da208505672bd08f8928ac8074398c3a5e046f95a482a96a1ad7e28876d28", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d2ba2529-2602-4843-838f-21477aed9997": {"doc_hash": "1dff34d44f5943e5f41ab123dfcfca59c71cad38e553b16f076781d7a9aafc54", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "47417187-8f04-4fae-beb3-6ad8b18c9344": {"doc_hash": "2b54c009d8ca72fc1d7bbd7984f4f1cf08cbca8a995a7d6f283d722d70e0a485", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c82d3052-6d33-4a32-8fbf-1fc0bd6ce166": {"doc_hash": "233925aa3d7b528109fc5785ce61ef997f0f39be3d6f596ab98cd2fe1ffab84d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a2d63ff3-af2b-4bc3-9338-fdd9c5209d41": {"doc_hash": "b9edbde122e40d5e3880ee321669fd9c07c43d4ab1ef59c02fcf3a75724a4917", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "91869c96-9f89-44c7-a052-63380c48d539": {"doc_hash": "45663114d5ad395261c40c02054439dd1e21166aae1f962596616b03831ab9fc", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "25b440f2-b925-42a6-bce5-27beede2d1d5": {"doc_hash": "591bc8bc86854d64bc9e4266b44e81a8d9772c42048123069e8e08298f2d909f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "816ace9f-2828-449e-a3f3-a6955c8c266b": {"doc_hash": "aba588f1dfab8775cbd435a65007b2e79ef45194500f9044ecd0cdc33fd1d191", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "66840cf7-148c-45bf-aca9-9a5dbd806432": {"doc_hash": "33c988f5b23d73b8d209ff89ca84476b6af6e3a0fc796b93367971509efd2f83", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2d36b11d-9ee5-4487-bb44-3681b58dc315": {"doc_hash": "8ce89bde05a7964779552c6eb8dc69eba402b9ecd78eec6605355f35042a8f24", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "acec621d-59d2-44fb-a813-80db16e8ace8": {"doc_hash": "1d24d755429ac26c00103ba8d5b6679c635f7c741b5f947d6ecce262c4ba93f5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dc5c50c7-eda7-4da1-b97e-9e02ccab4aa0": {"doc_hash": "991c2bb2e9eca337c9916d13205b25e3668ab5e1ce0b09e4b1530b228511516b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c38de91a-6c43-452d-826d-867902c36353": {"doc_hash": "850fbc61e5098b52ca0b84613530f97c78a5a6a5d865db9068edb35e47cde996", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "32a357c5-58b1-4a29-8426-03894718d024": {"doc_hash": "a242ae6cb5616f1a9f6f64ef258ee7a09f1af624207a948622dd29509ca7ef8f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6959b7cd-4fbd-47e2-8de1-f22ccb8251f9": {"doc_hash": "b00c680cda2ccb7b185be6fdc81bbbc2ae3d3768eea36f5fcb84f7c7aee76bfa", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e15cf3f2-b63a-428a-be0b-8dfb48516116": {"doc_hash": "4c3cbdced025828967dc19e4eb7666a8e8d5fdaa6e55e21795c3292fc0b18b87", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8f79ca10-8897-460d-90df-c55c59c37dda": {"doc_hash": "f1fd569515ac7b8a5bc3cac69052a3976d35b5f74257ed3fa0b864c66f418e20", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d587bb29-a55b-437d-a72a-0290fb6ac4e8": {"doc_hash": "6f7a9d05fd935e15a2c122396936f84a5ba5cb2f613663dcaa101721b8994750", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0a7fc208-ce88-4a72-b599-57f8af2280a2": {"doc_hash": "2e56aa25f2bbd75f066e4c279d8b73aae60f076b7643c24e9f7a4ee7df881c19", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9d8740bf-f083-424b-9cab-ad8231b15f0c": {"doc_hash": "26e0390cffe7220a6b67b3af1c5615746b944fe456eb5ce2f1311c08dbedb11f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d3032879-67a0-4ba1-804d-7fe94803578e": {"doc_hash": "444a54d37d01f19b5faa365f51b3868adb195c9faed7c669ad28b78f6dfcab5d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "53dc86a5-ec44-4298-8c23-0d4125cbcd59": {"doc_hash": "8f34a77130a3a272ba4f9b2ed567768fa52cab6d7653f213c30f6957f99876a5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "aece8fb3-763e-4fc8-81fd-73234ff42a15": {"doc_hash": "6612489c77f399ce676a60cbef607f89c43de9a3c79c9829053c01d4ba382025", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6cd3b4ca-8188-4fbd-9901-ee552334f981": {"doc_hash": "1ad0912f09b1647569e889fcce3cdc9a2e521387bbda4b77dc747700680b9efd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fdcde95e-abcb-4a48-add9-0000b9cedc63": {"doc_hash": "ffb60acc0f03b45da4fa16e1b70f8d9635272bbb14ccf474e87c08c0da63dff5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0c5ce62d-9b5f-4375-a25b-31539cbb019e": {"doc_hash": "ee0df9621073f4048002bc3a196147e72d093dbecfc689574a6640eab625b3fb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fe5dbf49-332b-4c36-8d5c-3f49fcbbeb7e": {"doc_hash": "d2397f995736389051d2dfbfb7349e925a37b05930a5342538fea15a943aa8e0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ab021809-1cfa-4693-9ae9-3ac5471a6c96": {"doc_hash": "acfa404f99857cd9b87709a46f0ee2056f8b85666c38818b6ec751c24662d634", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ac64eb8e-e4e9-43eb-ba76-62082fafd7a1": {"doc_hash": "de52f4aa5171d66b71d90eba09465469716177e75106d6c7b847a1237124d751", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "189d22b1-81ba-4ceb-808e-17d04d7e637d": {"doc_hash": "00dc9f9178ebe3dfe9d9c0224bc80965823d483d0c19ecf87a6dea8d183dd46d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "027f24ee-13b3-44a4-a4c5-47f60a894c4d": {"doc_hash": "0cf7143e3fa3a34b95ce3c1a9c3f77d8df6ccb28bfe833e3b8b4bb1dd20251af", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f71f39dc-0c17-4c6b-8470-0258a599212a": {"doc_hash": "c79082b1007bd191bf7994b0a007836f5b4cc86efb75b4517c89fe6f593d9b4d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5074b1d9-4bf3-4898-8ee6-1a3be739a40d": {"doc_hash": "140c56016e0ab50850a530f221bee49aa8a0479ef7ae1939013218995bca6d4f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8d98c37f-f2a5-43a8-ba95-37df1a851078": {"doc_hash": "2e0cef5781cb60d9a77be9a55bc7a10d4796a69285c1cecddbebf0d49ea4f376", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a74876de-3fb5-462e-8ab8-f0cf22b5d425": {"doc_hash": "80876d55a160d4358f3b3bfc4a2e3f6ee6366e880476560c0553c27cee5b4649", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e35a3c38-1db8-4c1d-9d8d-fbbf2584602d": {"doc_hash": "3bff282d73fdcea0f30292a9fa09592fa8d1d5cc2095ac3f7869c9c5f8105919", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1b86fae1-803b-4264-85eb-db5493403373": {"doc_hash": "573d09428f2a5e0bb3151fe3793940ec4b67f55902d8bc1fb8f72a4393ead003", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0cd25b7e-25be-4bed-81d7-37dc5c1b8e46": {"doc_hash": "561a386dc708836bb69ca48c4562f8b66d9c833f2b5fe75d52f1f2ea62ad3e46", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a4705da8-b72f-4071-845b-bcb6c2e7b989": {"doc_hash": "83807abc157e0f2378caf233efe174e03db76f9002bfc7aae49e97b2307d7bc1", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3e649744-0fd3-4b8a-b617-724d6eb78955": {"doc_hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0d200043-eec5-47e4-b46c-d547bc14ffa0": {"doc_hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2b839629-7174-4c31-9119-f482194356e7": {"doc_hash": "0ce401db8a6c169af70a898a571f404fe7c2554223fc55206b8498f765ec33e8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b079b4d9-36e0-4a32-8616-7751d682fba4": {"doc_hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0081dec6-f249-43f3-bd14-69d5b5235ec7": {"doc_hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f53aa019-93b4-42bb-9f9d-7621c3d2d638": {"doc_hash": "176373b5bf59962e081828ba7fccc6d38eeb14bf184675a43b7df7e373bceb0d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b5b6d707-f8d6-437a-b0ee-6e65b22d1d6f": {"doc_hash": "79e9f37405f945f31c222ac231542881438416477937cdc793770b5aeabd9f8a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "605a62f0-1470-48db-84d7-09e73acea979": {"doc_hash": "c45e4d20037d2c2e32283fd18cf0a66cd4e8dff260c486c0c76ecab971a0a378", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "62d9324a-2a48-413c-ad9e-e7c3df34f682": {"doc_hash": "bb3bac21f72081b50e4d651915eb6acaa1babd25f2768c2de14b47ef7d54b30a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6254492b-2c25-4944-bddb-3c428054e4f3": {"doc_hash": "85969c8288d185d946750aa08b34b5f3bb8816425966bc3b5f62925adf60f659", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "18b0fbe9-ee13-41a5-acd7-aebedb0b924a": {"doc_hash": "6dfd4ee4d2c5615835533070f4f4f2b07e70d3871de95d3926b3c4a608aab992", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0fe65464-c60c-4a57-b787-af587b328b86": {"doc_hash": "aa8cebfbf9ed4e172f79a806ee59cf9753892f0af5b93c9ee8b83dcc8b3892bb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2f1d8bb1-5078-4933-b4ff-cf884ce36335": {"doc_hash": "83e0554efe43c9e0330592e87b602c729f7c08e1bec10f3a9de30b4c464ebdce", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "736f87bb-60b3-4a4e-a220-6bd02b32b2f4": {"doc_hash": "3c75bc336135edfdb69f05a06dc27e62024d69d0036e44dc7e3331dcc638d410", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c5a196dd-5e75-4d95-b426-b725e4b42cd7": {"doc_hash": "0193657e707aeff44489775183cb2b22943adfcd8cd575a6b9a176a54636c0c8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fce034b3-46ea-45aa-b0cf-93c9ae6aad02": {"doc_hash": "ff0d4aa0e333b129ebe3fb3766b22ed47d41bcf79e84b1be81fa5f6cc80fd57a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8d7e015a-47bc-43a7-8865-ee6edc7459e8": {"doc_hash": "18b887820572351789fcff7a227b4201927b58b49a657a96381a436aabf54a89", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "131b0a41-092a-4429-a258-50c9c0bf3758": {"doc_hash": "bed063e09dd5c18e573328dc711905d038b0d0306fb4583314ce8f5f7dbc8252", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8dc39263-552f-40cf-a602-34ae1ecd66ea": {"doc_hash": "faf45708b5a93e9eb2e6ea46601a925af3f0d780e51092f928ab599358026b18", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ab7e104b-a645-4bfb-83c0-d165301336c5": {"doc_hash": "aeaf2d2f9705e3bd87e6bd62ef42714aeafdbe7a0c47656e58b92a8f1aae5bbb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7777e25b-0fee-4725-b85d-24b6e116feaa": {"doc_hash": "69c2a3c96da41ea9697cca9a3e8d2fba2758cf48e351e68a7b5f46f568d60ed5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6d33ab1e-0ad8-4ea8-8206-804e1cae8056": {"doc_hash": "169fe9850095b7734f67d9605eb1677c2da7f69759c43fcd1888b163f86953e7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a1eebe90-13ca-43aa-abd2-1f7b3cd4e43d": {"doc_hash": "22d09d20d3a69c8321d1240fad93a4a45eea475ea8e575ec1208df7ad89a3678", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5f2325c4-636e-4558-8043-5c0f46755246": {"doc_hash": "bdedcb03cc9bbdfdba0a51437853cd953eb6a742502d6686eab9833bf7eb86c5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b9862b7b-7fd6-4cde-85ff-8816dc093853": {"doc_hash": "fa6991820fafdca763f6f0c776fb0c6b0ec351cc00f3b1714aaa2ba485dc4bdf", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "52479432-9e3f-4e5e-88c3-d7c93332aafd": {"doc_hash": "33ea751838b0e3db451337a86d161a5e39cf99ef004d6f18730f6a851ab1caba", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e85de838-213c-4589-9408-108afb582f89": {"doc_hash": "fd0cfffef06217fa27503181ea25e45ff1a4629e14ccb724d543a7b8ce6f94dd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b04ca275-a026-4749-911a-ec6466ba92b0": {"doc_hash": "6ebaed164b70a2af0f60f6d6b0bf96a06f8f08669ddc4b3ecbee88a8620b38cb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6e5dd9c1-5d4d-4cd3-b639-5a24f408e42c": {"doc_hash": "f3cf11ff0ea9bd5d39e011336bd5da3c9a779b76cc0acd548bbeccdfd6e6976f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "192c098d-4183-405d-99e2-f669b6f15d14": {"doc_hash": "97187db859e5e08093b64e8a5e6bfe6537a4b8e66171ec9e4c392d93e91acc83", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0c377869-376e-4f2b-a063-033b930e9b75": {"doc_hash": "a59aee1ded419905f0470e856fd103ffe3e37e2eb9459bd4970de33c6ba07303", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9870db52-5644-48fd-85ed-43e64d1f2e95": {"doc_hash": "75f1ea27cd54c2aac7e09d0ec10f166d5da9e1f0dbf39215f61e82a86e966606", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "72f842bf-576a-454b-b51d-d9be724b0316": {"doc_hash": "defbc0a54d329e4002fd4f1a56c7e2b2214ad3edaf47abeda494fca043689b44", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c35ea646-50c9-4f6b-ab17-165a8018f3d9": {"doc_hash": "b26f0e375f2a077255b1be87ee444326046d1a201ba1b1ec2781c8edd7889b0f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9d165fb2-1322-4ee3-a1e3-9d6028a6c6b2": {"doc_hash": "85e2df44c0595de04af4a66795fb8cb50f5be76833d804dd6726a4b2eed92e37", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cf8a4d5e-d2ed-479b-840a-e9d7bf0d3415": {"doc_hash": "4dc65aaf842dc10bd58b129abacd61ac0cae35f0e7abaaf900276a143aa9603f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c101d93b-0b90-4c72-9878-fd8a52f005f3": {"doc_hash": "129ae0fa18e094b7ac04cf82b1bc1b8383ca3fad3a4c83315962f100494eecbb", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bd04a337-7655-44f2-be8f-3dc94bffd1da": {"doc_hash": "cb1970dc7f1831ad5e853c223d1ad6df60392f05cda7a69f676f6b81c5bfb3c2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d378ed94-168c-4033-aa9a-939898fa877a": {"doc_hash": "ebd7eb6dd76000be4e6fa4319c6aed9dced465ce1d97489a526c88d7a59005a9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6850df06-973b-427e-9580-85fec0c08c77": {"doc_hash": "5eccd2828100eec3e1c552041a70ee9282f6740fd88eb8731cd6a01edc5f59e4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9f871db1-fb35-409c-916f-0d70b15c34b8": {"doc_hash": "0b8fa98a208eb01c203e53539e583ba9da3d1facf4e80a4911ed4d8a81b62b7b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a3c3a124-2959-4465-aed4-6c0f43127a3b": {"doc_hash": "f35f8021851ae78a2ce2d45a96cf571ab3f803da050ec8da8040a35a729239b7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9308b161-9f6b-46fe-a1ac-7ede7c1579ca": {"doc_hash": "1d1f9f82268c52585ae55c94f67eaaec229e5694e02afdb3ed5caff4f3cf6297", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a316cf69-a59e-406c-b749-aa68e1821d77": {"doc_hash": "569b09f53b4b7fbcf02aced3d338bd84ff072c569530feab5f8de16e626b6c49", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "da993a47-0a10-4936-8d7b-030e9db858e9": {"doc_hash": "28df913563e6d4e39d0bf6de89cbe714b2d5c75e0a78d36e1b0811d34c5a8042", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8fc02bbd-23b3-4785-99d6-0f838f790c87": {"doc_hash": "806a07b19106e35a76fb0e53199f6819a7fcfb0f88fd537d3ecc1d8ebeb1e304", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e2fdf670-04ab-41bd-9398-bfd0769b5d14": {"doc_hash": "022bb0dc550f57f66cdf62fc696adb27a950b9f7b8d116b55649b4a746f8485a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b8a00d1f-9eff-40d8-ab37-5d8e8f5549da": {"doc_hash": "1b619f95e6b56215a8553b9e9d1e8834142f81828f039ec29e58d673a8e9dec5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "57e1aa90-3eae-4e7a-a625-ab8465ade852": {"doc_hash": "8a45d0db2e7c0ed3bbb9a4256ca82f71771cf2e83e8d95d8fd95eaac27a2a738", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "ad8ba3d7-3a80-4b06-b50a-43b85cfca8af": {"doc_hash": "7ea0ea61655a0dfcd5208208176571aca393893c7532d05a9bad06aab9fa2d0c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d8152773-d4e7-49da-846e-ec08e80d78ef": {"doc_hash": "1aec701cf198015d1c6e3f7fafbfb75a6334b50d6554aedd125d6768fa9a8f32", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fd91dd22-2003-47cb-a6c0-a0c18dac6199": {"doc_hash": "6e6ad1e00104b5509864079778c69bad8140775953b6735dd2421336260e4817", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0a3212bb-80c6-4ff5-a769-3fa5f9478ccb": {"doc_hash": "d1cbe82e99a35e089670559d838d30f57159eb96088995b0962a03894ae09b5c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "080c63f0-e347-416f-a942-d7e8f89d2936": {"doc_hash": "6be2cd846792293b69dddd2018bbb54a6c8d30121faab0ebe2304e53ed3fe533", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0055bdba-e0ea-458d-b9f3-38c0d3323a05": {"doc_hash": "eb6d7b1a017d5414e49715687edc9da35df570a2357128a64b34d3fecd963f82", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "e6a1e9fc-51a8-4b57-9b9c-283dd919b0cd": {"doc_hash": "83d87a002b2347bf70a04a31046e20f074c5cc4eb69a293d7c3b2e8ef191ae82", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c3858157-7440-45af-9a1b-007044eec5ce": {"doc_hash": "cd928a1a4aa92d8b5e331b75212bc175f941657ab954dee96c3ba614a83ae7d0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1b78e919-db7d-49a2-9d5c-7683134e1068": {"doc_hash": "d3ee8625ceed3deefb824aefb67a0c6e09313ae8f10b448e1cf9f2de9746a3d9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5f633749-b464-4508-82d3-ec7bf7875ff5": {"doc_hash": "6ae07422142650b542c81318375972884b940d38fb4de72ddafd1f4fe4d079db", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5f4eb45d-7dc8-4b95-a9ae-41c896bd56ec": {"doc_hash": "31c83f84f061e3f09094c050b67cf71c86e46ac1bc18f721de897533edd85c2d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3d94159e-1a55-417d-a2e2-5d0c0f07fbce": {"doc_hash": "09ee977650ed1bf1be2587195164a64ab32277a6bfad2cfaa6f0c70423af9e0e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4b88f44a-3e8b-41eb-9ec1-60cc555c7427": {"doc_hash": "f9018aa7e416b12514b9a630e7b7b3046be45e83648101a9bf421c6656024d3c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f8347344-7450-4a17-841e-b421ba968d02": {"doc_hash": "3f87609c5daaa7a17cd13712bdf280af9b267978833f932220044873cc873bd9", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "eb035e3f-9fa4-4b81-b289-7d0c7f6156df": {"doc_hash": "274059304f66a5f2f1befb458db58dc36afe132c098c79c26f333aa7b75b2d52", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1ed9c19a-011e-4944-b76a-af2d97537b5d": {"doc_hash": "f4d5bc3744ccabe1ace2d8943a21b41e861b789c068b8fc63b4ccfdd2a6410f7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "659bed95-39d6-40c6-8d6f-0814055a7aa5": {"doc_hash": "82d0ff17d3669c73f6c1e1a7f7cc4283e85c97b1834d1c8f32dccf67349be91b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8b33fe4d-e031-4aec-b819-cd8cef328fc3": {"doc_hash": "8671dbfde6d3de7489545fded6e8a54dadaa6ac30ee75a245b24216334aaf3a7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1c20c283-ddec-46f5-a14c-c580796b950c": {"doc_hash": "662d0eb157cde60b54091347313ba2528806c21a8f77df2a8ab2df7eb6ea8e79", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7131c697-3542-4fa6-bf2a-a525e2158769": {"doc_hash": "e720995ef66d8208f54203e3a9eb17f3d293feb7a5504a1b855007dcd2b4cfaf", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cf3553c2-6280-4c33-aab8-9cba666c8a0d": {"doc_hash": "29c28b2300086ba576e574bb5361fbe40dd2746d2c0c3162f31c3e7dbdd4c91d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "cf17e4bc-9056-428f-8799-d509d863a5e3": {"doc_hash": "89e14ae916ac6f695b84d37416e0088c4d704bbf6e216bf1a48a686fe478f9e7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "836a49a1-2430-4db6-aa81-5586cc5ad898": {"doc_hash": "2a3a9b092c9d0b27faf248752157b3f61b74aa53c52560aadd84371df76ed95e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7b26f732-ee4e-4960-b538-3876b5175ba0": {"doc_hash": "f385f79b6712a552d073329e058b38c18c6d2b971f1b1ab8f9f5474e3c0f78e8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7fe734c2-17ac-4e8a-aa90-986481b27ae5": {"doc_hash": "a8050f5353943605fd9d4056c7c1f78a76cc1186580ef5969820120a0838fe43", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "04dfd9fe-6fe0-4a41-ae5b-a993ac8d166f": {"doc_hash": "b88a822fa3f57248396fc1cfcfb163a58c4540c349d47416ed808feb128a5c4e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "174d4e98-8027-4ff3-bd3b-487e1259f402": {"doc_hash": "fc2d5e8c94b57f4f6054e95a9ef51b28697fe44c9838d409c716a994d3f61de2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f26ac318-d67a-4bc4-a247-6d8101dea8d0": {"doc_hash": "ffc2fa3b9e7901daed2c4065d832f48a668a5c523727ad54cf51c7ff6e8b353e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c42344e0-8c9b-4f9b-a511-f69aa4fd3fc8": {"doc_hash": "8e480259a0a8f97d11f79f1aae28939c77ecf049955b76f02d2ab0f4a9590f4d", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "379384dd-d943-4316-90a7-b6c326592a0e": {"doc_hash": "620f44ae972bdc3beecda74d4b08c266fe5567f5db1351f68d9d2b1f43a38f71", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2dd84bf4-e8e8-4a22-8576-dcba0b015eef": {"doc_hash": "75410e3c5f1daf3d090eab454b79396a31b67c3063990b3b89d57fe1c3fa96a5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f7ebcbcb-b285-4385-b848-30a15d11cedb": {"doc_hash": "638f49b9bfdae120b80bce95ecc67786e2f42c29d30e88f2c420f3c4d2aebb16", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7d0d3e00-a795-49bf-accf-867e946ca2ea": {"doc_hash": "2f34227b388d9ec3d470b1ba68a2cc6a4305dce9ac85acb3cb6d6e5e868a5510", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "3220e706-be91-46e7-9ad2-62f8347fcff3": {"doc_hash": "c056c6ac4c6e959e3b27512f64936fa233141145f59c5d0f257cb4bcc3eef658", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0c7f434c-bc43-492d-9bd9-bb6b3e6f1140": {"doc_hash": "7330a26bf25035fca5eb7a518bd66531ab52b5f11308d4012ea899d4f0f00d28", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "209f8cd2-3d2c-455e-aff7-398206fe0af4": {"doc_hash": "5695947331506d5713c3f70b8b2e3f87d3d6703bdece459c12a1d97e75709ef7", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9d403329-e1e5-436b-a8da-2c869d37d9d3": {"doc_hash": "497937b4db145a187a9a4aeaf0310db020d5dc06e0293efb4d85af272b422a08", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "536ec7fa-6399-486b-a969-d5682a51d1da": {"doc_hash": "b378059ee0208552307c78395eba68441ee0638f806cfede6a96ed072c39f78c", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bfe660ec-4e3f-464f-abf9-52c3b4d45a4e": {"doc_hash": "6961159a2da6ca992c82270c0024e6e941891b36fff0b77eeb41ecc094ec6d56", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "34bbb280-4777-4523-940a-957b29505fbc": {"doc_hash": "9355a9e0a7621c23b9a4017a76ba6f247f312506b20e3fa98ca4a9dbeedddbc6", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "d494b51b-f065-4a27-a49b-1bd145a9b9fb": {"doc_hash": "357031d130ce461918df36ae796cbed15ffa2a0067c6b43b28e12d255b157cb4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c698be59-2870-414d-b543-4c7ef4a4aca8": {"doc_hash": "dad6ceea546d83b038b600f52547a53bfc78dd483d5e2d6a3ad8115d9a2644c8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "30802c63-3d56-4648-a9f8-2f43162e1e77": {"doc_hash": "1c595640f66bc6899b8bd6efbf3c80558f3abe616499fe48ace4076c7413dfe5", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "6c1b3c3d-2c24-4f0e-aa83-a77b273a4df5": {"doc_hash": "307efa9fb5929003b045f141567dfde9c905d747996d1dbf8d37a6ca06395832", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "68513895-8058-4994-912e-4ff192c37a66": {"doc_hash": "14ddadd33fcc12df3190b0224ea7124de18ebefbc335291e3d33b31696a76f6b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b55b3907-4bed-4173-836b-7cc10bfbdeb3": {"doc_hash": "5dc753113483b658a147c3ac94b9787d3233f0287e64ea95def39cfb27de76ec", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a3e5180f-8b19-4255-adf1-b1b7608d0da1": {"doc_hash": "b75ae3e2754be582fed515ed3f2a73ff8e0fcd6ff3856473ce6a6748ea29d3fd", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "414d3841-f7d5-4245-9d84-0d28bec2c214": {"doc_hash": "403c6e7b76cd1f6d5cb3037766c307ae0079f7fdadfbaafaf6be8d2c718e2dab", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "93d17f47-b31d-4ceb-b993-fc45d4ebef93": {"doc_hash": "e73d5d85c675ef93b9374d8e50ccafecea0144657a7fec234ec04cf2527772e3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4b6518cd-0e53-4ff8-aea7-9fabfddd93be": {"doc_hash": "34f2266acd93ada8b45ab4125aa6c8f85396159355b4433fbf96b412718ccd73", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "54fdadc1-6569-421b-9fdc-4828179987e7": {"doc_hash": "38586132b60657fd86f5406a816101f14b6602ae5c82a2a2a184ccde17bc5389", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "adcdae26-5520-4326-9a46-5b9e173a97db": {"doc_hash": "d2615b97a193de38eb4f2105feadb7ebd694baaf4421443a0160c6cf92f73440", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "963b129b-5793-40d7-8a6a-6f1ab7f51de4": {"doc_hash": "a0786e713122fff96fb34d89d6f29296ca83784807f360d1e3679da9c536e783", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "12862353-18db-4c6a-9ecc-eb8ae2d74dee": {"doc_hash": "8c95ded2a329c233da9184fdf335502705fd9f33ff7cf2b0fb38d0917077a3ec", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1f513a3d-338c-4677-91fe-8b932d296745": {"doc_hash": "744399ac22dc54cf65e10dbb7fbaa583b0d08a1e1143dc62897865899da4ae8a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "2650d1bf-380e-4db7-96bf-30a4a4f8f749": {"doc_hash": "4675a7475784be281f7e8ef88b7706e21f998f62713815fba3fc73676fadadb3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4bae7834-bdf9-4547-9246-6858bd0d2511": {"doc_hash": "94d595781c9ae005b5882fe03eac96ef0c8b2e1664726f33ef1a8d6fadee301a", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "beacb639-6fa0-420c-a15b-2bf5f9797def": {"doc_hash": "467984a3b09e7fa229c6e0820642ed4b357fb3f4308ca8a34ad8258298f9d073", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a8e5ca1a-76ab-40bf-b27d-514ca03b30b3": {"doc_hash": "2ee8941d15838235bc6ba0f89f82b902c16481536afd965830fea35acb2fe32e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8a40a62d-fdfd-4aa4-adb6-cc13a1669afd": {"doc_hash": "67c2542a12cde9ae779a219bf62184698db46e0c1f915c8ff26f4162bd2bf3a3", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8d3912a6-1214-4a18-b74a-266f60e00c59": {"doc_hash": "07ebd086292dc71572da9f2862e7c5a42fda374de22b3e62b2b69acc1268611f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "68e2e68b-3dab-42b5-bd8b-e284ebaada26": {"doc_hash": "d42dd08fe7055c4f653f8f62fea54c5cf4839c543522ee5742458f13f8428881", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8bfda55d-e806-4919-95bc-bd7db323597b": {"doc_hash": "f02b3f2e10455d1d65c1b114ebb65601386cf9c6b8d4d33be4401c08cd125bc2", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a6ca2304-8539-476f-9881-6aa9b88f47e6": {"doc_hash": "2ef984e1eac7d2609a113ebacb88b58a43831521990ce393fe95be82e6937775", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f4a69d63-70df-4ec6-949a-1caa11d0fba7": {"doc_hash": "1a1cac9c8a00089d29beac4a203552bd235566e1d8e6ed78ce10d01b6875394e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "75d93b56-bd8c-4edf-b9fd-72b621e6be1e": {"doc_hash": "6b2aff211e28e99a09f2bb7f747c311a1c1d62a11f9da313e4a3a032dee00532", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "8facfbc9-a3d9-49ad-8125-6fa7b4850d13": {"doc_hash": "f09cf18fb8849cb6f733074686689929d6692f93fcb98d818192e46dd07153e0", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "69940f2a-c138-4b92-8e4a-0d0a79873c94": {"doc_hash": "126e3b9fa81e282665900103aed2999059c67d9465631a787a679ad0dd473051", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "5455a826-68c3-4381-ace5-ff5ff6d91551": {"doc_hash": "e97170b78e6c5be56d44ff559166780ac670ac835ead4f448da419b5f898264b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "51d53027-0cd9-4540-a68b-dd48152cf47f": {"doc_hash": "c116f331403c7227241b5998007a207ca0c84f7dcc136a6e6ec10f7066af4915", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4d198dbd-653a-4d79-9114-e3227cd189a8": {"doc_hash": "420f99e22338f7f8f98d47b12568d4b274d08c5d8e44d79d5625c051d942d062", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "fc6cb691-4a50-4238-93f9-aff8f376e093": {"doc_hash": "036db61d36ddfe5f9197ec5e7bab29db9fa5ff545eb007940e7926243584ab07", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "bf72cdf9-c1ea-4a82-bccb-eb4fe6fe3a94": {"doc_hash": "4533edae29e10a0343128ab34b590b11cb52dcf83f9cf33a26722bb4cd62c2ae", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "4d8d7c5e-013b-43b0-865b-184ad235a289": {"doc_hash": "866fa60e89ea137c34c577427162612f82c2eef466b2bf9af24eed7f35f839ee", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f4c69cf3-a804-4894-a451-a1d60f19afc7": {"doc_hash": "cbdf05bd76f004cf3a5ea42e450ef271db95be6af9eed6a4eaae9310883161e8", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "c87dd377-031c-42a0-8d48-cffa4e53ec87": {"doc_hash": "e3037b7a52deaa19bb422da508e595f5c9e27349c239e5411db816d020c11f87", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "81beb354-7c44-4725-8df1-412198d95c1a": {"doc_hash": "f4b07c06185a258cbb2c997d8135610aa9a040bc1c2cb7482c32f4930695cf58", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "57bd4f3f-6e76-4c9f-82cd-e7e331c6a4ff": {"doc_hash": "7d55af2343c9676a6bcebb347b2c1d2d72e3d6517c78103a11d2860f93402c67", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "130aa38e-1632-40ce-af3c-6ac3747d5427": {"doc_hash": "bae433bb8454ccfc47c478905a3e65da2b73bcefdb09e32db31b77b88b3e525e", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "a39145fb-bef7-4313-9d92-5fe305c6ea71": {"doc_hash": "0cc6ad11ae92ae41e72b2703c44db913087ed48f25fe40f06903be9d226ffffe", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "06201481-187f-4efe-a93d-73726b3d7320": {"doc_hash": "24195c5c4eb269ff5319edddfc558334febac6d659efc8f0779f115d0d65de16", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "be556533-e83e-42c6-bdda-c3b3c8a2a938": {"doc_hash": "a77d596576cedfcb839ff4d7f776776d4b60fe78cd999764f2fe5cec3441fe1b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "07551196-8f79-41f3-8832-cb5cd682f1d6": {"doc_hash": "ea82ae5d88b6bd21c98f4dacc9b33bd666e0ca30126e906293e33c1b706b3ca4", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "313a755d-0a41-40c7-87ee-4076e4016ce3": {"doc_hash": "002a9f763c724ac4091509274e98b3aa4aff96bac720bb899d7c360b333dc24b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f0b50cf0-cafe-4f10-8368-54c8d83cd893": {"doc_hash": "1ff388d137f5dd280e31f3d57b5cee579deceee863e3b305f9b0ebca28528929", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "dfcb3c74-f321-4320-a674-c6e0b5a2b6e6": {"doc_hash": "e281de46ccfc231397af0ea4305277791f109e8fe513340b37f511cdfb4996ae", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "30596029-e1f0-4743-bf1b-2149adb179ad": {"doc_hash": "95b2c195cdac74830e2ecb2cc529dbce74e8d8e7a5c12db07d481e802e5061ac", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "b2b34315-afb7-4d4d-83a9-020d962ead26": {"doc_hash": "4805422eec5f24f2423f769006710b78431847d0cc2106a7de99051f4dcd664b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "1dcc1967-78bd-4213-9fda-2f930633742c": {"doc_hash": "b43a664aeacbade3505da4f4d85713db15513a3d2e5c943df74af735f9a65c20", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "7e280d40-975b-4328-881e-7161ae5de011": {"doc_hash": "67ef770d9dfaa898db560fc76727513fc8e41492340a0ade28c219a3619af59f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f29d3426-e28a-4838-a6ab-6c2802c1e0ac": {"doc_hash": "1b4c758ec90bef24bf92d8f8c8ee162ab4733606a3e4b13e334a70462ce41235", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "0a2e85cb-59a5-4cfe-97b8-f81bead9645b": {"doc_hash": "b191418393281e9fd5e2071e1290cf43c6827a5deb2b862f7ae0b90813f8534f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "f9e7244a-25e6-4df7-b8f1-f44d53217a3e": {"doc_hash": "c77de92e0e545d17d12ec0f2be4790e4c97514843a91b2dd27ef39aa8bf57657", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "73a7f987-771f-4a99-9025-f18204f71cc6": {"doc_hash": "fe909022451f2b8851925619a6dee96fb46c3617829a2e09764e6d09251f8f41", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9766d90b-0f4a-42c8-95b8-90e6b5b4cf88": {"doc_hash": "3fdda749345cf077417b9c1ce742477805a2d966a854e822295f7802359d27ed", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "9f769458-ff9f-4215-b3e4-77c0a13bf4b9": {"doc_hash": "bb58d23f372b8ccf688cf6242f0e15b761d9ae58136f55d3ab70ebd114773b1f", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}, "05e71140-a1bf-4f9c-ab3b-579ffbde6cac": {"doc_hash": "7566faf9b0e740befce7e17d26b796f7da157cf99fd5ecd8e490b55ea6c09c9b", "ref_doc_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81"}}, "docstore/data": {"e8274ee9-ba00-42ce-bb26-4d8187390e23": {"__data__": {"id_": "e8274ee9-ba00-42ce-bb26-4d8187390e23", "embedding": null, "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. ", "original_text": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cef4feef-1d4b-4a19-8f2d-eddf42571fc0", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. ", "original_text": "Our framework adopts a two-stage pipeline.\n"}, "hash": "0a45aaa647f277fd604d73da5fa5bacfcef8782df36ccdae0027010218123223", "class_name": "RelatedNodeInfo"}}, "hash": "b6dd2cf3b75ea5f63ed4fad1470af35dc31ccbad0f3ceeffde882fcc52d055df", "text": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem. ", "start_char_idx": 0, "end_char_idx": 395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cef4feef-1d4b-4a19-8f2d-eddf42571fc0": {"__data__": {"id_": "cef4feef-1d4b-4a19-8f2d-eddf42571fc0", "embedding": null, "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. ", "original_text": "Our framework adopts a two-stage pipeline.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8274ee9-ba00-42ce-bb26-4d8187390e23", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. ", "original_text": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem. "}, "hash": "b6dd2cf3b75ea5f63ed4fad1470af35dc31ccbad0f3ceeffde882fcc52d055df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a160cad-319d-457e-a464-c290c923a7aa", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. ", "original_text": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. "}, "hash": "522e79aac68382e248f7bb36c6e5921a3b6aa959bac544d7ebf8565043525cd7", "class_name": "RelatedNodeInfo"}}, "hash": "0a45aaa647f277fd604d73da5fa5bacfcef8782df36ccdae0027010218123223", "text": "Our framework adopts a two-stage pipeline.\n", "start_char_idx": 395, "end_char_idx": 438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a160cad-319d-457e-a464-c290c923a7aa": {"__data__": {"id_": "4a160cad-319d-457e-a464-c290c923a7aa", "embedding": null, "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. ", "original_text": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cef4feef-1d4b-4a19-8f2d-eddf42571fc0", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. ", "original_text": "Our framework adopts a two-stage pipeline.\n"}, "hash": "0a45aaa647f277fd604d73da5fa5bacfcef8782df36ccdae0027010218123223", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5096675-ddbc-4845-b44f-0c7f13dce6af", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. ", "original_text": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. "}, "hash": "fcde081cc0677e8a63f81125cd860caed5f5099a0b021af6a35af2c60d82d707", "class_name": "RelatedNodeInfo"}}, "hash": "522e79aac68382e248f7bb36c6e5921a3b6aa959bac544d7ebf8565043525cd7", "text": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. ", "start_char_idx": 438, "end_char_idx": 585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5096675-ddbc-4845-b44f-0c7f13dce6af": {"__data__": {"id_": "c5096675-ddbc-4845-b44f-0c7f13dce6af", "embedding": null, "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. ", "original_text": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a160cad-319d-457e-a464-c290c923a7aa", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. ", "original_text": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. "}, "hash": "522e79aac68382e248f7bb36c6e5921a3b6aa959bac544d7ebf8565043525cd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca42b2d9-b8a6-4e5e-88ca-8eee1aede0c0", "node_type": "1", "metadata": {"window": "Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. ", "original_text": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. "}, "hash": "3d6167eb6ee259a58a3b5f465414e29f99956137edf9d9147d26da38f021753a", "class_name": "RelatedNodeInfo"}}, "hash": "fcde081cc0677e8a63f81125cd860caed5f5099a0b021af6a35af2c60d82d707", "text": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. ", "start_char_idx": 585, "end_char_idx": 703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca42b2d9-b8a6-4e5e-88ca-8eee1aede0c0": {"__data__": {"id_": "ca42b2d9-b8a6-4e5e-88ca-8eee1aede0c0", "embedding": null, "metadata": {"window": "Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. ", "original_text": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5096675-ddbc-4845-b44f-0c7f13dce6af", "node_type": "1", "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. ", "original_text": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration. "}, "hash": "fcde081cc0677e8a63f81125cd860caed5f5099a0b021af6a35af2c60d82d707", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f451bfee-e8cc-4a42-ba43-c071a99f1a39", "node_type": "1", "metadata": {"window": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n", "original_text": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. "}, "hash": "d2fb2cafdf766ace779d634ae240c7c0437c2d5dc70d8f93949ea1fc614a231f", "class_name": "RelatedNodeInfo"}}, "hash": "3d6167eb6ee259a58a3b5f465414e29f99956137edf9d9147d26da38f021753a", "text": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. ", "start_char_idx": 703, "end_char_idx": 875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f451bfee-e8cc-4a42-ba43-c071a99f1a39": {"__data__": {"id_": "f451bfee-e8cc-4a42-ba43-c071a99f1a39", "embedding": null, "metadata": {"window": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n", "original_text": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca42b2d9-b8a6-4e5e-88ca-8eee1aede0c0", "node_type": "1", "metadata": {"window": "Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. ", "original_text": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability. "}, "hash": "3d6167eb6ee259a58a3b5f465414e29f99956137edf9d9147d26da38f021753a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90935691-b8d0-40a5-a6bf-2d230b2e5984", "node_type": "1", "metadata": {"window": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n", "original_text": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. "}, "hash": "357794f70b22dabdd2c971abdad570797c8dbc6cb20d099ee2352d03d0bb7d2b", "class_name": "RelatedNodeInfo"}}, "hash": "d2fb2cafdf766ace779d634ae240c7c0437c2d5dc70d8f93949ea1fc614a231f", "text": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. ", "start_char_idx": 875, "end_char_idx": 1053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90935691-b8d0-40a5-a6bf-2d230b2e5984": {"__data__": {"id_": "90935691-b8d0-40a5-a6bf-2d230b2e5984", "embedding": null, "metadata": {"window": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n", "original_text": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f451bfee-e8cc-4a42-ba43-c071a99f1a39", "node_type": "1", "metadata": {"window": "In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios.  The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n", "original_text": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference. "}, "hash": "d2fb2cafdf766ace779d634ae240c7c0437c2d5dc70d8f93949ea1fc614a231f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b83f7f3-d247-4137-8680-c85e508db105", "node_type": "1", "metadata": {"window": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). ", "original_text": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n"}, "hash": "53aef2457583747866e3e695d1a9d8e8f4a4906327e248a6ac171b7a8d65759d", "class_name": "RelatedNodeInfo"}}, "hash": "357794f70b22dabdd2c971abdad570797c8dbc6cb20d099ee2352d03d0bb7d2b", "text": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. ", "start_char_idx": 1053, "end_char_idx": 1251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b83f7f3-d247-4137-8680-c85e508db105": {"__data__": {"id_": "1b83f7f3-d247-4137-8680-c85e508db105", "embedding": null, "metadata": {"window": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). ", "original_text": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90935691-b8d0-40a5-a6bf-2d230b2e5984", "node_type": "1", "metadata": {"window": "The second stage\nleverages the generative ability of latent diffusion models, to achieve realistic\nimage restoration.  Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n", "original_text": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets. "}, "hash": "357794f70b22dabdd2c971abdad570797c8dbc6cb20d099ee2352d03d0bb7d2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4557d954-490a-4380-8fb3-1c75ff766294", "node_type": "1", "metadata": {"window": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. ", "original_text": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n"}, "hash": "037ff031bdaed18a0511ecd8468f5574b8d4b7e5488c1cb15809e5fd5aea3d82", "class_name": "RelatedNodeInfo"}}, "hash": "53aef2457583747866e3e695d1a9d8e8f4a4906327e248a6ac171b7a8d65759d", "text": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n", "start_char_idx": 1251, "end_char_idx": 1317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4557d954-490a-4380-8fb3-1c75ff766294": {"__data__": {"id_": "4557d954-490a-4380-8fb3-1c75ff766294", "embedding": null, "metadata": {"window": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. ", "original_text": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b83f7f3-d247-4137-8680-c85e508db105", "node_type": "1", "metadata": {"window": "Specifically, we introduce an injective modulation sub-network \u2013\nLAControlNet for finetuning, while the pre-trained Stable Diffusion is to maintain\nits generative ability.  Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). ", "original_text": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n"}, "hash": "53aef2457583747866e3e695d1a9d8e8f4a4906327e248a6ac171b7a8d65759d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d061be7-7b18-46da-af7a-4ef12412709b", "node_type": "1", "metadata": {"window": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. ", "original_text": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). "}, "hash": "d6946040cb1ea16d923278170a079e0483c8ae8c500bfe8ecbaf264178495cb0", "class_name": "RelatedNodeInfo"}}, "hash": "037ff031bdaed18a0511ecd8468f5574b8d4b7e5488c1cb15809e5fd5aea3d82", "text": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n", "start_char_idx": 1317, "end_char_idx": 1428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d061be7-7b18-46da-af7a-4ef12412709b": {"__data__": {"id_": "0d061be7-7b18-46da-af7a-4ef12412709b", "embedding": null, "metadata": {"window": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. ", "original_text": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4557d954-490a-4380-8fb3-1c75ff766294", "node_type": "1", "metadata": {"window": "Finally, we introduce a controllable module that allows users\nto balance quality and fidelity by introducing the latent image guidance in the\ndenoising process during inference.  Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. ", "original_text": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n"}, "hash": "037ff031bdaed18a0511ecd8468f5574b8d4b7e5488c1cb15809e5fd5aea3d82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81d44862-df30-410c-95c3-3c7d8cf56f26", "node_type": "1", "metadata": {"window": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. ", "original_text": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. "}, "hash": "225d055ed31a706ad14c6abd4492c2b875b0bb76d32a76e1b1f016e9a6d6bd58", "class_name": "RelatedNodeInfo"}}, "hash": "d6946040cb1ea16d923278170a079e0483c8ae8c500bfe8ecbaf264178495cb0", "text": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). ", "start_char_idx": 1428, "end_char_idx": 1671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81d44862-df30-410c-95c3-3c7d8cf56f26": {"__data__": {"id_": "81d44862-df30-410c-95c3-3c7d8cf56f26", "embedding": null, "metadata": {"window": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. ", "original_text": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d061be7-7b18-46da-af7a-4ef12412709b", "node_type": "1", "metadata": {"window": "Extensive experiments have demonstrated its\nsuperiority over state-of-the-art approaches for both blind image super-resolution\nand blind face restoration tasks on synthetic and real-world datasets.  The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. ", "original_text": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling). "}, "hash": "d6946040cb1ea16d923278170a079e0483c8ae8c500bfe8ecbaf264178495cb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "878f74d8-f2eb-4a3e-9200-6047c5dc8e3f", "node_type": "1", "metadata": {"window": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n", "original_text": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. "}, "hash": "db4a3708b5a0f89e484aa58c8b112c94989ecd1a4d8b011d6dc1daf3400fb2f2", "class_name": "RelatedNodeInfo"}}, "hash": "225d055ed31a706ad14c6abd4492c2b875b0bb76d32a76e1b1f016e9a6d6bd58", "text": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. ", "start_char_idx": 1671, "end_char_idx": 1829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "878f74d8-f2eb-4a3e-9200-6047c5dc8e3f": {"__data__": {"id_": "878f74d8-f2eb-4a3e-9200-6047c5dc8e3f", "embedding": null, "metadata": {"window": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n", "original_text": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81d44862-df30-410c-95c3-3c7d8cf56f26", "node_type": "1", "metadata": {"window": "The code is\navailable at https://github.com/XPixelGroup/DiffBIR .\n 1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. ", "original_text": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability. "}, "hash": "225d055ed31a706ad14c6abd4492c2b875b0bb76d32a76e1b1f016e9a6d6bd58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c80912f-8ad6-4a0c-9c7f-0bff0150fc32", "node_type": "1", "metadata": {"window": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n", "original_text": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. "}, "hash": "f139509e4652a7ed204309a5952b02047d261d9a6445bea0e3fd8c2add419856", "class_name": "RelatedNodeInfo"}}, "hash": "db4a3708b5a0f89e484aa58c8b112c94989ecd1a4d8b011d6dc1daf3400fb2f2", "text": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. ", "start_char_idx": 1829, "end_char_idx": 1951, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c80912f-8ad6-4a0c-9c7f-0bff0150fc32": {"__data__": {"id_": "3c80912f-8ad6-4a0c-9c7f-0bff0150fc32", "embedding": null, "metadata": {"window": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n", "original_text": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "878f74d8-f2eb-4a3e-9200-6047c5dc8e3f", "node_type": "1", "metadata": {"window": "1 Introduction\nImage restoration aims at reconstructing a high-quality image from its low-quality observation.\n Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n", "original_text": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction. "}, "hash": "db4a3708b5a0f89e484aa58c8b112c94989ecd1a4d8b011d6dc1daf3400fb2f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b15c81e7-8222-42d7-b86f-9adb2ae16a23", "node_type": "1", "metadata": {"window": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). ", "original_text": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n"}, "hash": "95d12158034c627df66e7165f2a95ebba55b4260585edf680a3a70a45858a53a", "class_name": "RelatedNodeInfo"}}, "hash": "f139509e4652a7ed204309a5952b02047d261d9a6445bea0e3fd8c2add419856", "text": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. ", "start_char_idx": 1951, "end_char_idx": 2066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b15c81e7-8222-42d7-b86f-9adb2ae16a23": {"__data__": {"id_": "b15c81e7-8222-42d7-b86f-9adb2ae16a23", "embedding": null, "metadata": {"window": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). ", "original_text": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c80912f-8ad6-4a0c-9c7f-0bff0150fc32", "node_type": "1", "metadata": {"window": "Typical image restoration problems, such as image denoising, deblurring and super-resolution, are\nusually defined under a constrained setting, where the degradation process is simple and known ( e.g.,\nGaussian noise and Bicubic downsampling).  They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n", "original_text": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations. "}, "hash": "f139509e4652a7ed204309a5952b02047d261d9a6445bea0e3fd8c2add419856", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ce69935-20c6-448c-bf4e-163ea822990d", "node_type": "1", "metadata": {"window": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n", "original_text": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n"}, "hash": "fe943cb290b17e1572a6567a5b2cda3c196a9eb1e2cdf9e2a9de42b85eae980a", "class_name": "RelatedNodeInfo"}}, "hash": "95d12158034c627df66e7165f2a95ebba55b4260585edf680a3a70a45858a53a", "text": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n", "start_char_idx": 2066, "end_char_idx": 2225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ce69935-20c6-448c-bf4e-163ea822990d": {"__data__": {"id_": "0ce69935-20c6-448c-bf4e-163ea822990d", "embedding": null, "metadata": {"window": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n", "original_text": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b15c81e7-8222-42d7-b86f-9adb2ae16a23", "node_type": "1", "metadata": {"window": "They have successfully promoted a vast number of\nexcellent restoration algorithms [ 14;65;36;7;58;63;8], but are born to have limited generalization\nability.  To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). ", "original_text": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n"}, "hash": "95d12158034c627df66e7165f2a95ebba55b4260585edf680a3a70a45858a53a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce98d52d-904e-4928-93e9-a8d03ecbddcd", "node_type": "1", "metadata": {"window": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. ", "original_text": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). "}, "hash": "d57b997c11c392856a6842a6d28a57b7040c66d15cf8d1ee52ff67d49164f151", "class_name": "RelatedNodeInfo"}}, "hash": "fe943cb290b17e1572a6567a5b2cda3c196a9eb1e2cdf9e2a9de42b85eae980a", "text": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n", "start_char_idx": 2225, "end_char_idx": 2331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce98d52d-904e-4928-93e9-a8d03ecbddcd": {"__data__": {"id_": "ce98d52d-904e-4928-93e9-a8d03ecbddcd", "embedding": null, "metadata": {"window": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. ", "original_text": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ce69935-20c6-448c-bf4e-163ea822990d", "node_type": "1", "metadata": {"window": "To deal with real-world degraded images, blind image restoration (BIR) comes into view and\nbecomes a promising direction.  The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n", "original_text": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n"}, "hash": "fe943cb290b17e1572a6567a5b2cda3c196a9eb1e2cdf9e2a9de42b85eae980a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97413fd0-d6fd-4277-970d-e5a877e61e2f", "node_type": "1", "metadata": {"window": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. ", "original_text": "They all have achieved remarkable progress, but also have apparent limitations.\n"}, "hash": "287b946518a0d3f3de8c292e70eb950b65b146f77779d5cc6f6ee630d425cd7f", "class_name": "RelatedNodeInfo"}}, "hash": "d57b997c11c392856a6842a6d28a57b7040c66d15cf8d1ee52ff67d49164f151", "text": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). ", "start_char_idx": 2331, "end_char_idx": 2549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97413fd0-d6fd-4277-970d-e5a877e61e2f": {"__data__": {"id_": "97413fd0-d6fd-4277-970d-e5a877e61e2f", "embedding": null, "metadata": {"window": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. ", "original_text": "They all have achieved remarkable progress, but also have apparent limitations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce98d52d-904e-4928-93e9-a8d03ecbddcd", "node_type": "1", "metadata": {"window": "The ultimate goal of BIR is to realize realistic image reconstruction\non general images with general degradations.  BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. ", "original_text": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR). "}, "hash": "d57b997c11c392856a6842a6d28a57b7040c66d15cf8d1ee52ff67d49164f151", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d609659d-5cb6-4ba9-bc98-399080737c58", "node_type": "1", "metadata": {"window": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. ", "original_text": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. "}, "hash": "5907ee7dd59fe1786c67b45e02363daf57ea6fa7a3fff15c74c52fe633e05440", "class_name": "RelatedNodeInfo"}}, "hash": "287b946518a0d3f3de8c292e70eb950b65b146f77779d5cc6f6ee630d425cd7f", "text": "They all have achieved remarkable progress, but also have apparent limitations.\n", "start_char_idx": 2549, "end_char_idx": 2629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d609659d-5cb6-4ba9-bc98-399080737c58": {"__data__": {"id_": "d609659d-5cb6-4ba9-bc98-399080737c58", "embedding": null, "metadata": {"window": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. ", "original_text": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97413fd0-d6fd-4277-970d-e5a877e61e2f", "node_type": "1", "metadata": {"window": "BIR does not only extend the boundary of classic image\nrestoration tasks, but also has a wide practical application field ( e.g., old photo/film restoration).\n The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. ", "original_text": "They all have achieved remarkable progress, but also have apparent limitations.\n"}, "hash": "287b946518a0d3f3de8c292e70eb950b65b146f77779d5cc6f6ee630d425cd7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6f1e251-e584-4bda-b3d8-75e38b5a46d3", "node_type": "1", "metadata": {"window": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. ", "original_text": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. "}, "hash": "b75fb7a870d8fa630360c7d6276bf00873bafaf5512e13716705adcc4d3043e3", "class_name": "RelatedNodeInfo"}}, "hash": "5907ee7dd59fe1786c67b45e02363daf57ea6fa7a3fff15c74c52fe633e05440", "text": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. ", "start_char_idx": 2629, "end_char_idx": 2764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6f1e251-e584-4bda-b3d8-75e38b5a46d3": {"__data__": {"id_": "d6f1e251-e584-4bda-b3d8-75e38b5a46d3", "embedding": null, "metadata": {"window": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. ", "original_text": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d609659d-5cb6-4ba9-bc98-399080737c58", "node_type": "1", "metadata": {"window": "The research of BIR is still in its primary stage, thus requiring more explanations of its current state.\n According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. ", "original_text": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations. "}, "hash": "5907ee7dd59fe1786c67b45e02363daf57ea6fa7a3fff15c74c52fe633e05440", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c0b321f-7758-4ecf-9643-0e364cc6e45a", "node_type": "1", "metadata": {"window": "They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. ", "original_text": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. "}, "hash": "e27c043f1dceeda49dfeda178ffd139112e5e246537cf4d9b82c5e6d6eea45a4", "class_name": "RelatedNodeInfo"}}, "hash": "b75fb7a870d8fa630360c7d6276bf00873bafaf5512e13716705adcc4d3043e3", "text": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. ", "start_char_idx": 2764, "end_char_idx": 2876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c0b321f-7758-4ecf-9643-0e364cc6e45a": {"__data__": {"id_": "9c0b321f-7758-4ecf-9643-0e364cc6e45a", "embedding": null, "metadata": {"window": "They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. ", "original_text": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6f1e251-e584-4bda-b3d8-75e38b5a46d3", "node_type": "1", "metadata": {"window": "According to the problem settings, existing BIR methods can be roughly grouped into three research\ntopics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face\nrestoration (BFR).  They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. ", "original_text": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55]. "}, "hash": "b75fb7a870d8fa630360c7d6276bf00873bafaf5512e13716705adcc4d3043e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ecd85f8-dbaf-4b55-89e9-716765432483", "node_type": "1", "metadata": {"window": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. ", "original_text": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. "}, "hash": "1d0641b1c5f014f359c15705ec68b9b3c6871badffeb00f0486d936360ac07e7", "class_name": "RelatedNodeInfo"}}, "hash": "e27c043f1dceeda49dfeda178ffd139112e5e246537cf4d9b82c5e6d6eea45a4", "text": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. ", "start_char_idx": 2876, "end_char_idx": 2956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ecd85f8-dbaf-4b55-89e9-716765432483": {"__data__": {"id_": "0ecd85f8-dbaf-4b55-89e9-716765432483", "embedding": null, "metadata": {"window": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. ", "original_text": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c0b321f-7758-4ecf-9643-0e364cc6e45a", "node_type": "1", "metadata": {"window": "They all have achieved remarkable progress, but also have apparent limitations.\n BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. ", "original_text": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem. "}, "hash": "e27c043f1dceeda49dfeda178ffd139112e5e246537cf4d9b82c5e6d6eea45a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "664f34c4-55ba-4f85-ab87-6bd62c4a68f1", "node_type": "1", "metadata": {"window": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n", "original_text": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. "}, "hash": "06cd6e844b5d7ed3f5841a56ee3381112ad55b8edbaec3883fde9617deb19bf9", "class_name": "RelatedNodeInfo"}}, "hash": "1d0641b1c5f014f359c15705ec68b9b3c6871badffeb00f0486d936360ac07e7", "text": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. ", "start_char_idx": 2956, "end_char_idx": 3085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "664f34c4-55ba-4f85-ab87-6bd62c4a68f1": {"__data__": {"id_": "664f34c4-55ba-4f85-ab87-6bd62c4a68f1", "embedding": null, "metadata": {"window": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n", "original_text": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ecd85f8-dbaf-4b55-89e9-716765432483", "node_type": "1", "metadata": {"window": "BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution\nimage contains unknown degradations.  According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. ", "original_text": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately. "}, "hash": "1d0641b1c5f014f359c15705ec68b9b3c6871badffeb00f0486d936360ac07e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbab38fa-d062-4e81-9927-37234979f613", "node_type": "1", "metadata": {"window": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n", "original_text": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. "}, "hash": "452823db6d635acf64d3d30eb8a4f32c041691f6285c0715c33d1f796b6db3f6", "class_name": "RelatedNodeInfo"}}, "hash": "06cd6e844b5d7ed3f5841a56ee3381112ad55b8edbaec3883fde9617deb19bf9", "text": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. ", "start_char_idx": 3085, "end_char_idx": 3210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbab38fa-d062-4e81-9927-37234979f613": {"__data__": {"id_": "cbab38fa-d062-4e81-9927-37234979f613", "embedding": null, "metadata": {"window": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n", "original_text": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "664f34c4-55ba-4f85-ab87-6bd62c4a68f1", "node_type": "1", "metadata": {"window": "According to the recent BSR survey [ 37], the most popular\nsolutions may be BSRGAN [ 64] and Real-ESRGAN [ 55].  They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n", "original_text": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner. "}, "hash": "06cd6e844b5d7ed3f5841a56ee3381112ad55b8edbaec3883fde9617deb19bf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "183b765f-6eed-470b-97f3-9117d6259e52", "node_type": "1", "metadata": {"window": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. ", "original_text": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n"}, "hash": "065d5c68670fa7b82ee51d4d65d37abf88cb6b5a094eecb11a2a2f6538cd2403", "class_name": "RelatedNodeInfo"}}, "hash": "452823db6d635acf64d3d30eb8a4f32c041691f6285c0715c33d1f796b6db3f6", "text": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. ", "start_char_idx": 3210, "end_char_idx": 3344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "183b765f-6eed-470b-97f3-9117d6259e52": {"__data__": {"id_": "183b765f-6eed-470b-97f3-9117d6259e52", "embedding": null, "metadata": {"window": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. ", "original_text": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbab38fa-d062-4e81-9927-37234979f613", "node_type": "1", "metadata": {"window": "They formulate BSR as a supervised\nlarge-scale degradation overfitting problem.  To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n", "original_text": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. "}, "hash": "452823db6d635acf64d3d30eb8a4f32c041691f6285c0715c33d1f796b6db3f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abeaa74d-f8f1-4534-a241-1403569a365d", "node_type": "1", "metadata": {"window": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. ", "original_text": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n"}, "hash": "0c40a7752b13bbb336d9e2c2d15d0ed081ea5d4f91f628a4fc704d4c7cb1063d", "class_name": "RelatedNodeInfo"}}, "hash": "065d5c68670fa7b82ee51d4d65d37abf88cb6b5a094eecb11a2a2f6538cd2403", "text": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n", "start_char_idx": 3344, "end_char_idx": 3501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abeaa74d-f8f1-4534-a241-1403569a365d": {"__data__": {"id_": "abeaa74d-f8f1-4534-a241-1403569a365d", "embedding": null, "metadata": {"window": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. ", "original_text": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "183b765f-6eed-470b-97f3-9117d6259e52", "node_type": "1", "metadata": {"window": "To simulate real-world degradations, a degradation\nshuffle strategy and high-order degradation modeling are proposed separately.  Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. ", "original_text": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n"}, "hash": "065d5c68670fa7b82ee51d4d65d37abf88cb6b5a094eecb11a2a2f6538cd2403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a5d1693-cbd7-4bbc-8344-0c99b4422256", "node_type": "1", "metadata": {"window": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). ", "original_text": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. "}, "hash": "daacda500377ceffeb67384f045cd70eb4e783c15fc6dfd18759cf049bbf703d", "class_name": "RelatedNodeInfo"}}, "hash": "0c40a7752b13bbb336d9e2c2d15d0ed081ea5d4f91f628a4fc704d4c7cb1063d", "text": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n", "start_char_idx": 3501, "end_char_idx": 3602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a5d1693-cbd7-4bbc-8344-0c99b4422256": {"__data__": {"id_": "0a5d1693-cbd7-4bbc-8344-0c99b4422256", "embedding": null, "metadata": {"window": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). ", "original_text": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abeaa74d-f8f1-4534-a241-1403569a365d", "node_type": "1", "metadata": {"window": "Then the adversarial\nloss [ 31;17;56;41;49] is incorporated for learning the reconstruction process in an end-to-end\nmanner.  They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. ", "original_text": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n"}, "hash": "0c40a7752b13bbb336d9e2c2d15d0ed081ea5d4f91f628a4fc704d4c7cb1063d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cde80f3-7689-45fe-a5a2-0f45ac271e0b", "node_type": "1", "metadata": {"window": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details. ", "original_text": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. "}, "hash": "0efeec3879bf6255612fdda21f68639b7350696a1765623510415a77ba510896", "class_name": "RelatedNodeInfo"}}, "hash": "daacda500377ceffeb67384f045cd70eb4e783c15fc6dfd18759cf049bbf703d", "text": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. ", "start_char_idx": 3602, "end_char_idx": 3694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cde80f3-7689-45fe-a5a2-0f45ac271e0b": {"__data__": {"id_": "5cde80f3-7689-45fe-a5a2-0f45ac271e0b", "embedding": null, "metadata": {"window": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details. ", "original_text": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a5d1693-cbd7-4bbc-8344-0c99b4422256", "node_type": "1", "metadata": {"window": "They have indeed removed most degradations on general images, but cannot generate\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint.  Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). ", "original_text": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images. "}, "hash": "daacda500377ceffeb67384f045cd70eb4e783c15fc6dfd18759cf049bbf703d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "260c04c9-89e8-4ed3-8031-ced614c6c6b3", "node_type": "1", "metadata": {"window": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. ", "original_text": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). "}, "hash": "2f4fa73607dbc3da6cd07a9a092c3c12cacf7f24998a9c944a452a05981bc2d6", "class_name": "RelatedNodeInfo"}}, "hash": "0efeec3879bf6255612fdda21f68639b7350696a1765623510415a77ba510896", "text": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. ", "start_char_idx": 3694, "end_char_idx": 3865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "260c04c9-89e8-4ed3-8031-ced614c6c6b3": {"__data__": {"id_": "260c04c9-89e8-4ed3-8031-ced614c6c6b3", "embedding": null, "metadata": {"window": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. ", "original_text": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cde80f3-7689-45fe-a5a2-0f45ac271e0b", "node_type": "1", "metadata": {"window": "Under review.arXiv:2308.15070v1  [cs.CV]  29 Aug 2023\n\n(a) Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.\n (b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details. ", "original_text": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases. "}, "hash": "0efeec3879bf6255612fdda21f68639b7350696a1765623510415a77ba510896", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0217ff6-b34e-419b-8cf7-7d8251b47bb7", "node_type": "1", "metadata": {"window": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n", "original_text": "( Zoom in for best view )\nrealistic details. "}, "hash": "86b98acf07cf8c93681944e3635bfff346c43d6f147a0c7b35764f627acd0227", "class_name": "RelatedNodeInfo"}}, "hash": "2f4fa73607dbc3da6cd07a9a092c3c12cacf7f24998a9c944a452a05981bc2d6", "text": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). ", "start_char_idx": 3865, "end_char_idx": 4013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0217ff6-b34e-419b-8cf7-7d8251b47bb7": {"__data__": {"id_": "c0217ff6-b34e-419b-8cf7-7d8251b47bb7", "embedding": null, "metadata": {"window": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n", "original_text": "( Zoom in for best view )\nrealistic details. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260c04c9-89e8-4ed3-8031-ced614c6c6b3", "node_type": "1", "metadata": {"window": "(b) Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.\n Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. ", "original_text": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings). "}, "hash": "2f4fa73607dbc3da6cd07a9a092c3c12cacf7f24998a9c944a452a05981bc2d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "254b2434-cdc3-4bb2-8694-b99f3485c28c", "node_type": "1", "metadata": {"window": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. ", "original_text": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. "}, "hash": "50b95cbd796c585232c6498992d9859dc2239b2c59e3575595be8ca4ffa6bd7e", "class_name": "RelatedNodeInfo"}}, "hash": "86b98acf07cf8c93681944e3635bfff346c43d6f147a0c7b35764f627acd0227", "text": "( Zoom in for best view )\nrealistic details. ", "start_char_idx": 4013, "end_char_idx": 4058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "254b2434-cdc3-4bb2-8694-b99f3485c28c": {"__data__": {"id_": "254b2434-cdc3-4bb2-8694-b99f3485c28c", "embedding": null, "metadata": {"window": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. ", "original_text": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0217ff6-b34e-419b-8cf7-7d8251b47bb7", "node_type": "1", "metadata": {"window": "Figure 1: Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n", "original_text": "( Zoom in for best view )\nrealistic details. "}, "hash": "86b98acf07cf8c93681944e3635bfff346c43d6f147a0c7b35764f627acd0227", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "379ad591-952e-4f83-bc9b-07f1c2508045", "node_type": "1", "metadata": {"window": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n", "original_text": "The second group ZIR is a newly emerged direction.\n"}, "hash": "bea172b2808b4ea589abd4f656ce311a8295b8b336b838738fcda392aad6f1d6", "class_name": "RelatedNodeInfo"}}, "hash": "50b95cbd796c585232c6498992d9859dc2239b2c59e3575595be8ca4ffa6bd7e", "text": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. ", "start_char_idx": 4058, "end_char_idx": 4179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "379ad591-952e-4f83-bc9b-07f1c2508045": {"__data__": {"id_": "379ad591-952e-4f83-bc9b-07f1c2508045", "embedding": null, "metadata": {"window": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n", "original_text": "The second group ZIR is a newly emerged direction.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "254b2434-cdc3-4bb2-8694-b99f3485c28c", "node_type": "1", "metadata": {"window": "Compared to\nBSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not\nerase small details; 4) overcome severe cases.  Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. ", "original_text": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem. "}, "hash": "50b95cbd796c585232c6498992d9859dc2239b2c59e3575595be8ca4ffa6bd7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daf2519e-01f9-4177-b2c1-eb77cba5c722", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. ", "original_text": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. "}, "hash": "2d9a4b7eec9e9a9532d4b75a8eaa600afe904cbd8e5b49a2ce5e9fb090d999c7", "class_name": "RelatedNodeInfo"}}, "hash": "bea172b2808b4ea589abd4f656ce311a8295b8b336b838738fcda392aad6f1d6", "text": "The second group ZIR is a newly emerged direction.\n", "start_char_idx": 4179, "end_char_idx": 4230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "daf2519e-01f9-4177-b2c1-eb77cba5c722": {"__data__": {"id_": "daf2519e-01f9-4177-b2c1-eb77cba5c722", "embedding": null, "metadata": {"window": "( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. ", "original_text": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "379ad591-952e-4f83-bc9b-07f1c2508045", "node_type": "1", "metadata": {"window": "Compared to BFR methods, DiffBIR can 1) handle occlusion\ncases; 2) obtain satisfactory restoration beyond facial areas ( e.g., headwear, earrings).  ( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n", "original_text": "The second group ZIR is a newly emerged direction.\n"}, "hash": "bea172b2808b4ea589abd4f656ce311a8295b8b336b838738fcda392aad6f1d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52be0b8c-ee5b-4aef-a516-2ab212d4aa55", "node_type": "1", "metadata": {"window": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR. ", "original_text": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n"}, "hash": "1eecd19f71b599c6802837dd932d8e8dcdd66d31cae636131585a170fcf39c4f", "class_name": "RelatedNodeInfo"}}, "hash": "2d9a4b7eec9e9a9532d4b75a8eaa600afe904cbd8e5b49a2ce5e9fb090d999c7", "text": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. ", "start_char_idx": 4230, "end_char_idx": 4294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52be0b8c-ee5b-4aef-a516-2ab212d4aa55": {"__data__": {"id_": "52be0b8c-ee5b-4aef-a516-2ab212d4aa55", "embedding": null, "metadata": {"window": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR. ", "original_text": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "daf2519e-01f9-4177-b2c1-eb77cba5c722", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nrealistic details.  Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. ", "original_text": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16]. "}, "hash": "2d9a4b7eec9e9a9532d4b75a8eaa600afe904cbd8e5b49a2ce5e9fb090d999c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6327bac8-7c0d-4084-83cb-1f71dbfb0d87", "node_type": "1", "metadata": {"window": "The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. ", "original_text": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. "}, "hash": "d965c42643bec9c3c10d0029e5b3f59f03684b6623e3ec151786ff6bdfc521ff", "class_name": "RelatedNodeInfo"}}, "hash": "1eecd19f71b599c6802837dd932d8e8dcdd66d31cae636131585a170fcf39c4f", "text": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n", "start_char_idx": 4294, "end_char_idx": 4427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6327bac8-7c0d-4084-83cb-1f71dbfb0d87": {"__data__": {"id_": "6327bac8-7c0d-4084-83cb-1f71dbfb0d87", "embedding": null, "metadata": {"window": "The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. ", "original_text": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52be0b8c-ee5b-4aef-a516-2ab212d4aa55", "node_type": "1", "metadata": {"window": "Furthermore, their degradation settings are limited to \u00d74/\u00d78super-resolution,\nwhich is not complete for the BIR problem.  The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR. ", "original_text": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n"}, "hash": "1eecd19f71b599c6802837dd932d8e8dcdd66d31cae636131585a170fcf39c4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aeee6eeb-44a0-4739-88f2-8e1edef5dc7b", "node_type": "1", "metadata": {"window": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. ", "original_text": "However, the problem setting of ZIR is not in accordance with BIR. "}, "hash": "1382fb6033d1a9e96261457b86f5468917cf9903a36ffc571000bb248507a62a", "class_name": "RelatedNodeInfo"}}, "hash": "d965c42643bec9c3c10d0029e5b3f59f03684b6623e3ec151786ff6bdfc521ff", "text": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. ", "start_char_idx": 4427, "end_char_idx": 4536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aeee6eeb-44a0-4739-88f2-8e1edef5dc7b": {"__data__": {"id_": "aeee6eeb-44a0-4739-88f2-8e1edef5dc7b", "embedding": null, "metadata": {"window": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. ", "original_text": "However, the problem setting of ZIR is not in accordance with BIR. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6327bac8-7c0d-4084-83cb-1f71dbfb0d87", "node_type": "1", "metadata": {"window": "The second group ZIR is a newly emerged direction.\n Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. ", "original_text": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks. "}, "hash": "d965c42643bec9c3c10d0029e5b3f59f03684b6623e3ec151786ff6bdfc521ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2773337b-1970-4419-a4a4-41afe0252c8c", "node_type": "1", "metadata": {"window": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n", "original_text": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. "}, "hash": "039b58c4f960df2097d021365682c8106a58e9b81fc4522445c2575bb5d54968", "class_name": "RelatedNodeInfo"}}, "hash": "1382fb6033d1a9e96261457b86f5468917cf9903a36ffc571000bb248507a62a", "text": "However, the problem setting of ZIR is not in accordance with BIR. ", "start_char_idx": 4536, "end_char_idx": 4603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2773337b-1970-4419-a4a4-41afe0252c8c": {"__data__": {"id_": "2773337b-1970-4419-a4a4-41afe0252c8c", "embedding": null, "metadata": {"window": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n", "original_text": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aeee6eeb-44a0-4739-88f2-8e1edef5dc7b", "node_type": "1", "metadata": {"window": "Representative works are DDRM [ 26], DDNM [ 57], and GDP [ 16].  They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. ", "original_text": "However, the problem setting of ZIR is not in accordance with BIR. "}, "hash": "1382fb6033d1a9e96261457b86f5468917cf9903a36ffc571000bb248507a62a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7658497d-6e31-48a6-a62b-f2ec2a7c5ca9", "node_type": "1", "metadata": {"window": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. ", "original_text": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. "}, "hash": "ee17b7c4f108c74d9b4bd9233e6b524f517d30972e71cad5474d7defceded71e", "class_name": "RelatedNodeInfo"}}, "hash": "039b58c4f960df2097d021365682c8106a58e9b81fc4522445c2575bb5d54968", "text": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. ", "start_char_idx": 4603, "end_char_idx": 4741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7658497d-6e31-48a6-a62b-f2ec2a7c5ca9": {"__data__": {"id_": "7658497d-6e31-48a6-a62b-f2ec2a7c5ca9", "embedding": null, "metadata": {"window": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. ", "original_text": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2773337b-1970-4419-a4a4-41afe0252c8c", "node_type": "1", "metadata": {"window": "They incorporate the powerful\ndiffusion model as the additional prior, thus having greater generative ability than GAN-base methods.\n With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n", "original_text": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations. "}, "hash": "039b58c4f960df2097d021365682c8106a58e9b81fc4522445c2575bb5d54968", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac532669-0517-4549-bafb-a9586ffb2de4", "node_type": "1", "metadata": {"window": "However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. ", "original_text": "The third group is BFR, which focuses on human face restoration.\n"}, "hash": "bda0b3ea77009f36c008a6c48ffa91dc37276fc0d14db059e4695f5894bfa8b1", "class_name": "RelatedNodeInfo"}}, "hash": "ee17b7c4f108c74d9b4bd9233e6b524f517d30972e71cad5474d7defceded71e", "text": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. ", "start_char_idx": 4741, "end_char_idx": 4851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac532669-0517-4549-bafb-a9586ffb2de4": {"__data__": {"id_": "ac532669-0517-4549-bafb-a9586ffb2de4", "embedding": null, "metadata": {"window": "However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. ", "original_text": "The third group is BFR, which focuses on human face restoration.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7658497d-6e31-48a6-a62b-f2ec2a7c5ca9", "node_type": "1", "metadata": {"window": "With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic\nIR tasks.  However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. ", "original_text": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations. "}, "hash": "ee17b7c4f108c74d9b4bd9233e6b524f517d30972e71cad5474d7defceded71e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41b6e169-cbbc-47a7-9722-c28ba70d5afa", "node_type": "1", "metadata": {"window": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. ", "original_text": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. "}, "hash": "45d9e310e3c950c2caebc95169e11db3d60bc074910c2c7da0e402d5044db8bd", "class_name": "RelatedNodeInfo"}}, "hash": "bda0b3ea77009f36c008a6c48ffa91dc37276fc0d14db059e4695f5894bfa8b1", "text": "The third group is BFR, which focuses on human face restoration.\n", "start_char_idx": 4851, "end_char_idx": 4916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41b6e169-cbbc-47a7-9722-c28ba70d5afa": {"__data__": {"id_": "41b6e169-cbbc-47a7-9722-c28ba70d5afa", "embedding": null, "metadata": {"window": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. ", "original_text": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac532669-0517-4549-bafb-a9586ffb2de4", "node_type": "1", "metadata": {"window": "However, the problem setting of ZIR is not in accordance with BIR.  Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. ", "original_text": "The third group is BFR, which focuses on human face restoration.\n"}, "hash": "bda0b3ea77009f36c008a6c48ffa91dc37276fc0d14db059e4695f5894bfa8b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d13f11f-b9cc-46ad-8d01-152018f095f3", "node_type": "1", "metadata": {"window": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR. ", "original_text": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. "}, "hash": "acde9f441bb5dd348e121a2328a94d05843b2ff55cdafb26bd1470d19dac8e8d", "class_name": "RelatedNodeInfo"}}, "hash": "45d9e310e3c950c2caebc95169e11db3d60bc074910c2c7da0e402d5044db8bd", "text": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. ", "start_char_idx": 4916, "end_char_idx": 4987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d13f11f-b9cc-46ad-8d01-152018f095f3": {"__data__": {"id_": "3d13f11f-b9cc-46ad-8d01-152018f095f3", "embedding": null, "metadata": {"window": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR. ", "original_text": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41b6e169-cbbc-47a7-9722-c28ba70d5afa", "node_type": "1", "metadata": {"window": "Their methods can\nonly deal with clearly defined degradations (linear or non-linear), but cannot generalize well to\nunknown degradations.  In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. ", "original_text": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18]. "}, "hash": "45d9e310e3c950c2caebc95169e11db3d60bc074910c2c7da0e402d5044db8bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffb5a522-16c7-4b7a-8769-9d2eafe99a9b", "node_type": "1", "metadata": {"window": "The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n", "original_text": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. "}, "hash": "ecc151e7a1d2e99c500d45ae6b19d9b4485eccc087442b86f972c9befad6b835", "class_name": "RelatedNodeInfo"}}, "hash": "acde9f441bb5dd348e121a2328a94d05843b2ff55cdafb26bd1470d19dac8e8d", "text": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. ", "start_char_idx": 4987, "end_char_idx": 5108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffb5a522-16c7-4b7a-8769-9d2eafe99a9b": {"__data__": {"id_": "ffb5a522-16c7-4b7a-8769-9d2eafe99a9b", "embedding": null, "metadata": {"window": "The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n", "original_text": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d13f11f-b9cc-46ad-8d01-152018f095f3", "node_type": "1", "metadata": {"window": "In other words, they can achieve realistic reconstruction on general images,\nbut not on general degradations.  The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR. ", "original_text": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network. "}, "hash": "acde9f441bb5dd348e121a2328a94d05843b2ff55cdafb26bd1470d19dac8e8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8eb0c6bd-c0bf-4c08-8d93-c32405ae5c8b", "node_type": "1", "metadata": {"window": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. ", "original_text": "Nevertheless, BFR is only a sub-domain of BIR. "}, "hash": "44c7ad8abb56f862ce2f181f14879c456559c0e38262da523d7c20ec25cefbe7", "class_name": "RelatedNodeInfo"}}, "hash": "ecc151e7a1d2e99c500d45ae6b19d9b4485eccc087442b86f972c9befad6b835", "text": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. ", "start_char_idx": 5108, "end_char_idx": 5250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8eb0c6bd-c0bf-4c08-8d93-c32405ae5c8b": {"__data__": {"id_": "8eb0c6bd-c0bf-4c08-8d93-c32405ae5c8b", "embedding": null, "metadata": {"window": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. ", "original_text": "Nevertheless, BFR is only a sub-domain of BIR. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffb5a522-16c7-4b7a-8769-9d2eafe99a9b", "node_type": "1", "metadata": {"window": "The third group is BFR, which focuses on human face restoration.\n State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n", "original_text": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images. "}, "hash": "ecc151e7a1d2e99c500d45ae6b19d9b4485eccc087442b86f972c9befad6b835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa4abd91-d3be-47cf-8d6a-83ac560d7751", "node_type": "1", "metadata": {"window": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n", "original_text": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n"}, "hash": "6d8ff0369507b4e366bc9027019de5f8f1b0e87786600bc19c83839d6590a8f3", "class_name": "RelatedNodeInfo"}}, "hash": "44c7ad8abb56f862ce2f181f14879c456559c0e38262da523d7c20ec25cefbe7", "text": "Nevertheless, BFR is only a sub-domain of BIR. ", "start_char_idx": 5250, "end_char_idx": 5297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa4abd91-d3be-47cf-8d6a-83ac560d7751": {"__data__": {"id_": "aa4abd91-d3be-47cf-8d6a-83ac560d7751", "embedding": null, "metadata": {"window": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n", "original_text": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8eb0c6bd-c0bf-4c08-8d93-c32405ae5c8b", "node_type": "1", "metadata": {"window": "State-of-the-art methods can refer to CodeFormer [ 68] and VQFR [ 18].  They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. ", "original_text": "Nevertheless, BFR is only a sub-domain of BIR. "}, "hash": "44c7ad8abb56f862ce2f181f14879c456559c0e38262da523d7c20ec25cefbe7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc4b1c60-0e08-4821-b3ae-31fd4d27191e", "node_type": "1", "metadata": {"window": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. ", "original_text": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. "}, "hash": "810077761cbbfc1976bfc412470d705a5bb5d8882ff515ef2c253a4486029458", "class_name": "RelatedNodeInfo"}}, "hash": "6d8ff0369507b4e366bc9027019de5f8f1b0e87786600bc19c83839d6590a8f3", "text": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n", "start_char_idx": 5297, "end_char_idx": 5408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc4b1c60-0e08-4821-b3ae-31fd4d27191e": {"__data__": {"id_": "dc4b1c60-0e08-4821-b3ae-31fd4d27191e", "embedding": null, "metadata": {"window": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. ", "original_text": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa4abd91-d3be-47cf-8d6a-83ac560d7751", "node_type": "1", "metadata": {"window": "They have a similar solution\npipeline as BSR methods, but are different in the degradation model and generation network.  Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n", "original_text": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n"}, "hash": "6d8ff0369507b4e366bc9027019de5f8f1b0e87786600bc19c83839d6590a8f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb327243-c8a4-4c59-b415-25c9ae61b3d9", "node_type": "1", "metadata": {"window": "Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. ", "original_text": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n"}, "hash": "a4de15acf03936ee08401bccf12785f6f9638e5ba56e10c2660d16892f5f12d7", "class_name": "RelatedNodeInfo"}}, "hash": "810077761cbbfc1976bfc412470d705a5bb5d8882ff515ef2c253a4486029458", "text": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. ", "start_char_idx": 5408, "end_char_idx": 5597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb327243-c8a4-4c59-b415-25c9ae61b3d9": {"__data__": {"id_": "bb327243-c8a4-4c59-b415-25c9ae61b3d9", "embedding": null, "metadata": {"window": "Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. ", "original_text": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc4b1c60-0e08-4821-b3ae-31fd4d27191e", "node_type": "1", "metadata": {"window": "Due to\na smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly\ngood results on real-world face images.  Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. ", "original_text": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously. "}, "hash": "810077761cbbfc1976bfc412470d705a5bb5d8882ff515ef2c253a4486029458", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ada15e1-1706-4b99-8de7-bfeb93ac7f24", "node_type": "1", "metadata": {"window": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies. ", "original_text": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. "}, "hash": "eaddd98796c6e97f8069d8c302344310682a12a85db78176620e4f3eb4ff4778", "class_name": "RelatedNodeInfo"}}, "hash": "a4de15acf03936ee08401bccf12785f6f9638e5ba56e10c2660d16892f5f12d7", "text": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n", "start_char_idx": 5597, "end_char_idx": 5666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ada15e1-1706-4b99-8de7-bfeb93ac7f24": {"__data__": {"id_": "7ada15e1-1706-4b99-8de7-bfeb93ac7f24", "embedding": null, "metadata": {"window": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies. ", "original_text": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb327243-c8a4-4c59-b415-25c9ae61b3d9", "node_type": "1", "metadata": {"window": "Nevertheless, BFR is only a sub-domain of BIR.  It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. ", "original_text": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n"}, "hash": "a4de15acf03936ee08401bccf12785f6f9638e5ba56e10c2660d16892f5f12d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8d4dc0c-3e90-4cf1-8ada-36bab89b747b", "node_type": "1", "metadata": {"window": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. ", "original_text": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. "}, "hash": "1e98729cead8c0298d77b94a4b97338ff56695578e196f4330788a876bbbe250", "class_name": "RelatedNodeInfo"}}, "hash": "eaddd98796c6e97f8069d8c302344310682a12a85db78176620e4f3eb4ff4778", "text": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. ", "start_char_idx": 5666, "end_char_idx": 5771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8d4dc0c-3e90-4cf1-8ada-36bab89b747b": {"__data__": {"id_": "e8d4dc0c-3e90-4cf1-8ada-36bab89b747b", "embedding": null, "metadata": {"window": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. ", "original_text": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ada15e1-1706-4b99-8de7-bfeb93ac7f24", "node_type": "1", "metadata": {"window": "It usually\n2\n\nassumes a fixed input size and restricted image space, thus cannot be applied to general images.\n According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies. ", "original_text": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework. "}, "hash": "eaddd98796c6e97f8069d8c302344310682a12a85db78176620e4f3eb4ff4778", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb707d36-c930-45dc-b610-aeef6228e73a", "node_type": "1", "metadata": {"window": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases. ", "original_text": "We\nalso make dedicated designs to realize these strategies. "}, "hash": "1695b8aa9ad01c9fa4a8299bcf5dc98ed751af6a969db80c9eb9e5346b1408f1", "class_name": "RelatedNodeInfo"}}, "hash": "1e98729cead8c0298d77b94a4b97338ff56695578e196f4330788a876bbbe250", "text": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. ", "start_char_idx": 5771, "end_char_idx": 6056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb707d36-c930-45dc-b610-aeef6228e73a": {"__data__": {"id_": "bb707d36-c930-45dc-b610-aeef6228e73a", "embedding": null, "metadata": {"window": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases. ", "original_text": "We\nalso make dedicated designs to realize these strategies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8d4dc0c-3e90-4cf1-8ada-36bab89b747b", "node_type": "1", "metadata": {"window": "According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic\nimage reconstruction on (2) general images with (3) general degradations, simultaneously.  Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. ", "original_text": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. "}, "hash": "1e98729cead8c0298d77b94a4b97338ff56695578e196f4330788a876bbbe250", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51e316c2-47fd-4b28-a986-2a8d1a32f0e8", "node_type": "1", "metadata": {"window": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. ", "original_text": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. "}, "hash": "6acd7610e1c83370891b664effa213cef4918ee99848df40d8cf0e9a5f73ca4e", "class_name": "RelatedNodeInfo"}}, "hash": "1695b8aa9ad01c9fa4a8299bcf5dc98ed751af6a969db80c9eb9e5346b1408f1", "text": "We\nalso make dedicated designs to realize these strategies. ", "start_char_idx": 6056, "end_char_idx": 6116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51e316c2-47fd-4b28-a986-2a8d1a32f0e8": {"__data__": {"id_": "51e316c2-47fd-4b28-a986-2a8d1a32f0e8", "embedding": null, "metadata": {"window": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. ", "original_text": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb707d36-c930-45dc-b610-aeef6228e73a", "node_type": "1", "metadata": {"window": "Therefore,\nwe desire a new BIR method to overcome these limitations.\n In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases. ", "original_text": "We\nalso make dedicated designs to realize these strategies. "}, "hash": "1695b8aa9ad01c9fa4a8299bcf5dc98ed751af6a969db80c9eb9e5346b1408f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e223b2f-e38c-4ca5-a0fe-e486da202ed4", "node_type": "1", "metadata": {"window": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. ", "original_text": "This helps DiffBIR to handle diverse and extreme degradation\ncases. "}, "hash": "e3567ec0980eeb470c2c87c96b65fa6c24e1729ca9d830c9924bb917f91cb27f", "class_name": "RelatedNodeInfo"}}, "hash": "6acd7610e1c83370891b664effa213cef4918ee99848df40d8cf0e9a5f73ca4e", "text": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. ", "start_char_idx": 6116, "end_char_idx": 6295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e223b2f-e38c-4ca5-a0fe-e486da202ed4": {"__data__": {"id_": "8e223b2f-e38c-4ca5-a0fe-e486da202ed4", "embedding": null, "metadata": {"window": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. ", "original_text": "This helps DiffBIR to handle diverse and extreme degradation\ncases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51e316c2-47fd-4b28-a986-2a8d1a32f0e8", "node_type": "1", "metadata": {"window": "In this work, we propose DiffBIR to integrate the advantages of previous works into a unified\nframework.  Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. ", "original_text": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model. "}, "hash": "6acd7610e1c83370891b664effa213cef4918ee99848df40d8cf0e9a5f73ca4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "790146e6-db42-4ac4-a7c1-7dd8773931fc", "node_type": "1", "metadata": {"window": "We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n", "original_text": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. "}, "hash": "299e54aae61bdc5054153303ccc5305f273f2c9f4f241743ac1ee2556edc3ab2", "class_name": "RelatedNodeInfo"}}, "hash": "e3567ec0980eeb470c2c87c96b65fa6c24e1729ca9d830c9924bb917f91cb27f", "text": "This helps DiffBIR to handle diverse and extreme degradation\ncases. ", "start_char_idx": 6295, "end_char_idx": 6363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "790146e6-db42-4ac4-a7c1-7dd8773931fc": {"__data__": {"id_": "790146e6-db42-4ac4-a7c1-7dd8773931fc", "embedding": null, "metadata": {"window": "We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n", "original_text": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e223b2f-e38c-4ca5-a0fe-e486da202ed4", "node_type": "1", "metadata": {"window": "Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize\nto real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve\ngenerative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity.  We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. ", "original_text": "This helps DiffBIR to handle diverse and extreme degradation\ncases. "}, "hash": "e3567ec0980eeb470c2c87c96b65fa6c24e1729ca9d830c9924bb917f91cb27f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84516289-b954-44cc-92cc-e080c0196b4e", "node_type": "1", "metadata": {"window": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). ", "original_text": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. "}, "hash": "90488b359c793ad54796880cdcaf12a16511882a85d6dd1ee9d08be4e462a658", "class_name": "RelatedNodeInfo"}}, "hash": "299e54aae61bdc5054153303ccc5305f273f2c9f4f241743ac1ee2556edc3ab2", "text": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. ", "start_char_idx": 6363, "end_char_idx": 6510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84516289-b954-44cc-92cc-e080c0196b4e": {"__data__": {"id_": "84516289-b954-44cc-92cc-e080c0196b4e", "embedding": null, "metadata": {"window": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). ", "original_text": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "790146e6-db42-4ac4-a7c1-7dd8773931fc", "node_type": "1", "metadata": {"window": "We\nalso make dedicated designs to realize these strategies.  First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n", "original_text": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task. "}, "hash": "299e54aae61bdc5054153303ccc5305f273f2c9f4f241743ac1ee2556edc3ab2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3df342d2-23b1-492d-97a5-de874e9371ec", "node_type": "1", "metadata": {"window": "This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. ", "original_text": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n"}, "hash": "79fc1ce1640a069d5b2a7fa84fb7e5c9889644a7440d5f4dfa115af6ea3c8664", "class_name": "RelatedNodeInfo"}}, "hash": "90488b359c793ad54796880cdcaf12a16511882a85d6dd1ee9d08be4e462a658", "text": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. ", "start_char_idx": 6510, "end_char_idx": 6622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3df342d2-23b1-492d-97a5-de874e9371ec": {"__data__": {"id_": "3df342d2-23b1-492d-97a5-de874e9371ec", "embedding": null, "metadata": {"window": "This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. ", "original_text": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84516289-b954-44cc-92cc-e080c0196b4e", "node_type": "1", "metadata": {"window": "First, to increase generalization ability, we\ncombine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate\na more practical degradation model.  This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). ", "original_text": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability. "}, "hash": "90488b359c793ad54796880cdcaf12a16511882a85d6dd1ee9d08be4e462a658", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c872629b-d5b5-4cc3-9c87-954a38042f3d", "node_type": "1", "metadata": {"window": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. ", "original_text": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). "}, "hash": "244c046efb62d0118b250e80aaa43e373a61270eef7a651d10fd8a6398b444e7", "class_name": "RelatedNodeInfo"}}, "hash": "79fc1ce1640a069d5b2a7fa84fb7e5c9889644a7440d5f4dfa115af6ea3c8664", "text": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n", "start_char_idx": 6622, "end_char_idx": 6854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c872629b-d5b5-4cc3-9c87-954a38042f3d": {"__data__": {"id_": "c872629b-d5b5-4cc3-9c87-954a38042f3d", "embedding": null, "metadata": {"window": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. ", "original_text": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3df342d2-23b1-492d-97a5-de874e9371ec", "node_type": "1", "metadata": {"window": "This helps DiffBIR to handle diverse and extreme degradation\ncases.  Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. ", "original_text": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n"}, "hash": "79fc1ce1640a069d5b2a7fa84fb7e5c9889644a7440d5f4dfa115af6ea3c8664", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57a41937-44b4-43dc-ab96-cf11ab655018", "node_type": "1", "metadata": {"window": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n", "original_text": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. "}, "hash": "d449bac1a7874bdb4d1046cb3bc788cd6b6637ac1d42b3b7e1d5be66bcbd31ee", "class_name": "RelatedNodeInfo"}}, "hash": "244c046efb62d0118b250e80aaa43e373a61270eef7a651d10fd8a6398b444e7", "text": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). ", "start_char_idx": 6854, "end_char_idx": 7010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57a41937-44b4-43dc-ab96-cf11ab655018": {"__data__": {"id_": "57a41937-44b4-43dc-ab96-cf11ab655018", "embedding": null, "metadata": {"window": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n", "original_text": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c872629b-d5b5-4cc3-9c87-954a38042f3d", "node_type": "1", "metadata": {"window": "Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network \u2013\nLAControlNet that can be optimized for our specific task.  Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. ", "original_text": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module). "}, "hash": "244c046efb62d0118b250e80aaa43e373a61270eef7a651d10fd8a6398b444e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47fe2c58-b375-49d0-ba3d-fad724367224", "node_type": "1", "metadata": {"window": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. ", "original_text": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. "}, "hash": "6104fcd7c126c586d7c945bbf840496346c0e899155c306a1fbc7581b2ee4930", "class_name": "RelatedNodeInfo"}}, "hash": "d449bac1a7874bdb4d1046cb3bc788cd6b6637ac1d42b3b7e1d5be66bcbd31ee", "text": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. ", "start_char_idx": 7010, "end_char_idx": 7226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47fe2c58-b375-49d0-ba3d-fad724367224": {"__data__": {"id_": "47fe2c58-b375-49d0-ba3d-fad724367224", "embedding": null, "metadata": {"window": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. ", "original_text": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57a41937-44b4-43dc-ab96-cf11ab655018", "node_type": "1", "metadata": {"window": "Similar to ZIR, the pre-trained Stable\nDiffusion is fixed during finetuning to maintain its generative ability.  Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n", "original_text": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two. "}, "hash": "d449bac1a7874bdb4d1046cb3bc788cd6b6637ac1d42b3b7e1d5be66bcbd31ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c28f0c5f-fad2-4470-860e-27fede132336", "node_type": "1", "metadata": {"window": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). ", "original_text": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n"}, "hash": "22452a7f3f9d1d4bb38a383122b8cea1098cb548283d6fc257c3e319dde92b11", "class_name": "RelatedNodeInfo"}}, "hash": "6104fcd7c126c586d7c945bbf840496346c0e899155c306a1fbc7581b2ee4930", "text": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. ", "start_char_idx": 7226, "end_char_idx": 7334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c28f0c5f-fad2-4470-860e-27fede132336": {"__data__": {"id_": "c28f0c5f-fad2-4470-860e-27fede132336", "embedding": null, "metadata": {"window": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). ", "original_text": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47fe2c58-b375-49d0-ba3d-fad724367224", "node_type": "1", "metadata": {"window": "Third, to realize faithful and\nrealistic image reconstruction, we first apply a Restoration Module ( i.e., SwinIR) to reduce most\ndegradations, and then finetune the Generation Module ( i.e., LAControlNet) to generate new textures.\n Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. ", "original_text": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training. "}, "hash": "6104fcd7c126c586d7c945bbf840496346c0e899155c306a1fbc7581b2ee4930", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c2cc220-673d-4eb2-a2fb-327889bce696", "node_type": "1", "metadata": {"window": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects. ", "original_text": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. "}, "hash": "1ea4a51add90ab6846a5b186792ff4320fff195c417e952f9b20e64ceefd47e0", "class_name": "RelatedNodeInfo"}}, "hash": "22452a7f3f9d1d4bb38a383122b8cea1098cb548283d6fc257c3e319dde92b11", "text": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n", "start_char_idx": 7334, "end_char_idx": 7444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c2cc220-673d-4eb2-a2fb-327889bce696": {"__data__": {"id_": "2c2cc220-673d-4eb2-a2fb-327889bce696", "embedding": null, "metadata": {"window": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects. ", "original_text": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c28f0c5f-fad2-4470-860e-27fede132336", "node_type": "1", "metadata": {"window": "Without this pipeline, the model may either produce over-smoothed results (remove Generation\nModule) or generate wrong details (remove Restoration Module).  In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). ", "original_text": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n"}, "hash": "22452a7f3f9d1d4bb38a383122b8cea1098cb548283d6fc257c3e319dde92b11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3384bcd0-4a73-42af-bead-9bbfecf42bf3", "node_type": "1", "metadata": {"window": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). ", "original_text": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). "}, "hash": "8513d41a75ff226ae7391b23aee85318d8bae186cae0314fe2eb47e163eb40db", "class_name": "RelatedNodeInfo"}}, "hash": "1ea4a51add90ab6846a5b186792ff4320fff195c417e952f9b20e64ceefd47e0", "text": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. ", "start_char_idx": 7444, "end_char_idx": 7600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3384bcd0-4a73-42af-bead-9bbfecf42bf3": {"__data__": {"id_": "3384bcd0-4a73-42af-bead-9bbfecf42bf3", "embedding": null, "metadata": {"window": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). ", "original_text": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c2cc220-673d-4eb2-a2fb-327889bce696", "node_type": "1", "metadata": {"window": "In addition, to meet users\u2019 diverse\nrequirements, we further propose a controllable module that could achieve continuous transition\neffects between restoration result in stage one and generation result in stage two.  This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects. ", "original_text": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets. "}, "hash": "1ea4a51add90ab6846a5b186792ff4320fff195c417e952f9b20e64ceefd47e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "642380f3-ed24-47a1-a6ca-8912e4eaf7f1", "node_type": "1", "metadata": {"window": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). ", "original_text": "We can observe the\ndifferences of these methods in some aspects. "}, "hash": "95f299b2f418aee27308f7430569f8d053501d661c24d20186c40fdc2ad7275a", "class_name": "RelatedNodeInfo"}}, "hash": "8513d41a75ff226ae7391b23aee85318d8bae186cae0314fe2eb47e163eb40db", "text": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). ", "start_char_idx": 7600, "end_char_idx": 7806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "642380f3-ed24-47a1-a6ca-8912e4eaf7f1": {"__data__": {"id_": "642380f3-ed24-47a1-a6ca-8912e4eaf7f1", "embedding": null, "metadata": {"window": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). ", "original_text": "We can observe the\ndifferences of these methods in some aspects. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3384bcd0-4a73-42af-bead-9bbfecf42bf3", "node_type": "1", "metadata": {"window": "This is achieved by\nintroducing the latent image guidance during the denoising process without re-training.  The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). ", "original_text": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al). "}, "hash": "8513d41a75ff226ae7391b23aee85318d8bae186cae0314fe2eb47e163eb40db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6962488e-43aa-44de-8451-7c7ccc73d9e8", "node_type": "1", "metadata": {"window": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). ", "original_text": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). "}, "hash": "7f8f31c7a4961dfcb9437ed0b4ebc346dc44d98946c8903cacf82216d0a768a7", "class_name": "RelatedNodeInfo"}}, "hash": "95f299b2f418aee27308f7430569f8d053501d661c24d20186c40fdc2ad7275a", "text": "We can observe the\ndifferences of these methods in some aspects. ", "start_char_idx": 7806, "end_char_idx": 7871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6962488e-43aa-44de-8451-7c7ccc73d9e8": {"__data__": {"id_": "6962488e-43aa-44de-8451-7c7ccc73d9e8", "embedding": null, "metadata": {"window": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). ", "original_text": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "642380f3-ed24-47a1-a6ca-8912e4eaf7f1", "node_type": "1", "metadata": {"window": "The gradient\nscale that applies to the latent image distance can be tuned to trade off realness and fidelity.\n Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). ", "original_text": "We can observe the\ndifferences of these methods in some aspects. "}, "hash": "95f299b2f418aee27308f7430569f8d053501d661c24d20186c40fdc2ad7275a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "884f7d0a-de8d-43aa-ae8c-7965db26117f", "node_type": "1", "metadata": {"window": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). ", "original_text": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). "}, "hash": "2cc88731e73a7db5d7eb9b18ae30a46dc0a219d256c3e2e43894848ac8f576ab", "class_name": "RelatedNodeInfo"}}, "hash": "7f8f31c7a4961dfcb9437ed0b4ebc346dc44d98946c8903cacf82216d0a768a7", "text": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). ", "start_char_idx": 7871, "end_char_idx": 8021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "884f7d0a-de8d-43aa-ae8c-7965db26117f": {"__data__": {"id_": "884f7d0a-de8d-43aa-ae8c-7965db26117f", "embedding": null, "metadata": {"window": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). ", "original_text": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6962488e-43aa-44de-8451-7c7ccc73d9e8", "node_type": "1", "metadata": {"window": "Equipped with the above components, the proposed DiffBIR demonstrates excellent performance\nin both BSR and BFR tasks on synthetic and real-world datasets.  It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). ", "original_text": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row). "}, "hash": "7f8f31c7a4961dfcb9437ed0b4ebc346dc44d98946c8903cacf82216d0a768a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6719bd5-f89c-401c-8fbb-3d60bc3ba0e6", "node_type": "1", "metadata": {"window": "We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. ", "original_text": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). "}, "hash": "5765de64b4648ef23a9d29bde2d6d4fb0a846fa029514af3e4771fcbde10b823", "class_name": "RelatedNodeInfo"}}, "hash": "2cc88731e73a7db5d7eb9b18ae30a46dc0a219d256c3e2e43894848ac8f576ab", "text": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). ", "start_char_idx": 8021, "end_char_idx": 8168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6719bd5-f89c-401c-8fbb-3d60bc3ba0e6": {"__data__": {"id_": "a6719bd5-f89c-401c-8fbb-3d60bc3ba0e6", "embedding": null, "metadata": {"window": "We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. ", "original_text": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "884f7d0a-de8d-43aa-ae8c-7965db26117f", "node_type": "1", "metadata": {"window": "It is worth noting that DiffBIR\nachieves a great performance leap in general image restoration, outperforming existing BSR and BFR\nmethods ( e.g., BSRGAN [ 64], Real-ESRGAN [ 55], CodeFormer [ 68], et.al).  We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). ", "original_text": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row). "}, "hash": "2cc88731e73a7db5d7eb9b18ae30a46dc0a219d256c3e2e43894848ac8f576ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94fad515-7751-4b16-8ed5-d439effc9231", "node_type": "1", "metadata": {"window": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). ", "original_text": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). "}, "hash": "31bd9f589287c94f9c343aa47f438c75f399f7eab32d89c1b603661d91516500", "class_name": "RelatedNodeInfo"}}, "hash": "5765de64b4648ef23a9d29bde2d6d4fb0a846fa029514af3e4771fcbde10b823", "text": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). ", "start_char_idx": 8168, "end_char_idx": 8302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94fad515-7751-4b16-8ed5-d439effc9231": {"__data__": {"id_": "94fad515-7751-4b16-8ed5-d439effc9231", "embedding": null, "metadata": {"window": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). ", "original_text": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6719bd5-f89c-401c-8fbb-3d60bc3ba0e6", "node_type": "1", "metadata": {"window": "We can observe the\ndifferences of these methods in some aspects.  For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. ", "original_text": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row). "}, "hash": "5765de64b4648ef23a9d29bde2d6d4fb0a846fa029514af3e4771fcbde10b823", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97dcdd9f-eec4-42bb-b926-8d4dcd350397", "node_type": "1", "metadata": {"window": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n", "original_text": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. "}, "hash": "c38620bd52e88ca12d8025a7f9f6c18324db030244ff147f8c82d4b3263e064f", "class_name": "RelatedNodeInfo"}}, "hash": "31bd9f589287c94f9c343aa47f438c75f399f7eab32d89c1b603661d91516500", "text": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). ", "start_char_idx": 8302, "end_char_idx": 8443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97dcdd9f-eec4-42bb-b926-8d4dcd350397": {"__data__": {"id_": "97dcdd9f-eec4-42bb-b926-8d4dcd350397", "embedding": null, "metadata": {"window": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n", "original_text": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94fad515-7751-4b16-8ed5-d439effc9231", "node_type": "1", "metadata": {"window": "For complex textures, BSR methods tend to generate\nunrealistic details, while DiffBIR can produce visually pleasant results, see Figure 1(first row).  For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). ", "original_text": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row). "}, "hash": "31bd9f589287c94f9c343aa47f438c75f399f7eab32d89c1b603661d91516500", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc0b8576-da5d-432c-8d75-c4f75f1f13f5", "node_type": "1", "metadata": {"window": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n", "original_text": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). "}, "hash": "ba33bfa39ed21d1ebe51c257911611c61fa257d40f910e6e3ef811e9f3e75331", "class_name": "RelatedNodeInfo"}}, "hash": "c38620bd52e88ca12d8025a7f9f6c18324db030244ff147f8c82d4b3263e064f", "text": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. ", "start_char_idx": 8443, "end_char_idx": 8536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc0b8576-da5d-432c-8d75-c4f75f1f13f5": {"__data__": {"id_": "bc0b8576-da5d-432c-8d75-c4f75f1f13f5", "embedding": null, "metadata": {"window": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n", "original_text": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97dcdd9f-eec4-42bb-b926-8d4dcd350397", "node_type": "1", "metadata": {"window": "For\nsemantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct\nsemantic details, see Figure 1(second row).  For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n", "original_text": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods. "}, "hash": "c38620bd52e88ca12d8025a7f9f6c18324db030244ff147f8c82d4b3263e064f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36282957-700b-4bd4-95a3-f76d9c2413f5", "node_type": "1", "metadata": {"window": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution. ", "original_text": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n"}, "hash": "16f1ad43cf59ec0ece6139873e2ed8aff29926aa07b7c857858458e1415c33e2", "class_name": "RelatedNodeInfo"}}, "hash": "ba33bfa39ed21d1ebe51c257911611c61fa257d40f910e6e3ef811e9f3e75331", "text": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). ", "start_char_idx": 8536, "end_char_idx": 8803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36282957-700b-4bd4-95a3-f76d9c2413f5": {"__data__": {"id_": "36282957-700b-4bd4-95a3-f76d9c2413f5", "embedding": null, "metadata": {"window": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution. ", "original_text": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc0b8576-da5d-432c-8d75-c4f75f1f13f5", "node_type": "1", "metadata": {"window": "For tiny stripes, BSR methods tend to erase those details,\nwhile DiffBIR can still enhance their structures, see Figure 1(third row).  Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n", "original_text": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)). "}, "hash": "ba33bfa39ed21d1ebe51c257911611c61fa257d40f910e6e3ef811e9f3e75331", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92c1a58f-bbe1-49ad-99da-c5301979a82d", "node_type": "1", "metadata": {"window": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. ", "original_text": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n"}, "hash": "86059099ac489e4a5c8b5d2ef0a8ab1eeeab937315a0103e1728c300c6e8330d", "class_name": "RelatedNodeInfo"}}, "hash": "16f1ad43cf59ec0ece6139873e2ed8aff29926aa07b7c857858458e1415c33e2", "text": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n", "start_char_idx": 8803, "end_char_idx": 8937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92c1a58f-bbe1-49ad-99da-c5301979a82d": {"__data__": {"id_": "92c1a58f-bbe1-49ad-99da-c5301979a82d", "embedding": null, "metadata": {"window": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. ", "original_text": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36282957-700b-4bd4-95a3-f76d9c2413f5", "node_type": "1", "metadata": {"window": "Moreover, DiffBIR is able\nto deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure\n1(the last row).  All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution. ", "original_text": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n"}, "hash": "16f1ad43cf59ec0ece6139873e2ed8aff29926aa07b7c857858458e1415c33e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0571fe7-563e-409a-a4e4-aabc59474fa7", "node_type": "1", "metadata": {"window": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. ", "original_text": "2 Related Work\nBlind Image Super-Resolution. "}, "hash": "60d973a8da08a972a4ce1194070b43015a32780493accd52450e96d464e783f2", "class_name": "RelatedNodeInfo"}}, "hash": "86059099ac489e4a5c8b5d2ef0a8ab1eeeab937315a0103e1728c300c6e8330d", "text": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n", "start_char_idx": 8937, "end_char_idx": 9087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0571fe7-563e-409a-a4e4-aabc59474fa7": {"__data__": {"id_": "c0571fe7-563e-409a-a4e4-aabc59474fa7", "embedding": null, "metadata": {"window": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. ", "original_text": "2 Related Work\nBlind Image Super-Resolution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92c1a58f-bbe1-49ad-99da-c5301979a82d", "node_type": "1", "metadata": {"window": "All these show that DiffBIR has successfully broken the bottlenecks of existing BSR\nmethods.  For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. ", "original_text": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n"}, "hash": "86059099ac489e4a5c8b5d2ef0a8ab1eeeab937315a0103e1728c300c6e8330d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ed9eb4f-7b31-4c86-bc67-1e695dfb0657", "node_type": "1", "metadata": {"window": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. ", "original_text": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. "}, "hash": "249c25890cfb5c849687a4481651fbe69c38048e80e9290e26e6adf18b47162a", "class_name": "RelatedNodeInfo"}}, "hash": "60d973a8da08a972a4ce1194070b43015a32780493accd52450e96d464e783f2", "text": "2 Related Work\nBlind Image Super-Resolution. ", "start_char_idx": 9087, "end_char_idx": 9132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ed9eb4f-7b31-4c86-bc67-1e695dfb0657": {"__data__": {"id_": "7ed9eb4f-7b31-4c86-bc67-1e695dfb0657", "embedding": null, "metadata": {"window": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. ", "original_text": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0571fe7-563e-409a-a4e4-aabc59474fa7", "node_type": "1", "metadata": {"window": "For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such\nas maintaining good fidelity on facial area occluded by other objects (see first row in 1 (b)), achieving\nsuccessful restoration beyond facial areas (see first row in 1 (b)).  In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. ", "original_text": "2 Related Work\nBlind Image Super-Resolution. "}, "hash": "60d973a8da08a972a4ce1194070b43015a32780493accd52450e96d464e783f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f02c0ba-3d10-4b27-9769-6e2e4bcb0736", "node_type": "1", "metadata": {"window": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. ", "original_text": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. "}, "hash": "a068a0ead4c10af8ec095639e55b2641621bac3f5d65c4dda447bf0bc243cd1b", "class_name": "RelatedNodeInfo"}}, "hash": "249c25890cfb5c849687a4481651fbe69c38048e80e9290e26e6adf18b47162a", "text": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. ", "start_char_idx": 9132, "end_char_idx": 9249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f02c0ba-3d10-4b27-9769-6e2e4bcb0736": {"__data__": {"id_": "1f02c0ba-3d10-4b27-9769-6e2e4bcb0736", "embedding": null, "metadata": {"window": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. ", "original_text": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ed9eb4f-7b31-4c86-bc67-1e695dfb0657", "node_type": "1", "metadata": {"window": "In conclusion, our DiffBIR could\nobtain competitive performance for both BSR and BFR tasks in a unified framework for the first time.\n Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. ", "original_text": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations. "}, "hash": "249c25890cfb5c849687a4481651fbe69c38048e80e9290e26e6adf18b47162a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d08cfaa1-3159-4bcc-8be2-918c79e8ad0c", "node_type": "1", "metadata": {"window": "2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. ", "original_text": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. "}, "hash": "08acdf09573724df630a782dfff6aa3647f91658c8a594c74eabe6425039dc8a", "class_name": "RelatedNodeInfo"}}, "hash": "a068a0ead4c10af8ec095639e55b2641621bac3f5d65c4dda447bf0bc243cd1b", "text": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. ", "start_char_idx": 9249, "end_char_idx": 9428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d08cfaa1-3159-4bcc-8be2-918c79e8ad0c": {"__data__": {"id_": "d08cfaa1-3159-4bcc-8be2-918c79e8ad0c", "embedding": null, "metadata": {"window": "2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. ", "original_text": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f02c0ba-3d10-4b27-9769-6e2e4bcb0736", "node_type": "1", "metadata": {"window": "Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over\nthe existing state-of-the-art BSR and BFR methods.\n 2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. ", "original_text": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling. "}, "hash": "a068a0ead4c10af8ec095639e55b2641621bac3f5d65c4dda447bf0bc243cd1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fc2e7cf-ace6-4932-8343-71fc9495ae9b", "node_type": "1", "metadata": {"window": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. ", "original_text": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. "}, "hash": "1fe04213645abb6f692a64d54394e0458c4eaf8a9e47d26c38fd0fadb5ff9805", "class_name": "RelatedNodeInfo"}}, "hash": "08acdf09573724df630a782dfff6aa3647f91658c8a594c74eabe6425039dc8a", "text": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. ", "start_char_idx": 9428, "end_char_idx": 9539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fc2e7cf-ace6-4932-8343-71fc9495ae9b": {"__data__": {"id_": "7fc2e7cf-ace6-4932-8343-71fc9495ae9b", "embedding": null, "metadata": {"window": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. ", "original_text": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d08cfaa1-3159-4bcc-8be2-918c79e8ad0c", "node_type": "1", "metadata": {"window": "2 Related Work\nBlind Image Super-Resolution.  Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. ", "original_text": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations. "}, "hash": "08acdf09573724df630a782dfff6aa3647f91658c8a594c74eabe6425039dc8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dc339dd-6e81-4547-a747-c52c74c30495", "node_type": "1", "metadata": {"window": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. ", "original_text": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. "}, "hash": "ae4b933d286d2b62778ef24e5901ed3628674414162e49f1e69b67db0db59199", "class_name": "RelatedNodeInfo"}}, "hash": "1fe04213645abb6f692a64d54394e0458c4eaf8a9e47d26c38fd0fadb5ff9805", "text": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. ", "start_char_idx": 9539, "end_char_idx": 9661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dc339dd-6e81-4547-a747-c52c74c30495": {"__data__": {"id_": "0dc339dd-6e81-4547-a747-c52c74c30495", "embedding": null, "metadata": {"window": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. ", "original_text": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fc2e7cf-ace6-4932-8343-71fc9495ae9b", "node_type": "1", "metadata": {"window": "Latest advances [ 37] on BSR have explored more complex degrada-\ntion models to approximate real-world degradations.  In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. ", "original_text": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance. "}, "hash": "1fe04213645abb6f692a64d54394e0458c4eaf8a9e47d26c38fd0fadb5ff9805", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d79ef500-0789-4b32-a9d5-1e9ed2051e49", "node_type": "1", "metadata": {"window": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n", "original_text": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. "}, "hash": "ab952f174af59b7e9501453d6db9eba73319d7a6105a7584f79313f644a5c76f", "class_name": "RelatedNodeInfo"}}, "hash": "ae4b933d286d2b62778ef24e5901ed3628674414162e49f1e69b67db0db59199", "text": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. ", "start_char_idx": 9661, "end_char_idx": 9752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d79ef500-0789-4b32-a9d5-1e9ed2051e49": {"__data__": {"id_": "d79ef500-0789-4b32-a9d5-1e9ed2051e49", "embedding": null, "metadata": {"window": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n", "original_text": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dc339dd-6e81-4547-a747-c52c74c30495", "node_type": "1", "metadata": {"window": "In particular, BSRGAN [ 64] aims to synthesize\nmore practical degradations based on a random shuffling strategy, and Real-ESRGAN [ 55] exploits\n\"high-order\" degradation modeling.  They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. ", "original_text": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15]. "}, "hash": "ae4b933d286d2b62778ef24e5901ed3628674414162e49f1e69b67db0db59199", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d994b7ba-d5ed-4a88-a96a-bd1150633426", "node_type": "1", "metadata": {"window": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration. ", "original_text": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. "}, "hash": "cd2a28bf3e274344af6619bb9b11ed63cc76c05b989819586634af914e1d23e9", "class_name": "RelatedNodeInfo"}}, "hash": "ab952f174af59b7e9501453d6db9eba73319d7a6105a7584f79313f644a5c76f", "text": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. ", "start_char_idx": 9752, "end_char_idx": 9880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d994b7ba-d5ed-4a88-a96a-bd1150633426": {"__data__": {"id_": "d994b7ba-d5ed-4a88-a96a-bd1150633426", "embedding": null, "metadata": {"window": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration. ", "original_text": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d79ef500-0789-4b32-a9d5-1e9ed2051e49", "node_type": "1", "metadata": {"window": "They both utilize GANs [ 17;41;49;31;56] to learn the image\nreconstruction process under complex degradations.  SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n", "original_text": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details. "}, "hash": "ab952f174af59b7e9501453d6db9eba73319d7a6105a7584f79313f644a5c76f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c28ad670-06c2-4017-8ec2-6f87ecb6bcf4", "node_type": "1", "metadata": {"window": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. ", "original_text": "\u00d74/\u00d78), which is limited for BIR problem.\n"}, "hash": "f400be3ff643bed02e45f9440c4bbb8527bc1ecf26d5fcc32e2c26912cf3fdc5", "class_name": "RelatedNodeInfo"}}, "hash": "cd2a28bf3e274344af6619bb9b11ed63cc76c05b989819586634af914e1d23e9", "text": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. ", "start_char_idx": 9880, "end_char_idx": 9987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c28ad670-06c2-4017-8ec2-6f87ecb6bcf4": {"__data__": {"id_": "c28ad670-06c2-4017-8ec2-6f87ecb6bcf4", "embedding": null, "metadata": {"window": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. ", "original_text": "\u00d74/\u00d78), which is limited for BIR problem.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d994b7ba-d5ed-4a88-a96a-bd1150633426", "node_type": "1", "metadata": {"window": "SwinIR-GAN [ 36] uses the new prevailing\nbackbone Swin Transformer [ 38] to achieve better image restoration performance.  FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration. ", "original_text": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g. "}, "hash": "cd2a28bf3e274344af6619bb9b11ed63cc76c05b989819586634af914e1d23e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec3aa80a-b79a-4aa5-95a2-a29c72a69324", "node_type": "1", "metadata": {"window": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. ", "original_text": "Zero-shot Image Restoration. "}, "hash": "98d9c2d8049111c5432ffc5d7fecae1d4bba0bf63b667a2b14a75860f2aca824", "class_name": "RelatedNodeInfo"}}, "hash": "f400be3ff643bed02e45f9440c4bbb8527bc1ecf26d5fcc32e2c26912cf3fdc5", "text": "\u00d74/\u00d78), which is limited for BIR problem.\n", "start_char_idx": 9987, "end_char_idx": 10029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec3aa80a-b79a-4aa5-95a2-a29c72a69324": {"__data__": {"id_": "ec3aa80a-b79a-4aa5-95a2-a29c72a69324", "embedding": null, "metadata": {"window": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. ", "original_text": "Zero-shot Image Restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c28ad670-06c2-4017-8ec2-6f87ecb6bcf4", "node_type": "1", "metadata": {"window": "FeMaSR [ 6]\nformulates SR as a feature-matching problem based on pre-trained VQ-GAN [ 15].  Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. ", "original_text": "\u00d74/\u00d78), which is limited for BIR problem.\n"}, "hash": "f400be3ff643bed02e45f9440c4bbb8527bc1ecf26d5fcc32e2c26912cf3fdc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0c841ea-f297-416f-9bf9-4a3d6c4aaf05", "node_type": "1", "metadata": {"window": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n", "original_text": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. "}, "hash": "23b1ecb0c4a13e82b03fdaf1012f22d83c5090e36f135928e2ceb64b725f9217", "class_name": "RelatedNodeInfo"}}, "hash": "98d9c2d8049111c5432ffc5d7fecae1d4bba0bf63b667a2b14a75860f2aca824", "text": "Zero-shot Image Restoration. ", "start_char_idx": 10029, "end_char_idx": 10058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0c841ea-f297-416f-9bf9-4a3d6c4aaf05": {"__data__": {"id_": "f0c841ea-f297-416f-9bf9-4a3d6c4aaf05", "embedding": null, "metadata": {"window": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n", "original_text": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec3aa80a-b79a-4aa5-95a2-a29c72a69324", "node_type": "1", "metadata": {"window": "Although BSR\nmethods can be useful to remove degradations in the real world, they are not good at generating\nrealistic details.  In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. ", "original_text": "Zero-shot Image Restoration. "}, "hash": "98d9c2d8049111c5432ffc5d7fecae1d4bba0bf63b667a2b14a75860f2aca824", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbf618e2-3e59-4a0b-9ae1-d2bbb16fdf2a", "node_type": "1", "metadata": {"window": "\u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n", "original_text": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. "}, "hash": "4b4a9d574cfd655dddaae409ec83ee0a989b4b8943d972b55c4b8792889b3cc1", "class_name": "RelatedNodeInfo"}}, "hash": "23b1ecb0c4a13e82b03fdaf1012f22d83c5090e36f135928e2ceb64b725f9217", "text": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. ", "start_char_idx": 10058, "end_char_idx": 10165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbf618e2-3e59-4a0b-9ae1-d2bbb16fdf2a": {"__data__": {"id_": "cbf618e2-3e59-4a0b-9ae1-d2bbb16fdf2a", "embedding": null, "metadata": {"window": "\u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n", "original_text": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0c841ea-f297-416f-9bf9-4a3d6c4aaf05", "node_type": "1", "metadata": {"window": "In addition, they typically assume the low-quality image input is downsampled by\nsome certain scales (e.g.  \u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n", "original_text": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner. "}, "hash": "23b1ecb0c4a13e82b03fdaf1012f22d83c5090e36f135928e2ceb64b725f9217", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dfbe85e-6966-4006-9e76-9e38a662161f", "node_type": "1", "metadata": {"window": "Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. ", "original_text": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n"}, "hash": "41f823a7d7e6f48043d46ec9d2f6f56700e1d2e0013362f6fda467a9300d7898", "class_name": "RelatedNodeInfo"}}, "hash": "4b4a9d574cfd655dddaae409ec83ee0a989b4b8943d972b55c4b8792889b3cc1", "text": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. ", "start_char_idx": 10165, "end_char_idx": 10280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dfbe85e-6966-4006-9e76-9e38a662161f": {"__data__": {"id_": "8dfbe85e-6966-4006-9e76-9e38a662161f", "embedding": null, "metadata": {"window": "Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. ", "original_text": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbf618e2-3e59-4a0b-9ae1-d2bbb16fdf2a", "node_type": "1", "metadata": {"window": "\u00d74/\u00d78), which is limited for BIR problem.\n Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n", "original_text": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space. "}, "hash": "4b4a9d574cfd655dddaae409ec83ee0a989b4b8943d972b55c4b8792889b3cc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d54808d6-2445-491a-ae50-ad8c4c63381c", "node_type": "1", "metadata": {"window": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. ", "original_text": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n"}, "hash": "ccd447bbb5537a61681369f3bccb4743993576fd99c3e48ccee89383ad89dfd8", "class_name": "RelatedNodeInfo"}}, "hash": "41f823a7d7e6f48043d46ec9d2f6f56700e1d2e0013362f6fda467a9300d7898", "text": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n", "start_char_idx": 10280, "end_char_idx": 10404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d54808d6-2445-491a-ae50-ad8c4c63381c": {"__data__": {"id_": "d54808d6-2445-491a-ae50-ad8c4c63381c", "embedding": null, "metadata": {"window": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. ", "original_text": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dfbe85e-6966-4006-9e76-9e38a662161f", "node_type": "1", "metadata": {"window": "Zero-shot Image Restoration.  ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. ", "original_text": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n"}, "hash": "41f823a7d7e6f48043d46ec9d2f6f56700e1d2e0013362f6fda467a9300d7898", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31d1dbd3-f2e1-4439-ac0a-6111e722609b", "node_type": "1", "metadata": {"window": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n", "original_text": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. "}, "hash": "30284e284578d09298f21a31d35ad1ec5fc582359b883ab7592339b0fe865e77", "class_name": "RelatedNodeInfo"}}, "hash": "ccd447bbb5537a61681369f3bccb4743993576fd99c3e48ccee89383ad89dfd8", "text": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n", "start_char_idx": 10404, "end_char_idx": 10502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31d1dbd3-f2e1-4439-ac0a-6111e722609b": {"__data__": {"id_": "31d1dbd3-f2e1-4439-ac0a-6111e722609b", "embedding": null, "metadata": {"window": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n", "original_text": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d54808d6-2445-491a-ae50-ad8c4c63381c", "node_type": "1", "metadata": {"window": "ZIR aims to achieve image restoration by leveraging a pre-trained\nprior network in an unsupervised manner.  Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. ", "original_text": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n"}, "hash": "ccd447bbb5537a61681369f3bccb4743993576fd99c3e48ccee89383ad89dfd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "108c69ac-2118-46d1-ab50-b69c77fb67e7", "node_type": "1", "metadata": {"window": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration. ", "original_text": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. "}, "hash": "dfa79d39f109090505dfa4fdd710190c73b2eab9c571ff1151aa6a6cd839ce34", "class_name": "RelatedNodeInfo"}}, "hash": "30284e284578d09298f21a31d35ad1ec5fc582359b883ab7592339b0fe865e77", "text": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. ", "start_char_idx": 10502, "end_char_idx": 10659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "108c69ac-2118-46d1-ab50-b69c77fb67e7": {"__data__": {"id_": "108c69ac-2118-46d1-ab50-b69c77fb67e7", "embedding": null, "metadata": {"window": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration. ", "original_text": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31d1dbd3-f2e1-4439-ac0a-6111e722609b", "node_type": "1", "metadata": {"window": "Earlier works [ 2;10;40;44] mainly concentrate on\nsearching a latent code within a pre-trained GAN\u2019s latent space.  Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n", "original_text": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space. "}, "hash": "30284e284578d09298f21a31d35ad1ec5fc582359b883ab7592339b0fe865e77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f0966f5-5b83-47bc-b176-0d2310b381b0", "node_type": "1", "metadata": {"window": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. ", "original_text": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n"}, "hash": "d9b3bdb7fc949d446c295b001c91f492d1996a5ba8bc721c75b64d3cd4d07b98", "class_name": "RelatedNodeInfo"}}, "hash": "dfa79d39f109090505dfa4fdd710190c73b2eab9c571ff1151aa6a6cd839ce34", "text": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. ", "start_char_idx": 10659, "end_char_idx": 10835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f0966f5-5b83-47bc-b176-0d2310b381b0": {"__data__": {"id_": "9f0966f5-5b83-47bc-b176-0d2310b381b0", "embedding": null, "metadata": {"window": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. ", "original_text": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "108c69ac-2118-46d1-ab50-b69c77fb67e7", "node_type": "1", "metadata": {"window": "Recent advancements in this\nfield embrace the utilization of Denoising Diffusion Probabilistic Models [ 21;51;52;46;45;48].\n DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration. ", "original_text": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference. "}, "hash": "dfa79d39f109090505dfa4fdd710190c73b2eab9c571ff1151aa6a6cd839ce34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58388634-5209-4f2d-a2da-60c4305ecb70", "node_type": "1", "metadata": {"window": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n", "original_text": "Blind Face Restoration. "}, "hash": "5fd0f80d34b5965f693c2f2a63226606603bc0e74fb7dc987b41e178756703ee", "class_name": "RelatedNodeInfo"}}, "hash": "d9b3bdb7fc949d446c295b001c91f492d1996a5ba8bc721c75b64d3cd4d07b98", "text": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n", "start_char_idx": 10835, "end_char_idx": 11034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58388634-5209-4f2d-a2da-60c4305ecb70": {"__data__": {"id_": "58388634-5209-4f2d-a2da-60c4305ecb70", "embedding": null, "metadata": {"window": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n", "original_text": "Blind Face Restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f0966f5-5b83-47bc-b176-0d2310b381b0", "node_type": "1", "metadata": {"window": "DDRM [ 26] introduces an SVD-based approach to handle linear image restoration tasks efficiently.\n 3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. ", "original_text": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n"}, "hash": "d9b3bdb7fc949d446c295b001c91f492d1996a5ba8bc721c75b64d3cd4d07b98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3198f679-0ea6-438b-b0d6-89935847b6c8", "node_type": "1", "metadata": {"window": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. ", "original_text": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. "}, "hash": "90e6ace5bfee1ea15b740b0448037b3fa8a69170dfee17974e8462cb32417d54", "class_name": "RelatedNodeInfo"}}, "hash": "5fd0f80d34b5965f693c2f2a63226606603bc0e74fb7dc987b41e178756703ee", "text": "Blind Face Restoration. ", "start_char_idx": 11034, "end_char_idx": 11058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3198f679-0ea6-438b-b0d6-89935847b6c8": {"__data__": {"id_": "3198f679-0ea6-438b-b0d6-89935847b6c8", "embedding": null, "metadata": {"window": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. ", "original_text": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58388634-5209-4f2d-a2da-60c4305ecb70", "node_type": "1", "metadata": {"window": "3\n\nMeanwhile, DDNM [ 57] analyzes the range-null space decomposition of a vector theoretically and\nthen designs a sampling schedule based on the null space.  Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n", "original_text": "Blind Face Restoration. "}, "hash": "5fd0f80d34b5965f693c2f2a63226606603bc0e74fb7dc987b41e178756703ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "277bcec3-4472-4755-a59f-b499fd04c445", "node_type": "1", "metadata": {"window": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. ", "original_text": "Early attempts utilize geometric priors (e.g.\n"}, "hash": "c2cc171f262651f8a434367ce6145fe97be93ef81a9314eb8a497eefb91caa51", "class_name": "RelatedNodeInfo"}}, "hash": "90e6ace5bfee1ea15b740b0448037b3fa8a69170dfee17974e8462cb32417d54", "text": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. ", "start_char_idx": 11058, "end_char_idx": 11177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "277bcec3-4472-4755-a59f-b499fd04c445": {"__data__": {"id_": "277bcec3-4472-4755-a59f-b499fd04c445", "embedding": null, "metadata": {"window": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. ", "original_text": "Early attempts utilize geometric priors (e.g.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3198f679-0ea6-438b-b0d6-89935847b6c8", "node_type": "1", "metadata": {"window": "Inspired by classifier guidance [ 12], GDP\n[16] introduces a more convenient and effective guidance approach, in which the degradation model\ncan be estimated during inference.  Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. ", "original_text": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information. "}, "hash": "90e6ace5bfee1ea15b740b0448037b3fa8a69170dfee17974e8462cb32417d54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d0b5ac-36f0-43c0-b443-ddf2813c5eec", "node_type": "1", "metadata": {"window": "Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n", "original_text": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. "}, "hash": "198af4f69711ffbbd63a2bc87d4e73c28a966e2873e383c33753e4b3919791f1", "class_name": "RelatedNodeInfo"}}, "hash": "c2cc171f262651f8a434367ce6145fe97be93ef81a9314eb8a497eefb91caa51", "text": "Early attempts utilize geometric priors (e.g.\n", "start_char_idx": 11177, "end_char_idx": 11223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18d0b5ac-36f0-43c0-b443-ddf2813c5eec": {"__data__": {"id_": "18d0b5ac-36f0-43c0-b443-ddf2813c5eec", "embedding": null, "metadata": {"window": "Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n", "original_text": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "277bcec3-4472-4755-a59f-b499fd04c445", "node_type": "1", "metadata": {"window": "Although these works contribute to the advancement of zero-shot\nimage restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in\nlow-quality images from real world.\n Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. ", "original_text": "Early attempts utilize geometric priors (e.g.\n"}, "hash": "c2cc171f262651f8a434367ce6145fe97be93ef81a9314eb8a497eefb91caa51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3932a3dd-55e3-40f8-ae62-598f2afa79cd", "node_type": "1", "metadata": {"window": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n", "original_text": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. "}, "hash": "12db07e5609b980d39f65a40c78d73b864a595f04b770206199cf70cf8f24f7a", "class_name": "RelatedNodeInfo"}}, "hash": "198af4f69711ffbbd63a2bc87d4e73c28a966e2873e383c33753e4b3919791f1", "text": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. ", "start_char_idx": 11223, "end_char_idx": 11409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3932a3dd-55e3-40f8-ae62-598f2afa79cd": {"__data__": {"id_": "3932a3dd-55e3-40f8-ae62-598f2afa79cd", "embedding": null, "metadata": {"window": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n", "original_text": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d0b5ac-36f0-43c0-b443-ddf2813c5eec", "node_type": "1", "metadata": {"window": "Blind Face Restoration.  As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n", "original_text": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process. "}, "hash": "198af4f69711ffbbd63a2bc87d4e73c28a966e2873e383c33753e4b3919791f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5a53aff-a935-4ac9-846a-ead97a8e8e4d", "node_type": "1", "metadata": {"window": "Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. ", "original_text": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n"}, "hash": "b5dbe4fcc3ce6ff635ab9c0d9a35de526b898aaea326e89c0a92ee1349df66b3", "class_name": "RelatedNodeInfo"}}, "hash": "12db07e5609b980d39f65a40c78d73b864a595f04b770206199cf70cf8f24f7a", "text": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. ", "start_char_idx": 11409, "end_char_idx": 11558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5a53aff-a935-4ac9-846a-ead97a8e8e4d": {"__data__": {"id_": "a5a53aff-a935-4ac9-846a-ead97a8e8e4d", "embedding": null, "metadata": {"window": "Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. ", "original_text": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3932a3dd-55e3-40f8-ae62-598f2afa79cd", "node_type": "1", "metadata": {"window": "As a specific sub-domain of general images, the face image typically\ncarries more structural and semantic information.  Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n", "original_text": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness. "}, "hash": "12db07e5609b980d39f65a40c78d73b864a595f04b770206199cf70cf8f24f7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f2bd9d5-4ee5-4f73-854b-327cc4dbd9c2", "node_type": "1", "metadata": {"window": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. ", "original_text": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n"}, "hash": "b55890db37e3777b060a169f4f9a74c20cdfe9c918acd09fad944f004ebc1fed", "class_name": "RelatedNodeInfo"}}, "hash": "b5dbe4fcc3ce6ff635ab9c0d9a35de526b898aaea326e89c0a92ee1349df66b3", "text": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n", "start_char_idx": 11558, "end_char_idx": 11716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f2bd9d5-4ee5-4f73-854b-327cc4dbd9c2": {"__data__": {"id_": "3f2bd9d5-4ee5-4f73-854b-327cc4dbd9c2", "embedding": null, "metadata": {"window": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. ", "original_text": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5a53aff-a935-4ac9-846a-ead97a8e8e4d", "node_type": "1", "metadata": {"window": "Early attempts utilize geometric priors (e.g.\n facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. ", "original_text": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n"}, "hash": "b5dbe4fcc3ce6ff635ab9c0d9a35de526b898aaea326e89c0a92ee1349df66b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f729335c-213c-4ece-b08a-7e3c7c3aa559", "node_type": "1", "metadata": {"window": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. ", "original_text": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. "}, "hash": "0b10bdaa9becbb0744a2fca015ce2a0ee81b8cef475bce2a1a583f300e46ca17", "class_name": "RelatedNodeInfo"}}, "hash": "b55890db37e3777b060a169f4f9a74c20cdfe9c918acd09fad944f004ebc1fed", "text": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n", "start_char_idx": 11716, "end_char_idx": 11891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f729335c-213c-4ece-b08a-7e3c7c3aa559": {"__data__": {"id_": "f729335c-213c-4ece-b08a-7e3c7c3aa559", "embedding": null, "metadata": {"window": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. ", "original_text": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f2bd9d5-4ee5-4f73-854b-327cc4dbd9c2", "node_type": "1", "metadata": {"window": "facial parsing maps [ 5], facial landmarks[ 9;27], and facial component heatmaps [ 62]) or reference\npriors[ 34;33;32;13] as auxiliary information to guide the face restoration process.  With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. ", "original_text": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n"}, "hash": "b55890db37e3777b060a169f4f9a74c20cdfe9c918acd09fad944f004ebc1fed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "168d5bbf-d925-4e19-a6a6-c4a4d1db67fa", "node_type": "1", "metadata": {"window": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. ", "original_text": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. "}, "hash": "d0351f712778ddf3962bb0df7fbb715b47e779df1ef83efe371c8036d90aabe4", "class_name": "RelatedNodeInfo"}}, "hash": "0b10bdaa9becbb0744a2fca015ce2a0ee81b8cef475bce2a1a583f300e46ca17", "text": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. ", "start_char_idx": 11891, "end_char_idx": 12054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "168d5bbf-d925-4e19-a6a6-c4a4d1db67fa": {"__data__": {"id_": "168d5bbf-d925-4e19-a6a6-c4a4d1db67fa", "embedding": null, "metadata": {"window": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. ", "original_text": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f729335c-213c-4ece-b08a-7e3c7c3aa559", "node_type": "1", "metadata": {"window": "With the rapid\ndevelopment of generative networks, many BFR approaches incorporate powerful generative-prior to\nreconstruct faces in great realness.  Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. ", "original_text": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images. "}, "hash": "0b10bdaa9becbb0744a2fca015ce2a0ee81b8cef475bce2a1a583f300e46ca17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "294f88cf-6923-4b09-9a65-414506bbddef", "node_type": "1", "metadata": {"window": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2. ", "original_text": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. "}, "hash": "9fa1ab7082abd0bbe47e1432a929cae07973663533f31890c05bd96d7b5d2efe", "class_name": "RelatedNodeInfo"}}, "hash": "d0351f712778ddf3962bb0df7fbb715b47e779df1ef83efe371c8036d90aabe4", "text": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. ", "start_char_idx": 12054, "end_char_idx": 12146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "294f88cf-6923-4b09-9a65-414506bbddef": {"__data__": {"id_": "294f88cf-6923-4b09-9a65-414506bbddef", "embedding": null, "metadata": {"window": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2. ", "original_text": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "168d5bbf-d925-4e19-a6a6-c4a4d1db67fa", "node_type": "1", "metadata": {"window": "Representative GAN-prior-based methods [ 54;61;19;4] have\ndemonstrated their capability in achieving both high-quality and high-fidelity face reconstruction.\n State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. ", "original_text": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible. "}, "hash": "d0351f712778ddf3962bb0df7fbb715b47e779df1ef83efe371c8036d90aabe4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fdb4e19-0761-47c8-8649-c916fe765000", "node_type": "1", "metadata": {"window": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n", "original_text": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. "}, "hash": "531e53cfe40d410de63b599ca119f64bcc848cf9b589ca591178a115c901a381", "class_name": "RelatedNodeInfo"}}, "hash": "9fa1ab7082abd0bbe47e1432a929cae07973663533f31890c05bd96d7b5d2efe", "text": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. ", "start_char_idx": 12146, "end_char_idx": 12270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fdb4e19-0761-47c8-8649-c916fe765000": {"__data__": {"id_": "4fdb4e19-0761-47c8-8649-c916fe765000", "embedding": null, "metadata": {"window": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n", "original_text": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "294f88cf-6923-4b09-9a65-414506bbddef", "node_type": "1", "metadata": {"window": "State-of-the-art works [ 68;18;59] introduce the HQ codebook to generate surprisingly realistic face\ndetails by exploiting Vector-Quantized (VQ) dictionary learning [53; 15].\n 3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2. ", "original_text": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss. "}, "hash": "9fa1ab7082abd0bbe47e1432a929cae07973663533f31890c05bd96d7b5d2efe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "103621d0-09fd-4a91-9824-e920671a949c", "node_type": "1", "metadata": {"window": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). ", "original_text": "The overall\nframework is illustrated in Figure 2. "}, "hash": "1ce7d8c0febdc565e2581c55c80284232001b492092655bb4e332ea7a3efa993", "class_name": "RelatedNodeInfo"}}, "hash": "531e53cfe40d410de63b599ca119f64bcc848cf9b589ca591178a115c901a381", "text": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. ", "start_char_idx": 12270, "end_char_idx": 12405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "103621d0-09fd-4a91-9824-e920671a949c": {"__data__": {"id_": "103621d0-09fd-4a91-9824-e920671a949c", "embedding": null, "metadata": {"window": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). ", "original_text": "The overall\nframework is illustrated in Figure 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fdb4e19-0761-47c8-8649-c916fe765000", "node_type": "1", "metadata": {"window": "3 Methodology\nIn this work, we aim to exploit a powerful generative prior \u2013 Stable Diffusion to solve blind restoration\nproblems for both general and face images.  Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n", "original_text": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss. "}, "hash": "531e53cfe40d410de63b599ca119f64bcc848cf9b589ca591178a115c901a381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bb20122-108a-495f-9434-2aca0c87ac3f", "node_type": "1", "metadata": {"window": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n", "original_text": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n"}, "hash": "68a20ee83c2906c35c32bcf5fce783c59b15f5a6cff28c34466871a82a907860", "class_name": "RelatedNodeInfo"}}, "hash": "1ce7d8c0febdc565e2581c55c80284232001b492092655bb4e332ea7a3efa993", "text": "The overall\nframework is illustrated in Figure 2. ", "start_char_idx": 12405, "end_char_idx": 12455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bb20122-108a-495f-9434-2aca0c87ac3f": {"__data__": {"id_": "4bb20122-108a-495f-9434-2aca0c87ac3f", "embedding": null, "metadata": {"window": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n", "original_text": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "103621d0-09fd-4a91-9824-e920671a949c", "node_type": "1", "metadata": {"window": "Our proposed framework adopts a two-stage pipeline\nthat is effective, robust, and flexible.  First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). ", "original_text": "The overall\nframework is illustrated in Figure 2. "}, "hash": "1ce7d8c0febdc565e2581c55c80284232001b492092655bb4e332ea7a3efa993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "823cd9e1-d006-4be6-b7cb-a14dc1b0006a", "node_type": "1", "metadata": {"window": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. ", "original_text": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). "}, "hash": "fcdfe1b175e25761c405e1066cc48574d89ab1ba7dcd08372d5c8ee50982cd01", "class_name": "RelatedNodeInfo"}}, "hash": "68a20ee83c2906c35c32bcf5fce783c59b15f5a6cff28c34466871a82a907860", "text": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n", "start_char_idx": 12455, "end_char_idx": 12619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "823cd9e1-d006-4be6-b7cb-a14dc1b0006a": {"__data__": {"id_": "823cd9e1-d006-4be6-b7cb-a14dc1b0006a", "embedding": null, "metadata": {"window": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. ", "original_text": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bb20122-108a-495f-9434-2aca0c87ac3f", "node_type": "1", "metadata": {"window": "First, we employ a Restoration Module to remove corruptions,\nsuch as noises or distortion artifacts, using regression loss.  As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n", "original_text": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n"}, "hash": "68a20ee83c2906c35c32bcf5fce783c59b15f5a6cff28c34466871a82a907860", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb8cc6b7-1528-4505-a61d-947cea801c73", "node_type": "1", "metadata": {"window": "The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. ", "original_text": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n"}, "hash": "344d223020b885b08b4ca2b8dc2f8c902f456bed3ec97b1867fc6477e2727107", "class_name": "RelatedNodeInfo"}}, "hash": "fcdfe1b175e25761c405e1066cc48574d89ab1ba7dcd08372d5c8ee50982cd01", "text": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). ", "start_char_idx": 12619, "end_char_idx": 12718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb8cc6b7-1528-4505-a61d-947cea801c73": {"__data__": {"id_": "cb8cc6b7-1528-4505-a61d-947cea801c73", "embedding": null, "metadata": {"window": "The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. ", "original_text": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "823cd9e1-d006-4be6-b7cb-a14dc1b0006a", "node_type": "1", "metadata": {"window": "As the lost local textures and coarse/fine\ndetails are still absent, we then leverage Stable Diffusion to remedy the information loss.  The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. ", "original_text": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2). "}, "hash": "fcdfe1b175e25761c405e1066cc48574d89ab1ba7dcd08372d5c8ee50982cd01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccb3a3bb-aef2-430f-8d9a-3658fa601db7", "node_type": "1", "metadata": {"window": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. ", "original_text": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. "}, "hash": "91dd8b15111d47c0ca51ef304e637fdfe821d0480b1b9c4fb546c97b9fed4efc", "class_name": "RelatedNodeInfo"}}, "hash": "344d223020b885b08b4ca2b8dc2f8c902f456bed3ec97b1867fc6477e2727107", "text": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n", "start_char_idx": 12718, "end_char_idx": 12856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccb3a3bb-aef2-430f-8d9a-3658fa601db7": {"__data__": {"id_": "ccb3a3bb-aef2-430f-8d9a-3658fa601db7", "embedding": null, "metadata": {"window": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. ", "original_text": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb8cc6b7-1528-4505-a61d-947cea801c73", "node_type": "1", "metadata": {"window": "The overall\nframework is illustrated in Figure 2.  Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. ", "original_text": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n"}, "hash": "344d223020b885b08b4ca2b8dc2f8c902f456bed3ec97b1867fc6477e2727107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1e06b3e-05ab-4f2b-adb6-c98cdd3db909", "node_type": "1", "metadata": {"window": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n", "original_text": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. "}, "hash": "1a885eb0675d7e7b57ce2fa5a5a7f6237efa93a1c00b9e1b25dc0aad4e29cf99", "class_name": "RelatedNodeInfo"}}, "hash": "91dd8b15111d47c0ca51ef304e637fdfe821d0480b1b9c4fb546c97b9fed4efc", "text": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. ", "start_char_idx": 12856, "end_char_idx": 13295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1e06b3e-05ab-4f2b-adb6-c98cdd3db909": {"__data__": {"id_": "f1e06b3e-05ab-4f2b-adb6-c98cdd3db909", "embedding": null, "metadata": {"window": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n", "original_text": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccb3a3bb-aef2-430f-8d9a-3658fa601db7", "node_type": "1", "metadata": {"window": "Specifically, we first pretrain a SwinIR [ 36] on large-scale\ndataset to achieve the preliminary degradation removal across diversified degradations (Section 3.1).\n Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. ", "original_text": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff. "}, "hash": "91dd8b15111d47c0ca51ef304e637fdfe821d0480b1b9c4fb546c97b9fed4efc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb1c5e7c-462d-4d08-927f-acf5f9ab5c19", "node_type": "1", "metadata": {"window": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model. ", "original_text": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. "}, "hash": "977cc86520e32de4a85917fb81e0d69f5bc1809b46bafb84ddc32f46e7dc671a", "class_name": "RelatedNodeInfo"}}, "hash": "1a885eb0675d7e7b57ce2fa5a5a7f6237efa93a1c00b9e1b25dc0aad4e29cf99", "text": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. ", "start_char_idx": 13295, "end_char_idx": 13401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb1c5e7c-462d-4d08-927f-acf5f9ab5c19": {"__data__": {"id_": "eb1c5e7c-462d-4d08-927f-acf5f9ab5c19", "embedding": null, "metadata": {"window": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model. ", "original_text": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1e06b3e-05ab-4f2b-adb6-c98cdd3db909", "node_type": "1", "metadata": {"window": "Then, the generative prior is leveraged for producing realistic restoration results (Section 3.2).  In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n", "original_text": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two. "}, "hash": "1a885eb0675d7e7b57ce2fa5a5a7f6237efa93a1c00b9e1b25dc0aad4e29cf99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a11ee045-9d8e-4996-9140-ecf6810450d8", "node_type": "1", "metadata": {"window": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. ", "original_text": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n"}, "hash": "ca48ec6c4cc201bdeb2cfbd7de8b71ed77941cdc83937ca5a920b4e6073f83a8", "class_name": "RelatedNodeInfo"}}, "hash": "977cc86520e32de4a85917fb81e0d69f5bc1809b46bafb84ddc32f46e7dc671a", "text": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. ", "start_char_idx": 13401, "end_char_idx": 13533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a11ee045-9d8e-4996-9140-ecf6810450d8": {"__data__": {"id_": "a11ee045-9d8e-4996-9140-ecf6810450d8", "embedding": null, "metadata": {"window": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. ", "original_text": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb1c5e7c-462d-4d08-927f-acf5f9ab5c19", "node_type": "1", "metadata": {"window": "In\naddition, a controllable module based on latent image guidance is introduced for trade-off between\nrealness andfidelity (Section 3.3).\n Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model. ", "original_text": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers. "}, "hash": "977cc86520e32de4a85917fb81e0d69f5bc1809b46bafb84ddc32f46e7dc671a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dc50bf4-82de-4080-b678-0e360d6fe086", "node_type": "1", "metadata": {"window": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. ", "original_text": "3.1 Pretraining for Degradation Removal\nDegradation Model. "}, "hash": "418de679bb608bff01edf3ff99a554a98dc7e56259159bb233271e6b5dbe70a2", "class_name": "RelatedNodeInfo"}}, "hash": "ca48ec6c4cc201bdeb2cfbd7de8b71ed77941cdc83937ca5a920b4e6073f83a8", "text": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n", "start_char_idx": 13533, "end_char_idx": 13688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dc50bf4-82de-4080-b678-0e360d6fe086": {"__data__": {"id_": "4dc50bf4-82de-4080-b678-0e360d6fe086", "embedding": null, "metadata": {"window": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. ", "original_text": "3.1 Pretraining for Degradation Removal\nDegradation Model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a11ee045-9d8e-4996-9140-ecf6810450d8", "node_type": "1", "metadata": {"window": "Ireg\ud835\udc67\ud835\udc61Stage 2\nFixed Trainable\ud835\udcd3Stage 1\n\ud835\udcd4\nParallel\nModule\u2130(\ud835\udc3c\ud835\udc5f\ud835\udc52\ud835\udc54)Restoration ModuleDenoiser\nNoise Resize Blur\nBlur Resize NoiseInitializeIdiff\nIHQ\nfirst -ordersecond -order++\ud835\udc67\ud835\udc61\u22121 \ud835\udc670\u00d7(T-1)\nConcatILQ\nFigure 2: The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal\nto obtain Ireg; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image\nreconstruction and obtain Idiff.  RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. ", "original_text": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n"}, "hash": "ca48ec6c4cc201bdeb2cfbd7de8b71ed77941cdc83937ca5a920b4e6073f83a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e9f65b0-2963-4492-ba06-2a6f06a5100b", "node_type": "1", "metadata": {"window": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n", "original_text": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. "}, "hash": "4b92a4050b549546cea1107f4b50053bbf7131e8fac3e95632949cf309936815", "class_name": "RelatedNodeInfo"}}, "hash": "418de679bb608bff01edf3ff99a554a98dc7e56259159bb233271e6b5dbe70a2", "text": "3.1 Pretraining for Degradation Removal\nDegradation Model. ", "start_char_idx": 13688, "end_char_idx": 13747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e9f65b0-2963-4492-ba06-2a6f06a5100b": {"__data__": {"id_": "3e9f65b0-2963-4492-ba06-2a6f06a5100b", "embedding": null, "metadata": {"window": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n", "original_text": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dc50bf4-82de-4080-b678-0e360d6fe086", "node_type": "1", "metadata": {"window": "RM is trained across diversified degradations in a self-supervised manner,\nand is fixed during stage-two.  LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. ", "original_text": "3.1 Pretraining for Degradation Removal\nDegradation Model. "}, "hash": "418de679bb608bff01edf3ff99a554a98dc7e56259159bb233271e6b5dbe70a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dca689b3-b3a4-4724-9c69-8a9f5095138b", "node_type": "1", "metadata": {"window": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. ", "original_text": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. "}, "hash": "9ad7b74a826b2be2bc3af0710491be2b4e510e57364572e26ec28ba6dac5972a", "class_name": "RelatedNodeInfo"}}, "hash": "4b92a4050b549546cea1107f4b50053bbf7131e8fac3e95632949cf309936815", "text": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. ", "start_char_idx": 13747, "end_char_idx": 13846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dca689b3-b3a4-4724-9c69-8a9f5095138b": {"__data__": {"id_": "dca689b3-b3a4-4724-9c69-8a9f5095138b", "embedding": null, "metadata": {"window": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. ", "original_text": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e9f65b0-2963-4492-ba06-2a6f06a5100b", "node_type": "1", "metadata": {"window": "LAControlNet contains a parallel module that is partially initialized with the\ndenoiser\u2019s checkpoint and has several fusion layers.  It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n", "original_text": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations. "}, "hash": "4b92a4050b549546cea1107f4b50053bbf7131e8fac3e95632949cf309936815", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27337c73-3249-47b9-a4df-a3c2012f6bf9", "node_type": "1", "metadata": {"window": "3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "original_text": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n"}, "hash": "2c49e04638effd263369dd62aba2472b1dc363252ac9f0fa97057c6d25a7142a", "class_name": "RelatedNodeInfo"}}, "hash": "9ad7b74a826b2be2bc3af0710491be2b4e510e57364572e26ec28ba6dac5972a", "text": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. ", "start_char_idx": 13846, "end_char_idx": 13932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27337c73-3249-47b9-a4df-a3c2012f6bf9": {"__data__": {"id_": "27337c73-3249-47b9-a4df-a3c2012f6bf9", "embedding": null, "metadata": {"window": "3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "original_text": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dca689b3-b3a4-4724-9c69-8a9f5095138b", "node_type": "1", "metadata": {"window": "It uses V AE\u2019s encoder to project the Iregto the latent space,\nand performs concatenation with the randomly sampled noisy ztas the conditioning mechanism.\n 3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. ", "original_text": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved. "}, "hash": "9ad7b74a826b2be2bc3af0710491be2b4e510e57364572e26ec28ba6dac5972a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c02b2f67-ed5e-4583-af70-80b6b64084c6", "node_type": "1", "metadata": {"window": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. ", "original_text": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. "}, "hash": "0d592565b94e14565e997f1dc6003ed7e42128c19357a9d5dd0a55741a94bbba", "class_name": "RelatedNodeInfo"}}, "hash": "2c49e04638effd263369dd62aba2472b1dc363252ac9f0fa97057c6d25a7142a", "text": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n", "start_char_idx": 13932, "end_char_idx": 14108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c02b2f67-ed5e-4583-af70-80b6b64084c6": {"__data__": {"id_": "c02b2f67-ed5e-4583-af70-80b6b64084c6", "embedding": null, "metadata": {"window": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. ", "original_text": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27337c73-3249-47b9-a4df-a3c2012f6bf9", "node_type": "1", "metadata": {"window": "3.1 Pretraining for Degradation Removal\nDegradation Model.  BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "original_text": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n"}, "hash": "2c49e04638effd263369dd62aba2472b1dc363252ac9f0fa97057c6d25a7142a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66de200d-fbee-4b88-946d-7cc5a8489ad9", "node_type": "1", "metadata": {"window": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n", "original_text": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. "}, "hash": "7f790d11b315730d05109c57852fe4960ab6604702e01fd95a7f430865772aca", "class_name": "RelatedNodeInfo"}}, "hash": "0d592565b94e14565e997f1dc6003ed7e42128c19357a9d5dd0a55741a94bbba", "text": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. ", "start_char_idx": 14108, "end_char_idx": 14212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66de200d-fbee-4b88-946d-7cc5a8489ad9": {"__data__": {"id_": "66de200d-fbee-4b88-946d-7cc5a8489ad9", "embedding": null, "metadata": {"window": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n", "original_text": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c02b2f67-ed5e-4583-af70-80b6b64084c6", "node_type": "1", "metadata": {"window": "BIR aims to restore clean images from low-quality (LQ) ones with unknown\nand complex degradations.  Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. ", "original_text": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64]. "}, "hash": "0d592565b94e14565e997f1dc6003ed7e42128c19357a9d5dd0a55741a94bbba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13671ef4-4481-46d9-b05a-e3ad81862097", "node_type": "1", "metadata": {"window": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module. ", "original_text": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. "}, "hash": "7366a5044f041b20701a526d2f1daed2d63d7520fd752f95deb0644eb3909a2a", "class_name": "RelatedNodeInfo"}}, "hash": "7f790d11b315730d05109c57852fe4960ab6604702e01fd95a7f430865772aca", "text": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "start_char_idx": 14212, "end_char_idx": 14449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13671ef4-4481-46d9-b05a-e3ad81862097": {"__data__": {"id_": "13671ef4-4481-46d9-b05a-e3ad81862097", "embedding": null, "metadata": {"window": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module. ", "original_text": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66de200d-fbee-4b88-946d-7cc5a8489ad9", "node_type": "1", "metadata": {"window": "Typically, blur, noise, compression artifacts, and low-resolution are\noften involved.  In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n", "original_text": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise. "}, "hash": "7f790d11b315730d05109c57852fe4960ab6604702e01fd95a7f430865772aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5ea6a92-abe3-4cb5-a163-620674cb2398", "node_type": "1", "metadata": {"window": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. ", "original_text": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n"}, "hash": "afd35d8e7aee8b579cdb415d99a515f849819d112595a2cf28fcc75eb56b8413", "class_name": "RelatedNodeInfo"}}, "hash": "7366a5044f041b20701a526d2f1daed2d63d7520fd752f95deb0644eb3909a2a", "text": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. ", "start_char_idx": 14449, "end_char_idx": 14621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5ea6a92-abe3-4cb5-a163-620674cb2398": {"__data__": {"id_": "e5ea6a92-abe3-4cb5-a163-620674cb2398", "embedding": null, "metadata": {"window": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. ", "original_text": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13671ef4-4481-46d9-b05a-e3ad81862097", "node_type": "1", "metadata": {"window": "In order to better cover the degradation space of the LQ images, we employ a\ncomprehensive degradation model that considers diversified degradation andhigh-order degradation .\n Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module. ", "original_text": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice. "}, "hash": "7366a5044f041b20701a526d2f1daed2d63d7520fd752f95deb0644eb3909a2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2509fc2a-c8f5-45b2-a7f3-887211b78c35", "node_type": "1", "metadata": {"window": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). ", "original_text": "Restoration Module. "}, "hash": "0171a55e0ac9ffc69b2071ae9377342c0c73ff6b07c96ebf8234ecb421b74bbc", "class_name": "RelatedNodeInfo"}}, "hash": "afd35d8e7aee8b579cdb415d99a515f849819d112595a2cf28fcc75eb56b8413", "text": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n", "start_char_idx": 14621, "end_char_idx": 14762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2509fc2a-c8f5-45b2-a7f3-887211b78c35": {"__data__": {"id_": "2509fc2a-c8f5-45b2-a7f3-887211b78c35", "embedding": null, "metadata": {"window": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). ", "original_text": "Restoration Module. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5ea6a92-abe3-4cb5-a163-620674cb2398", "node_type": "1", "metadata": {"window": "Among all degradations, blur,resize , and noise are the three key factors in real-world scenarios\n[64].  Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. ", "original_text": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n"}, "hash": "afd35d8e7aee8b579cdb415d99a515f849819d112595a2cf28fcc75eb56b8413", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75f1e5f6-0ec8-439b-ac40-0956e9fc29da", "node_type": "1", "metadata": {"window": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module. ", "original_text": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. "}, "hash": "6047aa083d99ee6d9172ad1a2cef60f7639560f561a9ff99fe3b0086394c37e6", "class_name": "RelatedNodeInfo"}}, "hash": "0171a55e0ac9ffc69b2071ae9377342c0c73ff6b07c96ebf8234ecb421b74bbc", "text": "Restoration Module. ", "start_char_idx": 14762, "end_char_idx": 14782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75f1e5f6-0ec8-439b-ac40-0956e9fc29da": {"__data__": {"id_": "75f1e5f6-0ec8-439b-ac40-0956e9fc29da", "embedding": null, "metadata": {"window": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module. ", "original_text": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2509fc2a-c8f5-45b2-a7f3-887211b78c35", "node_type": "1", "metadata": {"window": "Our diversified degradation involves blur: isotropic Gaussian and anisotropic Gaussian kernels;\nresize : area resize, bilinear interpolation and bicubic resize; noise : additive Gaussian noise, Poisson\nnoise, and JPEG compression noise.  Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). ", "original_text": "Restoration Module. "}, "hash": "0171a55e0ac9ffc69b2071ae9377342c0c73ff6b07c96ebf8234ecb421b74bbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "495e43cd-2597-4f13-adc8-cb97d8c8e7c7", "node_type": "1", "metadata": {"window": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. ", "original_text": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). "}, "hash": "7fe90e2a8a64dbe9c008c039111c0e9246671f317e0e1c332a3f0302c0d64a60", "class_name": "RelatedNodeInfo"}}, "hash": "6047aa083d99ee6d9172ad1a2cef60f7639560f561a9ff99fe3b0086394c37e6", "text": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. ", "start_char_idx": 14782, "end_char_idx": 15076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "495e43cd-2597-4f13-adc8-cb97d8c8e7c7": {"__data__": {"id_": "495e43cd-2597-4f13-adc8-cb97d8c8e7c7", "embedding": null, "metadata": {"window": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. ", "original_text": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75f1e5f6-0ec8-439b-ac40-0956e9fc29da", "node_type": "1", "metadata": {"window": "Regarding high-order degradation , we follow [ 55] to use the\nsecond-order degradation, which repeats the classical degradation model: blur-resize -noise process\n4\n\ntwice.  Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module. ", "original_text": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation. "}, "hash": "6047aa083d99ee6d9172ad1a2cef60f7639560f561a9ff99fe3b0086394c37e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3621708-7c95-47a7-8b1b-98581467aa5a", "node_type": "1", "metadata": {"window": "Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. ", "original_text": "We modify SwinIR [ 36] as our restoration module. "}, "hash": "14ac43efd089ba02fca67d95f7a61386cfa317c445080c9a1266399c3ae28565", "class_name": "RelatedNodeInfo"}}, "hash": "7fe90e2a8a64dbe9c008c039111c0e9246671f317e0e1c332a3f0302c0d64a60", "text": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). ", "start_char_idx": 15076, "end_char_idx": 15300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3621708-7c95-47a7-8b1b-98581467aa5a": {"__data__": {"id_": "c3621708-7c95-47a7-8b1b-98581467aa5a", "embedding": null, "metadata": {"window": "Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. ", "original_text": "We modify SwinIR [ 36] as our restoration module. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "495e43cd-2597-4f13-adc8-cb97d8c8e7c7", "node_type": "1", "metadata": {"window": "Note that our degradation model is designed for image restoration, thus all the degraded\nimages will be resized back to their original size.\n Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. ", "original_text": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3). "}, "hash": "7fe90e2a8a64dbe9c008c039111c0e9246671f317e0e1c332a3f0302c0d64a60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93723c87-5488-4771-9776-6672db8b8e7a", "node_type": "1", "metadata": {"window": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. ", "original_text": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. "}, "hash": "444e60e4bd74506c722cfeda210f10fb2b8fb95b40bf8eb5d260456d2600b30a", "class_name": "RelatedNodeInfo"}}, "hash": "14ac43efd089ba02fca67d95f7a61386cfa317c445080c9a1266399c3ae28565", "text": "We modify SwinIR [ 36] as our restoration module. ", "start_char_idx": 15300, "end_char_idx": 15350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93723c87-5488-4771-9776-6672db8b8e7a": {"__data__": {"id_": "93723c87-5488-4771-9776-6672db8b8e7a", "embedding": null, "metadata": {"window": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. ", "original_text": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3621708-7c95-47a7-8b1b-98581467aa5a", "node_type": "1", "metadata": {"window": "Restoration Module.  To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. ", "original_text": "We modify SwinIR [ 36] as our restoration module. "}, "hash": "14ac43efd089ba02fca67d95f7a61386cfa317c445080c9a1266399c3ae28565", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2bc1c73-b4b3-4390-b218-983589b51d27", "node_type": "1", "metadata": {"window": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). ", "original_text": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. "}, "hash": "60acc9eba020044a720e104a0c0c96e203e78f40939202c7e1a8aa1c45c5554f", "class_name": "RelatedNodeInfo"}}, "hash": "444e60e4bd74506c722cfeda210f10fb2b8fb95b40bf8eb5d260456d2600b30a", "text": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. ", "start_char_idx": 15350, "end_char_idx": 15485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2bc1c73-b4b3-4390-b218-983589b51d27": {"__data__": {"id_": "a2bc1c73-b4b3-4390-b218-983589b51d27", "embedding": null, "metadata": {"window": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). ", "original_text": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93723c87-5488-4771-9776-6672db8b8e7a", "node_type": "1", "metadata": {"window": "To build a robust generative image restoration pipeline, we adopt a conservative\nyet feasible solution by first removing most of the degradations (especially the noise and compression\nartifacts) in the LQ images, and then use the subsequent generative module to reproduce the lost\ninformation.  This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. ", "original_text": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8. "}, "hash": "444e60e4bd74506c722cfeda210f10fb2b8fb95b40bf8eb5d260456d2600b30a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02de84cb-85c0-4d9c-85d5-a52d503d661d", "node_type": "1", "metadata": {"window": "We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. ", "original_text": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. "}, "hash": "b9b0a1a8baa7ec15a51776d3df54f30b01e5e0569a26d1191c13b86dc15ab778", "class_name": "RelatedNodeInfo"}}, "hash": "60acc9eba020044a720e104a0c0c96e203e78f40939202c7e1a8aa1c45c5554f", "text": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. ", "start_char_idx": 15485, "end_char_idx": 15559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02de84cb-85c0-4d9c-85d5-a52d503d661d": {"__data__": {"id_": "02de84cb-85c0-4d9c-85d5-a52d503d661d", "embedding": null, "metadata": {"window": "We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. ", "original_text": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2bc1c73-b4b3-4390-b218-983589b51d27", "node_type": "1", "metadata": {"window": "This design will promote the latent diffusion model to focus more on textures/details\ngeneration without the distraction of noise corruption, and achieve more realistic/sharp results without\nwrong details (see Section 4.3).  We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). ", "original_text": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction. "}, "hash": "60acc9eba020044a720e104a0c0c96e203e78f40939202c7e1a8aa1c45c5554f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5a477a2-3f67-46b7-8ee1-fdeff571a1f9", "node_type": "1", "metadata": {"window": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. ", "original_text": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). "}, "hash": "2f6b6f8b5edab3e6f1f78d2207fb1ca3d09905e37826238f18d25aeb5295abe5", "class_name": "RelatedNodeInfo"}}, "hash": "b9b0a1a8baa7ec15a51776d3df54f30b01e5e0569a26d1191c13b86dc15ab778", "text": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. ", "start_char_idx": 15559, "end_char_idx": 15684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5a477a2-3f67-46b7-8ee1-fdeff571a1f9": {"__data__": {"id_": "c5a477a2-3f67-46b7-8ee1-fdeff571a1f9", "embedding": null, "metadata": {"window": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. ", "original_text": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02de84cb-85c0-4d9c-85d5-a52d503d661d", "node_type": "1", "metadata": {"window": "We modify SwinIR [ 36] as our restoration module.  Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. ", "original_text": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model. "}, "hash": "b9b0a1a8baa7ec15a51776d3df54f30b01e5e0569a26d1191c13b86dc15ab778", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a6a2fe9-9a10-4ba6-8d07-9abc3d1f27d0", "node_type": "1", "metadata": {"window": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss. ", "original_text": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. "}, "hash": "4455e4a650c0d8a96daaa9d62e59e664d8ad2a42bb836e1f9280c6b1262521c2", "class_name": "RelatedNodeInfo"}}, "hash": "2f6b6f8b5edab3e6f1f78d2207fb1ca3d09905e37826238f18d25aeb5295abe5", "text": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). ", "start_char_idx": 15684, "end_char_idx": 15825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a6a2fe9-9a10-4ba6-8d07-9abc3d1f27d0": {"__data__": {"id_": "1a6a2fe9-9a10-4ba6-8d07-9abc3d1f27d0", "embedding": null, "metadata": {"window": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss. ", "original_text": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5a477a2-3f67-46b7-8ee1-fdeff571a1f9", "node_type": "1", "metadata": {"window": "Specifically, we\nutilize the pixel unshuffle [ 50] operation to downsample the original low-quality input ILQwith a\nscale factor of 8.  Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. ", "original_text": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL). "}, "hash": "2f6b6f8b5edab3e6f1f78d2207fb1ca3d09905e37826238f18d25aeb5295abe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46e0dda1-f936-48c3-b22c-ff5212489ded", "node_type": "1", "metadata": {"window": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. ", "original_text": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. "}, "hash": "97e6e6ae8b7644afd5e29536e0f256fb6dfed292991abc91ef4900fbb8000d81", "class_name": "RelatedNodeInfo"}}, "hash": "4455e4a650c0d8a96daaa9d62e59e664d8ad2a42bb836e1f9280c6b1262521c2", "text": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. ", "start_char_idx": 15825, "end_char_idx": 15936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46e0dda1-f936-48c3-b22c-ff5212489ded": {"__data__": {"id_": "46e0dda1-f936-48c3-b22c-ff5212489ded", "embedding": null, "metadata": {"window": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. ", "original_text": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a6a2fe9-9a10-4ba6-8d07-9abc3d1f27d0", "node_type": "1", "metadata": {"window": "Then, a 3\u00d73convolutional layer is adopted for shallow feature extraction.  All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss. ", "original_text": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information. "}, "hash": "4455e4a650c0d8a96daaa9d62e59e664d8ad2a42bb836e1f9280c6b1262521c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42c67bed-fca5-4f55-9b4f-009b25892767", "node_type": "1", "metadata": {"window": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n", "original_text": "We optimize the parameters of the restoration module by minimizing the L2pixel loss. "}, "hash": "b5e8ede13bacd2564e9656a018e3005f9958fe4fcdd53d03d18e0ed4e5cda63e", "class_name": "RelatedNodeInfo"}}, "hash": "97e6e6ae8b7644afd5e29536e0f256fb6dfed292991abc91ef4900fbb8000d81", "text": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. ", "start_char_idx": 15936, "end_char_idx": 16159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42c67bed-fca5-4f55-9b4f-009b25892767": {"__data__": {"id_": "42c67bed-fca5-4f55-9b4f-009b25892767", "embedding": null, "metadata": {"window": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n", "original_text": "We optimize the parameters of the restoration module by minimizing the L2pixel loss. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46e0dda1-f936-48c3-b22c-ff5212489ded", "node_type": "1", "metadata": {"window": "All\nthe subsequent transformer operations are performed in low resolution space, which is similar to\nlatent diffusion model.  The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. ", "original_text": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer. "}, "hash": "97e6e6ae8b7644afd5e29536e0f256fb6dfed292991abc91ef4900fbb8000d81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3806bbf1-78d0-4360-9b31-0a60f8a10a5f", "node_type": "1", "metadata": {"window": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. ", "original_text": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. "}, "hash": "e8df5ad84bcc9db7eb0100023e36eca5ba4553d877238b2ec67a14d83f774e22", "class_name": "RelatedNodeInfo"}}, "hash": "b5e8ede13bacd2564e9656a018e3005f9958fe4fcdd53d03d18e0ed4e5cda63e", "text": "We optimize the parameters of the restoration module by minimizing the L2pixel loss. ", "start_char_idx": 16159, "end_char_idx": 16244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3806bbf1-78d0-4360-9b31-0a60f8a10a5f": {"__data__": {"id_": "3806bbf1-78d0-4360-9b31-0a60f8a10a5f", "embedding": null, "metadata": {"window": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. ", "original_text": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42c67bed-fca5-4f55-9b4f-009b25892767", "node_type": "1", "metadata": {"window": "The deep feature extraction adopts several Residual Swin Transformer Blocks\n(RSTB), and each RSTB has several Swin Transformer Layers (STL).  The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n", "original_text": "We optimize the parameters of the restoration module by minimizing the L2pixel loss. "}, "hash": "b5e8ede13bacd2564e9656a018e3005f9958fe4fcdd53d03d18e0ed4e5cda63e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "678fa503-2bce-40f0-ba88-fc0ccf92a4c2", "node_type": "1", "metadata": {"window": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. ", "original_text": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n"}, "hash": "3e8cd75bc98ddf79e00e9f3697b82adc13ed932d3a7cb7edb6228994c2450e3d", "class_name": "RelatedNodeInfo"}}, "hash": "e8df5ad84bcc9db7eb0100023e36eca5ba4553d877238b2ec67a14d83f774e22", "text": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. ", "start_char_idx": 16244, "end_char_idx": 16411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "678fa503-2bce-40f0-ba88-fc0ccf92a4c2": {"__data__": {"id_": "678fa503-2bce-40f0-ba88-fc0ccf92a4c2", "embedding": null, "metadata": {"window": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. ", "original_text": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3806bbf1-78d0-4360-9b31-0a60f8a10a5f", "node_type": "1", "metadata": {"window": "The shallow and deep features\nwill be added for maintaining both low-frequency and high-frequency information.  For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. ", "original_text": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively. "}, "hash": "e8df5ad84bcc9db7eb0100023e36eca5ba4553d877238b2ec67a14d83f774e22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04ee7a19-562d-43d6-86c1-8548f4b098ee", "node_type": "1", "metadata": {"window": "We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. ", "original_text": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. "}, "hash": "c5f83ab8ae7b8e82e1d59acaf40328732eeda2635398115bbd7916a4ffb14c11", "class_name": "RelatedNodeInfo"}}, "hash": "3e8cd75bc98ddf79e00e9f3697b82adc13ed932d3a7cb7edb6228994c2450e3d", "text": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n", "start_char_idx": 16411, "end_char_idx": 16514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04ee7a19-562d-43d6-86c1-8548f4b098ee": {"__data__": {"id_": "04ee7a19-562d-43d6-86c1-8548f4b098ee", "embedding": null, "metadata": {"window": "We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. ", "original_text": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "678fa503-2bce-40f0-ba88-fc0ccf92a4c2", "node_type": "1", "metadata": {"window": "For upsampling\nthe deep features back to the original image space, we perform nearest interpolation for three times,\nand each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation\nlayer.  We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. ", "original_text": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n"}, "hash": "3e8cd75bc98ddf79e00e9f3697b82adc13ed932d3a7cb7edb6228994c2450e3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83dbe5a6-94fe-4e7d-8c76-b84a51e03d11", "node_type": "1", "metadata": {"window": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. ", "original_text": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. "}, "hash": "974dc0539be1fed8deb67f3044eb6232b54a936151c99154ec76d3486d2c21b2", "class_name": "RelatedNodeInfo"}}, "hash": "c5f83ab8ae7b8e82e1d59acaf40328732eeda2635398115bbd7916a4ffb14c11", "text": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. ", "start_char_idx": 16514, "end_char_idx": 16600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83dbe5a6-94fe-4e7d-8c76-b84a51e03d11": {"__data__": {"id_": "83dbe5a6-94fe-4e7d-8c76-b84a51e03d11", "embedding": null, "metadata": {"window": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. ", "original_text": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04ee7a19-562d-43d6-86c1-8548f4b098ee", "node_type": "1", "metadata": {"window": "We optimize the parameters of the restoration module by minimizing the L2pixel loss.  The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. ", "original_text": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion. "}, "hash": "c5f83ab8ae7b8e82e1d59acaf40328732eeda2635398115bbd7916a4ffb14c11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14abff48-9c37-4f8e-aab9-959750f29fd5", "node_type": "1", "metadata": {"window": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space. ", "original_text": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. "}, "hash": "8b85c4347580d44b1a6fb2e184008a66750b1f770efb86d92f61d3c8bb22a4dc", "class_name": "RelatedNodeInfo"}}, "hash": "974dc0539be1fed8deb67f3044eb6232b54a936151c99154ec76d3486d2c21b2", "text": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. ", "start_char_idx": 16600, "end_char_idx": 16721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14abff48-9c37-4f8e-aab9-959750f29fd5": {"__data__": {"id_": "14abff48-9c37-4f8e-aab9-959750f29fd5", "embedding": null, "metadata": {"window": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space. ", "original_text": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83dbe5a6-94fe-4e7d-8c76-b84a51e03d11", "node_type": "1", "metadata": {"window": "The\nformulation is as follows:\nIreg=SwinIR (ILQ),Lreg=||Ireg\u2212IHQ||2\n2, (1)\nwhere IHQandILQdenote the high-quality image and the low-quality counterpart, respectively.  Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. ", "original_text": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion. "}, "hash": "974dc0539be1fed8deb67f3044eb6232b54a936151c99154ec76d3486d2c21b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cd2ea3b-2962-42f6-84d8-f5761861bd91", "node_type": "1", "metadata": {"window": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. ", "original_text": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. "}, "hash": "ed3cede6d2b1df909df45959d9bac7f579a40ebef0f18eb4ba5fb39a4fd63418", "class_name": "RelatedNodeInfo"}}, "hash": "8b85c4347580d44b1a6fb2e184008a66750b1f770efb86d92f61d3c8bb22a4dc", "text": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. ", "start_char_idx": 16721, "end_char_idx": 16848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cd2ea3b-2962-42f6-84d8-f5761861bd91": {"__data__": {"id_": "1cd2ea3b-2962-42f6-84d8-f5761861bd91", "embedding": null, "metadata": {"window": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. ", "original_text": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14abff48-9c37-4f8e-aab9-959750f29fd5", "node_type": "1", "metadata": {"window": "Ireg\nis obtained by regression learning and will be used for the finetuning on latent diffusion model.\n 3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space. ", "original_text": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution. "}, "hash": "8b85c4347580d44b1a6fb2e184008a66750b1f770efb86d92f61d3c8bb22a4dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "913ee0ee-58e5-4826-a7c6-099db479c482", "node_type": "1", "metadata": {"window": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n", "original_text": "The diffusion and denoising processes are performed in the latent space. "}, "hash": "b214521dbf86bc15c0dc1ac10c47b4e5660978f2c5c49f2b3fc1e86536a244f6", "class_name": "RelatedNodeInfo"}}, "hash": "ed3cede6d2b1df909df45959d9bac7f579a40ebef0f18eb4ba5fb39a4fd63418", "text": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. ", "start_char_idx": 16848, "end_char_idx": 17161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "913ee0ee-58e5-4826-a7c6-099db479c482": {"__data__": {"id_": "913ee0ee-58e5-4826-a7c6-099db479c482", "embedding": null, "metadata": {"window": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n", "original_text": "The diffusion and denoising processes are performed in the latent space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cd2ea3b-2962-42f6-84d8-f5761861bd91", "node_type": "1", "metadata": {"window": "3.2 Leverage Generative Prior for Image Reconstruction\nPreliminary: Stable Diffusion.  In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. ", "original_text": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67]. "}, "hash": "ed3cede6d2b1df909df45959d9bac7f579a40ebef0f18eb4ba5fb39a4fd63418", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "070c0135-aaa4-4288-b5c9-c893ced04488", "node_type": "1", "metadata": {"window": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n", "original_text": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. "}, "hash": "ca6734d5befeb1b7dcd8d0732f8ee9962afe7ac3e35c50ca022d934a6577bf88", "class_name": "RelatedNodeInfo"}}, "hash": "b214521dbf86bc15c0dc1ac10c47b4e5660978f2c5c49f2b3fc1e86536a244f6", "text": "The diffusion and denoising processes are performed in the latent space. ", "start_char_idx": 17161, "end_char_idx": 17234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "070c0135-aaa4-4288-b5c9-c893ced04488": {"__data__": {"id_": "070c0135-aaa4-4288-b5c9-c893ced04488", "embedding": null, "metadata": {"window": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n", "original_text": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "913ee0ee-58e5-4826-a7c6-099db479c482", "node_type": "1", "metadata": {"window": "In this paper, we implement our method based on the large-scale\ntext-to-image latent diffusion model \u2013 Stable Diffusion.  Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n", "original_text": "The diffusion and denoising processes are performed in the latent space. "}, "hash": "b214521dbf86bc15c0dc1ac10c47b4e5660978f2c5c49f2b3fc1e86536a244f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48005c0a-b136-4801-bdf2-5aeecf19e1b7", "node_type": "1", "metadata": {"window": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet. ", "original_text": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n"}, "hash": "14113dc4fa5b199335e5cbcf495388551a376f077728bc31f84f5205a73f8641", "class_name": "RelatedNodeInfo"}}, "hash": "ca6734d5befeb1b7dcd8d0732f8ee9962afe7ac3e35c50ca022d934a6577bf88", "text": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. ", "start_char_idx": 17234, "end_char_idx": 17435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48005c0a-b136-4801-bdf2-5aeecf19e1b7": {"__data__": {"id_": "48005c0a-b136-4801-bdf2-5aeecf19e1b7", "embedding": null, "metadata": {"window": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet. ", "original_text": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "070c0135-aaa4-4288-b5c9-c893ced04488", "node_type": "1", "metadata": {"window": "Diffusion models learn to generate data\nsamples through a denoising sequence that estimate the score of the data distribution.  In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n", "original_text": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s. "}, "hash": "ca6734d5befeb1b7dcd8d0732f8ee9962afe7ac3e35c50ca022d934a6577bf88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b45a12c-a4d3-4c52-bead-ad68bd0d4db0", "node_type": "1", "metadata": {"window": "The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. ", "original_text": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n"}, "hash": "e586c37fc347ae3d0bf7ff2c1b158a1f944cbef62c328db1baf0d273399c7a32", "class_name": "RelatedNodeInfo"}}, "hash": "14113dc4fa5b199335e5cbcf495388551a376f077728bc31f84f5205a73f8641", "text": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n", "start_char_idx": 17435, "end_char_idx": 17515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b45a12c-a4d3-4c52-bead-ad68bd0d4db0": {"__data__": {"id_": "6b45a12c-a4d3-4c52-bead-ad68bd0d4db0", "embedding": null, "metadata": {"window": "The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. ", "original_text": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48005c0a-b136-4801-bdf2-5aeecf19e1b7", "node_type": "1", "metadata": {"window": "In order\nto achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder [ 29]\nthat converts an image xinto a latent zwith encoder Eand reconstructs it with decoder D. This\nlatent representation is learned by using hybrid objectives of V AE [ 30], Patch-GAN [ 23], and LPIPS\n[67].  The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet. ", "original_text": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n"}, "hash": "14113dc4fa5b199335e5cbcf495388551a376f077728bc31f84f5205a73f8641", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29a8f5b4-53ea-4370-aa21-12ab943fe281", "node_type": "1", "metadata": {"window": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. ", "original_text": "LAControlNet. "}, "hash": "234c09c92c65bbdfedf7e2d8049fe49b3339dd2fc2f58febbbc4f60890e875be", "class_name": "RelatedNodeInfo"}}, "hash": "e586c37fc347ae3d0bf7ff2c1b158a1f944cbef62c328db1baf0d273399c7a32", "text": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n", "start_char_idx": 17515, "end_char_idx": 17887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29a8f5b4-53ea-4370-aa21-12ab943fe281": {"__data__": {"id_": "29a8f5b4-53ea-4370-aa21-12ab943fe281", "embedding": null, "metadata": {"window": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. ", "original_text": "LAControlNet. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b45a12c-a4d3-4c52-bead-ad68bd0d4db0", "node_type": "1", "metadata": {"window": "The diffusion and denoising processes are performed in the latent space.  In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. ", "original_text": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n"}, "hash": "e586c37fc347ae3d0bf7ff2c1b158a1f944cbef62c328db1baf0d273399c7a32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0520a598-6f1d-438b-8abc-c8030bd3d7de", "node_type": "1", "metadata": {"window": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). ", "original_text": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. "}, "hash": "3fb3adca96c48e2d2d0c881d629fe02abfea43de996dda21fac641066833d125", "class_name": "RelatedNodeInfo"}}, "hash": "234c09c92c65bbdfedf7e2d8049fe49b3339dd2fc2f58febbbc4f60890e875be", "text": "LAControlNet. ", "start_char_idx": 17887, "end_char_idx": 17901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0520a598-6f1d-438b-8abc-c8030bd3d7de": {"__data__": {"id_": "0520a598-6f1d-438b-8abc-c8030bd3d7de", "embedding": null, "metadata": {"window": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). ", "original_text": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29a8f5b4-53ea-4370-aa21-12ab943fe281", "node_type": "1", "metadata": {"window": "In diffusion process,\nGaussian noise with variance \u03b2t\u2208(0,1)at time tis added to the encoded latent z=E(x)for\nproducing the noisy latent:\nzt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, (2)\nwhere \u03f5\u223c N(0,I),\u03b1t= 1\u2212\u03b2tand\u00af\u03b1t=Qt\ns=1\u03b1s.  When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. ", "original_text": "LAControlNet. "}, "hash": "234c09c92c65bbdfedf7e2d8049fe49b3339dd2fc2f58febbbc4f60890e875be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d782296-551d-4f09-b0eb-a8e7ad1d40f7", "node_type": "1", "metadata": {"window": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. ", "original_text": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. "}, "hash": "c3a3c2df323d240ff12644e8e3dd9342404eee2369d645fb7204324ac34dcbb1", "class_name": "RelatedNodeInfo"}}, "hash": "3fb3adca96c48e2d2d0c881d629fe02abfea43de996dda21fac641066833d125", "text": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. ", "start_char_idx": 17901, "end_char_idx": 18060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d782296-551d-4f09-b0eb-a8e7ad1d40f7": {"__data__": {"id_": "0d782296-551d-4f09-b0eb-a8e7ad1d40f7", "embedding": null, "metadata": {"window": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. ", "original_text": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0520a598-6f1d-438b-8abc-c8030bd3d7de", "node_type": "1", "metadata": {"window": "When tis large enough, the latent ztis nearly a\nstandard Gaussian distribution.\n A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). ", "original_text": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images. "}, "hash": "3fb3adca96c48e2d2d0c881d629fe02abfea43de996dda21fac641066833d125", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1d93519-ba46-4f0c-8a3b-fd876e1bfc21", "node_type": "1", "metadata": {"window": "LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales. ", "original_text": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). "}, "hash": "2500a71824679c584b7e4d4fe9385c953ace52998eda3dd63ebb5303954544f7", "class_name": "RelatedNodeInfo"}}, "hash": "c3a3c2df323d240ff12644e8e3dd9342404eee2369d645fb7204324ac34dcbb1", "text": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. ", "start_char_idx": 18060, "end_char_idx": 18168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1d93519-ba46-4f0c-8a3b-fd876e1bfc21": {"__data__": {"id_": "f1d93519-ba46-4f0c-8a3b-fd876e1bfc21", "embedding": null, "metadata": {"window": "LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales. ", "original_text": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d782296-551d-4f09-b0eb-a8e7ad1d40f7", "node_type": "1", "metadata": {"window": "A network \u03f5\u03b8is learned by predicting the noise \u03f5conditioned on c(i.e., text prompts) at a randomly\npicked time-step t. The optimization of latent diffusion model is defined as follows:\nLldm=Ez,c,t,\u03f5 [||\u03f5\u2212\u03f5\u03b8(zt=\u221a\u00af\u03b1tz+\u221a\n1\u2212\u00af\u03b1t\u03f5, c, t)||2\n2], (3)\nwhere x, care sampled from the dataset and z=E(x),tis uniformly sampled and \u03f5is sampled from\nthe standard Gaussian distribution.\n LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. ", "original_text": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs. "}, "hash": "c3a3c2df323d240ff12644e8e3dd9342404eee2369d645fb7204324ac34dcbb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "629e172c-a952-439d-be4b-3daa3ce21ad4", "node_type": "1", "metadata": {"window": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. ", "original_text": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. "}, "hash": "25b9de0468f6ff156c8e7c878adebfa01cce6015a4f4b92042a729cde850d494", "class_name": "RelatedNodeInfo"}}, "hash": "2500a71824679c584b7e4d4fe9385c953ace52998eda3dd63ebb5303954544f7", "text": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). ", "start_char_idx": 18168, "end_char_idx": 18311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "629e172c-a952-439d-be4b-3daa3ce21ad4": {"__data__": {"id_": "629e172c-a952-439d-be4b-3daa3ce21ad4", "embedding": null, "metadata": {"window": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. ", "original_text": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1d93519-ba46-4f0c-8a3b-fd876e1bfc21", "node_type": "1", "metadata": {"window": "LAControlNet.  Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales. ", "original_text": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg). "}, "hash": "2500a71824679c584b7e4d4fe9385c953ace52998eda3dd63ebb5303954544f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc2b3f85-e35a-4544-a3ec-0a0258d53062", "node_type": "1", "metadata": {"window": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. ", "original_text": "In particular, the decoder receives the features from\nencoder and fuses them in different scales. "}, "hash": "33081daf0a23111491f2fc7c0e559fa5d20ea3a492b2a8316e77b567e2e2d169", "class_name": "RelatedNodeInfo"}}, "hash": "25b9de0468f6ff156c8e7c878adebfa01cce6015a4f4b92042a729cde850d494", "text": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. ", "start_char_idx": 18311, "end_char_idx": 18420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc2b3f85-e35a-4544-a3ec-0a0258d53062": {"__data__": {"id_": "cc2b3f85-e35a-4544-a3ec-0a0258d53062", "embedding": null, "metadata": {"window": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. ", "original_text": "In particular, the decoder receives the features from\nencoder and fuses them in different scales. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "629e172c-a952-439d-be4b-3daa3ce21ad4", "node_type": "1", "metadata": {"window": "Although stage-one could remove most degradations, the obtained Iregis often\nover-smoothed and still far from the distribution of high-quality natural images.  We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. ", "original_text": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder. "}, "hash": "25b9de0468f6ff156c8e7c878adebfa01cce6015a4f4b92042a729cde850d494", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc3c035c-e442-435e-bfca-2cf4c39fc81b", "node_type": "1", "metadata": {"window": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. ", "original_text": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. "}, "hash": "376d79bf19e2ebef179141fdf0e648ebf64f036392826698144841b1296cc252", "class_name": "RelatedNodeInfo"}}, "hash": "33081daf0a23111491f2fc7c0e559fa5d20ea3a492b2a8316e77b567e2e2d169", "text": "In particular, the decoder receives the features from\nencoder and fuses them in different scales. ", "start_char_idx": 18420, "end_char_idx": 18518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc3c035c-e442-435e-bfca-2cf4c39fc81b": {"__data__": {"id_": "fc3c035c-e442-435e-bfca-2cf4c39fc81b", "embedding": null, "metadata": {"window": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. ", "original_text": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc2b3f85-e35a-4544-a3ec-0a0258d53062", "node_type": "1", "metadata": {"window": "We then leverage\nthe pre-trained Stable Diffusion for image reconstruction with our obtained Ireg-IHQpairs.  First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. ", "original_text": "In particular, the decoder receives the features from\nencoder and fuses them in different scales. "}, "hash": "33081daf0a23111491f2fc7c0e559fa5d20ea3a492b2a8316e77b567e2e2d169", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b53d5e8c-663e-40f8-a993-28bc575b5264", "node_type": "1", "metadata": {"window": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder. ", "original_text": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. "}, "hash": "12d9f637a1ed94e7bbf56fc82e28e6cc8c2080db70841821c3989c4c548d3325", "class_name": "RelatedNodeInfo"}}, "hash": "376d79bf19e2ebef179141fdf0e648ebf64f036392826698144841b1296cc252", "text": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. ", "start_char_idx": 18518, "end_char_idx": 18660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b53d5e8c-663e-40f8-a993-28bc575b5264": {"__data__": {"id_": "b53d5e8c-663e-40f8-a993-28bc575b5264", "embedding": null, "metadata": {"window": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder. ", "original_text": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc3c035c-e442-435e-bfca-2cf4c39fc81b", "node_type": "1", "metadata": {"window": "First,\nwe utilize the encoder of Stable Diffusion\u2019s pretrained V AE to map Ireginto the latent space, and\nobtain the condition latent E(Ireg).  The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. ", "original_text": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser. "}, "hash": "376d79bf19e2ebef179141fdf0e648ebf64f036392826698144841b1296cc252", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba0a66c8-f78a-45af-860b-0d8920f9abcc", "node_type": "1", "metadata": {"window": "In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. ", "original_text": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. "}, "hash": "11250dda877f774674921eae065b95d93016d3950edd1e3895098d5a3c14c0d3", "class_name": "RelatedNodeInfo"}}, "hash": "12d9f637a1ed94e7bbf56fc82e28e6cc8c2080db70841821c3989c4c548d3325", "text": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. ", "start_char_idx": 18660, "end_char_idx": 18781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba0a66c8-f78a-45af-860b-0d8920f9abcc": {"__data__": {"id_": "ba0a66c8-f78a-45af-860b-0d8920f9abcc", "embedding": null, "metadata": {"window": "In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. ", "original_text": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b53d5e8c-663e-40f8-a993-28bc575b5264", "node_type": "1", "metadata": {"window": "The UNet [ 47] denoiser performs latent diffusion, which contains\nan encoder, a middle block, and a decoder.  In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder. ", "original_text": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module. "}, "hash": "12d9f637a1ed94e7bbf56fc82e28e6cc8c2080db70841821c3989c4c548d3325", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7fb0487-861c-4b5c-8777-6298ebd7f042", "node_type": "1", "metadata": {"window": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n", "original_text": "The outputs of the\nparallel module are added to the original UNet decoder. "}, "hash": "e6622a4245ee2160f170c23444f3b309ff596548ac1c427051a5793a4135e4cc", "class_name": "RelatedNodeInfo"}}, "hash": "11250dda877f774674921eae065b95d93016d3950edd1e3895098d5a3c14c0d3", "text": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. ", "start_char_idx": 18781, "end_char_idx": 19044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7fb0487-861c-4b5c-8777-6298ebd7f042": {"__data__": {"id_": "b7fb0487-861c-4b5c-8777-6298ebd7f042", "embedding": null, "metadata": {"window": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n", "original_text": "The outputs of the\nparallel module are added to the original UNet decoder. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba0a66c8-f78a-45af-860b-0d8920f9abcc", "node_type": "1", "metadata": {"window": "In particular, the decoder receives the features from\nencoder and fuses them in different scales.  Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. ", "original_text": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints. "}, "hash": "11250dda877f774674921eae065b95d93016d3950edd1e3895098d5a3c14c0d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06723684-d28f-4264-ae93-3c45472e303f", "node_type": "1", "metadata": {"window": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. ", "original_text": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. "}, "hash": "1de82da7e46afd787eb1a338ccae5534cd42ee16c89a034fa31ff3eceed44e96", "class_name": "RelatedNodeInfo"}}, "hash": "e6622a4245ee2160f170c23444f3b309ff596548ac1c427051a5793a4135e4cc", "text": "The outputs of the\nparallel module are added to the original UNet decoder. ", "start_char_idx": 19044, "end_char_idx": 19119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06723684-d28f-4264-ae93-3c45472e303f": {"__data__": {"id_": "06723684-d28f-4264-ae93-3c45472e303f", "embedding": null, "metadata": {"window": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. ", "original_text": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7fb0487-861c-4b5c-8777-6298ebd7f042", "node_type": "1", "metadata": {"window": "Here we create a parallel module (denoted as orange in\nFigure 2) that contains the same encoder and the middle block as in the UNet denoiser.  Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n", "original_text": "The outputs of the\nparallel module are added to the original UNet decoder. "}, "hash": "e6622a4245ee2160f170c23444f3b309ff596548ac1c427051a5793a4135e4cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f4c2e00-0a66-4bb6-bf3f-bb07d53cfe39", "node_type": "1", "metadata": {"window": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff. ", "original_text": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n"}, "hash": "054a3edf9a7cc6e982107fff3a9fced3f2b1365aaab4e07dc214408846687a4c", "class_name": "RelatedNodeInfo"}}, "hash": "1de82da7e46afd787eb1a338ccae5534cd42ee16c89a034fa31ff3eceed44e96", "text": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. ", "start_char_idx": 19119, "end_char_idx": 19213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f4c2e00-0a66-4bb6-bf3f-bb07d53cfe39": {"__data__": {"id_": "8f4c2e00-0a66-4bb6-bf3f-bb07d53cfe39", "embedding": null, "metadata": {"window": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff. ", "original_text": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06723684-d28f-4264-ae93-3c45472e303f", "node_type": "1", "metadata": {"window": "Then, we\nconcatenate the condition latent E(Ireg)with the randomly sampled noisy ztas the input for the\nparallel module.  Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. ", "original_text": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale. "}, "hash": "1de82da7e46afd787eb1a338ccae5534cd42ee16c89a034fa31ff3eceed44e96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45579813-f07c-4402-ab9f-7bc300497375", "node_type": "1", "metadata": {"window": "The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. ", "original_text": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. "}, "hash": "2c903f835ea477ab8a59d213431ec3b282a32c3e8895f5d6a5d0e169b6a59185", "class_name": "RelatedNodeInfo"}}, "hash": "054a3edf9a7cc6e982107fff3a9fced3f2b1365aaab4e07dc214408846687a4c", "text": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n", "start_char_idx": 19213, "end_char_idx": 19363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45579813-f07c-4402-ab9f-7bc300497375": {"__data__": {"id_": "45579813-f07c-4402-ab9f-7bc300497375", "embedding": null, "metadata": {"window": "The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. ", "original_text": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f4c2e00-0a66-4bb6-bf3f-bb07d53cfe39", "node_type": "1", "metadata": {"window": "Since this concatenation operation will increase the channel number of the first\nconvolutional layer in the parallel module, we initialize the newly added parameters to zero, where\nall other weights are initialized from the pre-trained UNet denoiser checkpoints.  The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff. ", "original_text": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n"}, "hash": "054a3edf9a7cc6e982107fff3a9fced3f2b1365aaab4e07dc214408846687a4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04449c97-0212-4bbf-a878-131e565c272e", "node_type": "1", "metadata": {"window": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. ", "original_text": "(4)\nThe obtained result in this stage is denoted as Idiff. "}, "hash": "0699d1a7ecef47603b250767a00e39d4fddea18c5b6dc5a097192a873f2f5c50", "class_name": "RelatedNodeInfo"}}, "hash": "2c903f835ea477ab8a59d213431ec3b282a32c3e8895f5d6a5d0e169b6a59185", "text": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. ", "start_char_idx": 19363, "end_char_idx": 19481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04449c97-0212-4bbf-a878-131e565c272e": {"__data__": {"id_": "04449c97-0212-4bbf-a878-131e565c272e", "embedding": null, "metadata": {"window": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. ", "original_text": "(4)\nThe obtained result in this stage is denoted as Idiff. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45579813-f07c-4402-ab9f-7bc300497375", "node_type": "1", "metadata": {"window": "The outputs of the\nparallel module are added to the original UNet decoder.  Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. ", "original_text": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2]. "}, "hash": "2c903f835ea477ab8a59d213431ec3b282a32c3e8895f5d6a5d0e169b6a59185", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b497066-45bc-4644-a27f-b19751a365cc", "node_type": "1", "metadata": {"window": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. ", "original_text": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. "}, "hash": "b189a4c7682a271dc8a09c2f5d39987c83a5630beee98a999b7b6bcc3a14e0de", "class_name": "RelatedNodeInfo"}}, "hash": "0699d1a7ecef47603b250767a00e39d4fddea18c5b6dc5a097192a873f2f5c50", "text": "(4)\nThe obtained result in this stage is denoted as Idiff. ", "start_char_idx": 19481, "end_char_idx": 19540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b497066-45bc-4644-a27f-b19751a365cc": {"__data__": {"id_": "4b497066-45bc-4644-a27f-b19751a365cc", "embedding": null, "metadata": {"window": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. ", "original_text": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04449c97-0212-4bbf-a878-131e565c272e", "node_type": "1", "metadata": {"window": "Moreover, one 1\u00d71convolutional layer is\napplied before the addition operation for each scale.  During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. ", "original_text": "(4)\nThe obtained result in this stage is denoted as Idiff. "}, "hash": "0699d1a7ecef47603b250767a00e39d4fddea18c5b6dc5a097192a873f2f5c50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f790045d-75cb-4878-9105-e96e28c20c25", "node_type": "1", "metadata": {"window": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. ", "original_text": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. "}, "hash": "64ec3cb70773a1c6f1352b3cf84f3edef9895565cefe9bf6f2776cc3b8f4ef2d", "class_name": "RelatedNodeInfo"}}, "hash": "b189a4c7682a271dc8a09c2f5d39987c83a5630beee98a999b7b6bcc3a14e0de", "text": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. ", "start_char_idx": 19540, "end_char_idx": 19641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f790045d-75cb-4878-9105-e96e28c20c25": {"__data__": {"id_": "f790045d-75cb-4878-9105-e96e28c20c25", "embedding": null, "metadata": {"window": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. ", "original_text": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b497066-45bc-4644-a27f-b19751a365cc", "node_type": "1", "metadata": {"window": "During finetuning, the parallel module and these\n5\n\n1\u00d71convolutional layers are optimized simultaneously, where the prompt condition is set to empty.\n We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. ", "original_text": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task. "}, "hash": "b189a4c7682a271dc8a09c2f5d39987c83a5630beee98a999b7b6bcc3a14e0de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79e81508-68de-4331-9222-ab284c0cb200", "node_type": "1", "metadata": {"window": "(4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. ", "original_text": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. "}, "hash": "7e5d6a0e17925228beb87de5b3ee939372b5612c713b398da949ab7e58bebd8a", "class_name": "RelatedNodeInfo"}}, "hash": "64ec3cb70773a1c6f1352b3cf84f3edef9895565cefe9bf6f2776cc3b8f4ef2d", "text": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. ", "start_char_idx": 19641, "end_char_idx": 19774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79e81508-68de-4331-9222-ab284c0cb200": {"__data__": {"id_": "79e81508-68de-4331-9222-ab284c0cb200", "embedding": null, "metadata": {"window": "(4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. ", "original_text": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f790045d-75cb-4878-9105-e96e28c20c25", "node_type": "1", "metadata": {"window": "We aim to minimize the following latent diffusion objective:\nLDiff =Ezt,c,t,\u03f5,E(Ireg)[||\u03f5\u2212\u03f5\u03b8(zt, c, t,E(Ireg))||2\n2].  (4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. ", "original_text": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion. "}, "hash": "64ec3cb70773a1c6f1352b3cf84f3edef9895565cefe9bf6f2776cc3b8f4ef2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ab2e8b3-e8b5-48ce-8977-857dde8ac794", "node_type": "1", "metadata": {"window": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n", "original_text": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. "}, "hash": "f6668f2c3b01918593ef42ec3b8c5e159d5387575def4844baa99f974b7474ed", "class_name": "RelatedNodeInfo"}}, "hash": "7e5d6a0e17925228beb87de5b3ee939372b5612c713b398da949ab7e58bebd8a", "text": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. ", "start_char_idx": 19774, "end_char_idx": 20025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ab2e8b3-e8b5-48ce-8977-857dde8ac794": {"__data__": {"id_": "0ab2e8b3-e8b5-48ce-8977-857dde8ac794", "embedding": null, "metadata": {"window": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n", "original_text": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79e81508-68de-4331-9222-ab284c0cb200", "node_type": "1", "metadata": {"window": "(4)\nThe obtained result in this stage is denoted as Idiff.  To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. ", "original_text": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information. "}, "hash": "7e5d6a0e17925228beb87de5b3ee939372b5612c713b398da949ab7e58bebd8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36fd7f88-a727-41f6-8342-439255a48fae", "node_type": "1", "metadata": {"window": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. ", "original_text": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. "}, "hash": "b7808ed904b9256eebf9540c53f233996dbd880d4adb564d8f0053ffb8d3cf98", "class_name": "RelatedNodeInfo"}}, "hash": "f6668f2c3b01918593ef42ec3b8c5e159d5387575def4844baa99f974b7474ed", "text": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. ", "start_char_idx": 20025, "end_char_idx": 20178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36fd7f88-a727-41f6-8342-439255a48fae": {"__data__": {"id_": "36fd7f88-a727-41f6-8342-439255a48fae", "embedding": null, "metadata": {"window": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. ", "original_text": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ab2e8b3-e8b5-48ce-8977-857dde8ac794", "node_type": "1", "metadata": {"window": "To summarize, only the skip-connected features in\nthe UNet denoiser are tuned for our specific task.  This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n", "original_text": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables. "}, "hash": "f6668f2c3b01918593ef42ec3b8c5e159d5387575def4844baa99f974b7474ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96b0fc55-79ab-4d65-bd3c-654c8f71edd5", "node_type": "1", "metadata": {"window": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. ", "original_text": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n"}, "hash": "0983810a383080d442fa26ab4fffc2028adcd9c1c9c89d6f41849dd7b89140dc", "class_name": "RelatedNodeInfo"}}, "hash": "b7808ed904b9256eebf9540c53f233996dbd880d4adb564d8f0053ffb8d3cf98", "text": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. ", "start_char_idx": 20178, "end_char_idx": 20342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96b0fc55-79ab-4d65-bd3c-654c8f71edd5": {"__data__": {"id_": "96b0fc55-79ab-4d65-bd3c-654c8f71edd5", "embedding": null, "metadata": {"window": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. ", "original_text": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36fd7f88-a727-41f6-8342-439255a48fae", "node_type": "1", "metadata": {"window": "This strategy alleviates overfitting in small training\ndataset, and could inherit the high-quality generation from Stable Diffusion.  More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. ", "original_text": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information. "}, "hash": "b7808ed904b9256eebf9540c53f233996dbd880d4adb564d8f0053ffb8d3cf98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29d78105-794b-4253-9e0b-e5d2aef31361", "node_type": "1", "metadata": {"window": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. ", "original_text": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. "}, "hash": "a21c9dc4033a23e86867ea25ee2b33803c45452ed8c8b3217636d2821fea24d0", "class_name": "RelatedNodeInfo"}}, "hash": "0983810a383080d442fa26ab4fffc2028adcd9c1c9c89d6f41849dd7b89140dc", "text": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n", "start_char_idx": 20342, "end_char_idx": 20489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29d78105-794b-4253-9e0b-e5d2aef31361": {"__data__": {"id_": "29d78105-794b-4253-9e0b-e5d2aef31361", "embedding": null, "metadata": {"window": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. ", "original_text": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96b0fc55-79ab-4d65-bd3c-654c8f71edd5", "node_type": "1", "metadata": {"window": "More importantly,\nour conditioning mechanism is more straightforward and effective for image reconstruction task\ncompared to ControlNet [ 66], which utilizes an additional condition network trained from scratch\nfor encoding the condition information.  In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. ", "original_text": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n"}, "hash": "0983810a383080d442fa26ab4fffc2028adcd9c1c9c89d6f41849dd7b89140dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b0b288b-3e05-4deb-87e9-c98f464b41ab", "node_type": "1", "metadata": {"window": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. ", "original_text": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. "}, "hash": "1ce99dacc3557c86b333c79f36349c863666ad75277c6314f34b1cb4807af302", "class_name": "RelatedNodeInfo"}}, "hash": "a21c9dc4033a23e86867ea25ee2b33803c45452ed8c8b3217636d2821fea24d0", "text": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. ", "start_char_idx": 20489, "end_char_idx": 20725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b0b288b-3e05-4deb-87e9-c98f464b41ab": {"__data__": {"id_": "4b0b288b-3e05-4deb-87e9-c98f464b41ab", "embedding": null, "metadata": {"window": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. ", "original_text": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29d78105-794b-4253-9e0b-e5d2aef31361", "node_type": "1", "metadata": {"window": "In our LAControlNet, the well-trained V AE\u2019s encoder is\nable to project the condition images into the same representation space as the latent variables.  This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. ", "original_text": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences. "}, "hash": "a21c9dc4033a23e86867ea25ee2b33803c45452ed8c8b3217636d2821fea24d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3307634e-2960-4b3b-927d-cec61ea06947", "node_type": "1", "metadata": {"window": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. ", "original_text": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. "}, "hash": "30061c5cf3ae37dc0d4ec292d985944d7a1082869a29e8b4418eaffc5581266d", "class_name": "RelatedNodeInfo"}}, "hash": "1ce99dacc3557c86b333c79f36349c863666ad75277c6314f34b1cb4807af302", "text": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. ", "start_char_idx": 20725, "end_char_idx": 20909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3307634e-2960-4b3b-927d-cec61ea06947": {"__data__": {"id_": "3307634e-2960-4b3b-927d-cec61ea06947", "embedding": null, "metadata": {"window": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. ", "original_text": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b0b288b-3e05-4deb-87e9-c98f464b41ab", "node_type": "1", "metadata": {"window": "This\nstrategy significantly alleviates the burden on the alignment between the internal knowledge in latent\ndiffusion model and the external condition information.  In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. ", "original_text": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results. "}, "hash": "1ce99dacc3557c86b333c79f36349c863666ad75277c6314f34b1cb4807af302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fe55192-eed2-41a1-a063-324650fe1ea6", "node_type": "1", "metadata": {"window": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. ", "original_text": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. "}, "hash": "166852e7fff21dc56ae12e4c40d65b3816ab7160f8a71769149107b065930662", "class_name": "RelatedNodeInfo"}}, "hash": "30061c5cf3ae37dc0d4ec292d985944d7a1082869a29e8b4418eaffc5581266d", "text": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. ", "start_char_idx": 20909, "end_char_idx": 21064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fe55192-eed2-41a1-a063-324650fe1ea6": {"__data__": {"id_": "3fe55192-eed2-41a1-a063-324650fe1ea6", "embedding": null, "metadata": {"window": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. ", "original_text": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3307634e-2960-4b3b-927d-cec61ea06947", "node_type": "1", "metadata": {"window": "In practice, directly utilizing ControlNet for\nimage reconstruction leads to severe color shifts as shown in the ablation study (see Section 4.3).\n 3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. ", "original_text": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class. "}, "hash": "30061c5cf3ae37dc0d4ec292d985944d7a1082869a29e8b4418eaffc5581266d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6c36773-b7fa-4d13-9be8-0340134b6286", "node_type": "1", "metadata": {"window": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space. ", "original_text": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. "}, "hash": "92974d8d3285d85a1a4bfc241b5d3d97a903c6da0c4293bd866a3650fd8bcf20", "class_name": "RelatedNodeInfo"}}, "hash": "166852e7fff21dc56ae12e4c40d65b3816ab7160f8a71769149107b065930662", "text": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. ", "start_char_idx": 21064, "end_char_idx": 21168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c36773-b7fa-4d13-9be8-0340134b6286": {"__data__": {"id_": "e6c36773-b7fa-4d13-9be8-0340134b6286", "embedding": null, "metadata": {"window": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space. ", "original_text": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fe55192-eed2-41a1-a063-324650fe1ea6", "node_type": "1", "metadata": {"window": "3.3 Latent Image Guidance for Fidelity-Realness Trade-off\nAlthough the above two-stage approach could already achieve good restoration results, a trade-off\nbetween realness andfidelity is still needed due to various users\u2019 preferences.  Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. ", "original_text": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images. "}, "hash": "166852e7fff21dc56ae12e4c40d65b3816ab7160f8a71769149107b065930662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d03dd2ca-e113-4f6d-9980-7518e474b2a3", "node_type": "1", "metadata": {"window": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. ", "original_text": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. "}, "hash": "aca38b04f4a14eb97784944c08fb56a5651211d0ca7a78d5bf6e6e963e14d5ff", "class_name": "RelatedNodeInfo"}}, "hash": "92974d8d3285d85a1a4bfc241b5d3d97a903c6da0c4293bd866a3650fd8bcf20", "text": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. ", "start_char_idx": 21168, "end_char_idx": 21312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d03dd2ca-e113-4f6d-9980-7518e474b2a3": {"__data__": {"id_": "d03dd2ca-e113-4f6d-9980-7518e474b2a3", "embedding": null, "metadata": {"window": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. ", "original_text": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6c36773-b7fa-4d13-9be8-0340134b6286", "node_type": "1", "metadata": {"window": "Thus, we propose a\ncontrollable module that could guide the denoising process towards the obtained Iregin stage-one,\nthus obtaining an adjustment between realistic and smooth results.  Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space. ", "original_text": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models. "}, "hash": "92974d8d3285d85a1a4bfc241b5d3d97a903c6da0c4293bd866a3650fd8bcf20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "651b2efd-5513-42be-a84d-942792bec463", "node_type": "1", "metadata": {"window": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. ", "original_text": "In this\nwork, the diffusion and denoising processes are based on the latent space. "}, "hash": "8af84a4bc934e7e61b353db1cfc9f90ac078f41fe8f0c076a9538e3de5752973", "class_name": "RelatedNodeInfo"}}, "hash": "aca38b04f4a14eb97784944c08fb56a5651211d0ca7a78d5bf6e6e963e14d5ff", "text": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. ", "start_char_idx": 21312, "end_char_idx": 21435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "651b2efd-5513-42be-a84d-942792bec463": {"__data__": {"id_": "651b2efd-5513-42be-a84d-942792bec463", "embedding": null, "metadata": {"window": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. ", "original_text": "In this\nwork, the diffusion and denoising processes are based on the latent space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d03dd2ca-e113-4f6d-9980-7518e474b2a3", "node_type": "1", "metadata": {"window": "Classifier guidance is proposed\nby Dhariwal and Nichol [ 12] which utilizes a classifier trained on noisy images to guide generation\ntowards target class.  While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. ", "original_text": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt. "}, "hash": "aca38b04f4a14eb97784944c08fb56a5651211d0ca7a78d5bf6e6e963e14d5ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fac5c673-bd74-487b-8a26-e656f0d28350", "node_type": "1", "metadata": {"window": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. ", "original_text": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. "}, "hash": "34dd1291e9ecf7f035dcc558f5dee5b613a84781c1a113fe3b664c5c9c0fad5d", "class_name": "RelatedNodeInfo"}}, "hash": "8af84a4bc934e7e61b353db1cfc9f90ac078f41fe8f0c076a9538e3de5752973", "text": "In this\nwork, the diffusion and denoising processes are based on the latent space. ", "start_char_idx": 21435, "end_char_idx": 21518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fac5c673-bd74-487b-8a26-e656f0d28350": {"__data__": {"id_": "fac5c673-bd74-487b-8a26-e656f0d28350", "embedding": null, "metadata": {"window": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. ", "original_text": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "651b2efd-5513-42be-a84d-942792bec463", "node_type": "1", "metadata": {"window": "While in most cases, the pre-trained models that serve as guidance are usually\ntrained on clean images.  To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. ", "original_text": "In this\nwork, the diffusion and denoising processes are based on the latent space. "}, "hash": "8af84a4bc934e7e61b353db1cfc9f90ac078f41fe8f0c076a9538e3de5752973", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0f9908e-5a15-4bfe-9a0d-13c03d44ad48", "node_type": "1", "metadata": {"window": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. ", "original_text": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. "}, "hash": "088c12e5e7dfdcfcded7b5ab6d504f669547cd683dafe60765593cdddc495fd3", "class_name": "RelatedNodeInfo"}}, "hash": "34dd1291e9ecf7f035dcc558f5dee5b613a84781c1a113fe3b664c5c9c0fad5d", "text": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. ", "start_char_idx": 21518, "end_char_idx": 21628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0f9908e-5a15-4bfe-9a0d-13c03d44ad48": {"__data__": {"id_": "b0f9908e-5a15-4bfe-9a0d-13c03d44ad48", "embedding": null, "metadata": {"window": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. ", "original_text": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fac5c673-bd74-487b-8a26-e656f0d28350", "node_type": "1", "metadata": {"window": "To handle this situation, the work in [ 1;16] turn to guide the intermediate\nvariable \u02dcx0to control the generation process of diffusion models.  Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. ", "original_text": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t. "}, "hash": "34dd1291e9ecf7f035dcc558f5dee5b613a84781c1a113fe3b664c5c9c0fad5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b523d44-2250-4857-a871-2bb78ab13fcb", "node_type": "1", "metadata": {"window": "In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n", "original_text": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. "}, "hash": "d6b90823175c1aac4b40a668d2fdb0e98f01b80bba142388138d1a028b64e686", "class_name": "RelatedNodeInfo"}}, "hash": "088c12e5e7dfdcfcded7b5ab6d504f669547cd683dafe60765593cdddc495fd3", "text": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. ", "start_char_idx": 21628, "end_char_idx": 21840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b523d44-2250-4857-a871-2bb78ab13fcb": {"__data__": {"id_": "6b523d44-2250-4857-a871-2bb78ab13fcb", "embedding": null, "metadata": {"window": "In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n", "original_text": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0f9908e-5a15-4bfe-9a0d-13c03d44ad48", "node_type": "1", "metadata": {"window": "Specifically, in the sampling\nprocess, they estimate a clean image x0from the noisy image xtby estimating the noise in xt.  In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. ", "original_text": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2. "}, "hash": "088c12e5e7dfdcfcded7b5ab6d504f669547cd683dafe60765593cdddc495fd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b56eb80c-01e2-4675-957b-db0cec44214a", "node_type": "1", "metadata": {"window": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. ", "original_text": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. "}, "hash": "ec65db028d7bf11c916ac743093396e067378ce018ac06479f90d1c64ad96296", "class_name": "RelatedNodeInfo"}}, "hash": "d6b90823175c1aac4b40a668d2fdb0e98f01b80bba142388138d1a028b64e686", "text": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. ", "start_char_idx": 21840, "end_char_idx": 22032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b56eb80c-01e2-4675-957b-db0cec44214a": {"__data__": {"id_": "b56eb80c-01e2-4675-957b-db0cec44214a", "embedding": null, "metadata": {"window": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. ", "original_text": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b523d44-2250-4857-a871-2bb78ab13fcb", "node_type": "1", "metadata": {"window": "In this\nwork, the diffusion and denoising processes are based on the latent space.  Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n", "original_text": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent. "}, "hash": "d6b90823175c1aac4b40a668d2fdb0e98f01b80bba142388138d1a028b64e686", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "927e2f63-ced6-434e-bfbc-d696d3bfadef", "node_type": "1", "metadata": {"window": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. ", "original_text": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n"}, "hash": "7ef5997bd62eaffbe0da26381ec28c6d2c0d3b57ecacd1f8c30b30fb956b9251", "class_name": "RelatedNodeInfo"}}, "hash": "ec65db028d7bf11c916ac743093396e067378ce018ac06479f90d1c64ad96296", "text": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. ", "start_char_idx": 22032, "end_char_idx": 22236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "927e2f63-ced6-434e-bfbc-d696d3bfadef": {"__data__": {"id_": "927e2f63-ced6-434e-bfbc-d696d3bfadef", "embedding": null, "metadata": {"window": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. ", "original_text": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b56eb80c-01e2-4675-957b-db0cec44214a", "node_type": "1", "metadata": {"window": "Thus, we aim to obtain a\nclean latent z0by the following equation:\n\u02dcz0=zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t.  (5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. ", "original_text": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result. "}, "hash": "ec65db028d7bf11c916ac743093396e067378ce018ac06479f90d1c64ad96296", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c618e234-385d-46d5-a104-2aeab6ad1e65", "node_type": "1", "metadata": {"window": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. ", "original_text": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. "}, "hash": "c540bb5d60e88c13a636695b173ebf3deda17beb3c4bc0c7344f1568e79e2b28", "class_name": "RelatedNodeInfo"}}, "hash": "7ef5997bd62eaffbe0da26381ec28c6d2c0d3b57ecacd1f8c30b30fb956b9251", "text": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n", "start_char_idx": 22236, "end_char_idx": 22316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c618e234-385d-46d5-a104-2aeab6ad1e65": {"__data__": {"id_": "c618e234-385d-46d5-a104-2aeab6ad1e65", "embedding": null, "metadata": {"window": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. ", "original_text": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "927e2f63-ced6-434e-bfbc-d696d3bfadef", "node_type": "1", "metadata": {"window": "(5)\nThen, a latent-based loss Dlatent is defined as the L2distance between the latent image guidance\nE(Ireg)and the estimated clean latent \u02dcz0:\nDlatent (x, Ireg) =L(\u02dcz0,E(Ireg)) =X\nj1\nCjHjWj||\u02dcz0\u2212 E(Ireg))||2\n2.  (6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. ", "original_text": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n"}, "hash": "7ef5997bd62eaffbe0da26381ec28c6d2c0d3b57ecacd1f8c30b30fb956b9251", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82bb0c82-4d67-495b-89b8-1d57def2b5bc", "node_type": "1", "metadata": {"window": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). ", "original_text": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. "}, "hash": "9574326ebfa839d78a7f0ffaefaca487e560d67fb5133ff01aa77d027ea39493", "class_name": "RelatedNodeInfo"}}, "hash": "c540bb5d60e88c13a636695b173ebf3deda17beb3c4bc0c7344f1568e79e2b28", "text": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. ", "start_char_idx": 22316, "end_char_idx": 22762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82bb0c82-4d67-495b-89b8-1d57def2b5bc": {"__data__": {"id_": "82bb0c82-4d67-495b-89b8-1d57def2b5bc", "embedding": null, "metadata": {"window": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). ", "original_text": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c618e234-385d-46d5-a104-2aeab6ad1e65", "node_type": "1", "metadata": {"window": "(6)\nThe above guidance could iteratively force spatial alignment and color consistency between latent\nfeatures, and guide the generated latent to preserve the content of the reference latent.  Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. ", "original_text": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets. "}, "hash": "c540bb5d60e88c13a636695b173ebf3deda17beb3c4bc0c7344f1568e79e2b28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a1b692b-9874-46d6-938b-65d7b6c83322", "node_type": "1", "metadata": {"window": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n", "original_text": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. "}, "hash": "6f7e17844d7a314d793b14ba49c6ea63debcd9fd7d2fa00e2c2dfce8cd3f6353", "class_name": "RelatedNodeInfo"}}, "hash": "9574326ebfa839d78a7f0ffaefaca487e560d67fb5133ff01aa77d027ea39493", "text": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. ", "start_char_idx": 22762, "end_char_idx": 22839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a1b692b-9874-46d6-938b-65d7b6c83322": {"__data__": {"id_": "3a1b692b-9874-46d6-938b-65d7b6c83322", "embedding": null, "metadata": {"window": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n", "original_text": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82bb0c82-4d67-495b-89b8-1d57def2b5bc", "node_type": "1", "metadata": {"window": "Therefore,\none can control how much information (such as structure, layout and color) is maintained from the\nreference image Ireg, thus achieving a transition from generated output to more smooth result.  The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). ", "original_text": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR. "}, "hash": "9574326ebfa839d78a7f0ffaefaca487e560d67fb5133ff01aa77d027ea39493", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71e4855e-6988-4833-aab9-a789477d5bb3", "node_type": "1", "metadata": {"window": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. ", "original_text": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). "}, "hash": "92b911aa3af28316559863d2a506e070fe957af3ab02c400de3fd636e9e85b48", "class_name": "RelatedNodeInfo"}}, "hash": "6f7e17844d7a314d793b14ba49c6ea63debcd9fd7d2fa00e2c2dfce8cd3f6353", "text": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. ", "start_char_idx": 22839, "end_char_idx": 22903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71e4855e-6988-4833-aab9-a789477d5bb3": {"__data__": {"id_": "71e4855e-6988-4833-aab9-a789477d5bb3", "embedding": null, "metadata": {"window": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. ", "original_text": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a1b692b-9874-46d6-938b-65d7b6c83322", "node_type": "1", "metadata": {"window": "The\nwhole algorithm of our latent image guidance is illustrated in Algorithm 1.\n Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n", "original_text": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512. "}, "hash": "6f7e17844d7a314d793b14ba49c6ea63debcd9fd7d2fa00e2c2dfce8cd3f6353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7345b916-0dc3-4540-868a-17b763463334", "node_type": "1", "metadata": {"window": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n", "original_text": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n"}, "hash": "89a7c5a0293baeea3179ee0dfe1bf8bfec971909b80bc3277179b1d597acb13b", "class_name": "RelatedNodeInfo"}}, "hash": "92b911aa3af28316559863d2a506e070fe957af3ab02c400de3fd636e9e85b48", "text": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). ", "start_char_idx": 22903, "end_char_idx": 23055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7345b916-0dc3-4540-868a-17b763463334": {"__data__": {"id_": "7345b916-0dc3-4540-868a-17b763463334", "embedding": null, "metadata": {"window": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n", "original_text": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71e4855e-6988-4833-aab9-a789477d5bb3", "node_type": "1", "metadata": {"window": "Algorithm 1 Latent-guided diffusion, given a diffusion model \u03f5\u03b8, and the V AE\u2019s encoder Eand\ndecoder D\nInput: Guidance image Ireg, text description c(set to empty), diffusion steps T, gradient scale s\nOutput: Output image D(z0)\nSample zTfromN(0,I)\nfortfrom Tto1do\n\u02dcz0\u2190zt\u221a\u00af\u03b1t\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt, c, t,E(Ireg))\u221a\u00af\u03b1t\nL=L(\u02dcz0,E(Ireg))\nSample zt\u22121byN\u0000\n\u00b5\u03b8(zt)\u2212s\u2207\u02dcz0L, \u03c32\nt\u0001\nend for\nreturn D(z0)\n6\n\n4 Experiments\n4.1 Datasets, Implementation, Metrics\nDatasets.  We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. ", "original_text": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details). "}, "hash": "92b911aa3af28316559863d2a506e070fe957af3ab02c400de3fd636e9e85b48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31e4fa41-4d6d-416a-b1dc-050441f1938a", "node_type": "1", "metadata": {"window": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. ", "original_text": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. "}, "hash": "cb9c6c973311b487f704eb9856fd7dfa9143df2e4dc288059ed932bf17226c20", "class_name": "RelatedNodeInfo"}}, "hash": "89a7c5a0293baeea3179ee0dfe1bf8bfec971909b80bc3277179b1d597acb13b", "text": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n", "start_char_idx": 23055, "end_char_idx": 23138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31e4fa41-4d6d-416a-b1dc-050441f1938a": {"__data__": {"id_": "31e4fa41-4d6d-416a-b1dc-050441f1938a", "embedding": null, "metadata": {"window": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. ", "original_text": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7345b916-0dc3-4540-868a-17b763463334", "node_type": "1", "metadata": {"window": "We train DiffBIR on the ImageNet [ 11] dataset at 512\u00d7512resolution for BIR.  As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n", "original_text": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n"}, "hash": "89a7c5a0293baeea3179ee0dfe1bf8bfec971909b80bc3277179b1d597acb13b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a1019bb-d966-4d86-bc79-9684f1261dbd", "node_type": "1", "metadata": {"window": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n", "original_text": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n"}, "hash": "2816f10f85f2d9c154e20c730231415f8d81347b73d9ce06c4bfdceaeff85454", "class_name": "RelatedNodeInfo"}}, "hash": "cb9c6c973311b487f704eb9856fd7dfa9143df2e4dc288059ed932bf17226c20", "text": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. ", "start_char_idx": 23138, "end_char_idx": 23253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a1019bb-d966-4d86-bc79-9684f1261dbd": {"__data__": {"id_": "9a1019bb-d966-4d86-bc79-9684f1261dbd", "embedding": null, "metadata": {"window": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n", "original_text": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31e4fa41-4d6d-416a-b1dc-050441f1938a", "node_type": "1", "metadata": {"window": "As for\nBFR, we use FFHQ [ 25] dataset and resize it to 512\u00d7512.  To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. ", "original_text": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47. "}, "hash": "cb9c6c973311b487f704eb9856fd7dfa9143df2e4dc288059ed932bf17226c20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34639589-cbbe-44a8-93d1-aee506a563fa", "node_type": "1", "metadata": {"window": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation. ", "original_text": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. "}, "hash": "e74d9f6ce93399ff1c74e7dd8d49f9a41d8c2222166f308a50d30f98686ba5f9", "class_name": "RelatedNodeInfo"}}, "hash": "2816f10f85f2d9c154e20c730231415f8d81347b73d9ce06c4bfdceaeff85454", "text": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n", "start_char_idx": 23253, "end_char_idx": 23433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34639589-cbbe-44a8-93d1-aee506a563fa": {"__data__": {"id_": "34639589-cbbe-44a8-93d1-aee506a563fa", "embedding": null, "metadata": {"window": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation. ", "original_text": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a1019bb-d966-4d86-bc79-9684f1261dbd", "node_type": "1", "metadata": {"window": "To synthesize the LQ images, we utilize\nthe proposed degradation pipeline to process the HQ images during training (please see Appendix\nA for details).  For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n", "original_text": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n"}, "hash": "2816f10f85f2d9c154e20c730231415f8d81347b73d9ce06c4bfdceaeff85454", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5415ceb-c3d5-4806-8a5b-ecedcec97be2", "node_type": "1", "metadata": {"window": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). ", "original_text": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n"}, "hash": "30f2da50d3f8d5ba5f5aa0ee8929113388730e09e598418a056c626ef430ea08", "class_name": "RelatedNodeInfo"}}, "hash": "e74d9f6ce93399ff1c74e7dd8d49f9a41d8c2222166f308a50d30f98686ba5f9", "text": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. ", "start_char_idx": 23433, "end_char_idx": 23603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5415ceb-c3d5-4806-8a5b-ecedcec97be2": {"__data__": {"id_": "e5415ceb-c3d5-4806-8a5b-ecedcec97be2", "embedding": null, "metadata": {"window": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). ", "original_text": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34639589-cbbe-44a8-93d1-aee506a563fa", "node_type": "1", "metadata": {"window": "For BSR, we utilize RealSRSet [ 3] dataset for comparison in a real-world setting.\n For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation. ", "original_text": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68]. "}, "hash": "e74d9f6ce93399ff1c74e7dd8d49f9a41d8c2222166f308a50d30f98686ba5f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "345c5d6f-d714-4ff2-8833-002296696fdb", "node_type": "1", "metadata": {"window": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8. ", "original_text": "Implementation. "}, "hash": "f5b8a72521750760219ab2389c3293d2fc789f1572c8a04d9f67708928f0dae8", "class_name": "RelatedNodeInfo"}}, "hash": "30f2da50d3f8d5ba5f5aa0ee8929113388730e09e598418a056c626ef430ea08", "text": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n", "start_char_idx": 23603, "end_char_idx": 23780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "345c5d6f-d714-4ff2-8833-002296696fdb": {"__data__": {"id_": "345c5d6f-d714-4ff2-8833-002296696fdb", "embedding": null, "metadata": {"window": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8. ", "original_text": "Implementation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5415ceb-c3d5-4806-8a5b-ecedcec97be2", "node_type": "1", "metadata": {"window": "For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet,\ndenoted as Real47.  It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). ", "original_text": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n"}, "hash": "30f2da50d3f8d5ba5f5aa0ee8929113388730e09e598418a056c626ef430ea08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87934093-e51c-42eb-b07e-51cc0e6a372e", "node_type": "1", "metadata": {"window": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations. ", "original_text": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). "}, "hash": "bd7dbf200e8dc33405875006a993c1e27c0a532d0c3899cdaf9b652720441ba2", "class_name": "RelatedNodeInfo"}}, "hash": "f5b8a72521750760219ab2389c3293d2fc789f1572c8a04d9f67708928f0dae8", "text": "Implementation. ", "start_char_idx": 23780, "end_char_idx": 23796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87934093-e51c-42eb-b07e-51cc0e6a372e": {"__data__": {"id_": "87934093-e51c-42eb-b07e-51cc0e6a372e", "embedding": null, "metadata": {"window": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations. ", "original_text": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "345c5d6f-d714-4ff2-8833-002296696fdb", "node_type": "1", "metadata": {"window": "It contains general images of diverse scenes, such as natural outdoor landscapes,\nold photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.\n For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8. ", "original_text": "Implementation. "}, "hash": "f5b8a72521750760219ab2389c3293d2fc789f1572c8a04d9f67708928f0dae8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "563511a7-2bf4-40af-9098-6a3009de89c5", "node_type": "1", "metadata": {"window": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. ", "original_text": "The head number is set to 6 and the window\nsize is set to 8. "}, "hash": "2e9be561dd3c6820ff618744da407dc4ff3d87ebcb3d12ed60bec8e8e8c07964", "class_name": "RelatedNodeInfo"}}, "hash": "bd7dbf200e8dc33405875006a993c1e27c0a532d0c3899cdaf9b652720441ba2", "text": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). ", "start_char_idx": 23796, "end_char_idx": 23925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "563511a7-2bf4-40af-9098-6a3009de89c5": {"__data__": {"id_": "563511a7-2bf4-40af-9098-6a3009de89c5", "embedding": null, "metadata": {"window": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. ", "original_text": "The head number is set to 6 and the window\nsize is set to 8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87934093-e51c-42eb-b07e-51cc0e6a372e", "node_type": "1", "metadata": {"window": "For BFR task, we evaluate our method on a synthetic dataset CelebA-Test [ 39] and three real-world\ndatasets: LFW-Test [ 54], CelebChild-Test [ 54], and WIDER-Test [ 68].  In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations. ", "original_text": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL). "}, "hash": "bd7dbf200e8dc33405875006a993c1e27c0a532d0c3899cdaf9b652720441ba2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0689e590-e22d-4f37-96a5-9a1f2c4f9d20", "node_type": "1", "metadata": {"window": "Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124. ", "original_text": "We train the restoration module with a batch size of 96 for 150k iterations. "}, "hash": "5610cf2e9162ee34f907b2097939c0c19fd67ed9d69d1a8a5b4a371f1d592a30", "class_name": "RelatedNodeInfo"}}, "hash": "2e9be561dd3c6820ff618744da407dc4ff3d87ebcb3d12ed60bec8e8e8c07964", "text": "The head number is set to 6 and the window\nsize is set to 8. ", "start_char_idx": 23925, "end_char_idx": 23986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0689e590-e22d-4f37-96a5-9a1f2c4f9d20": {"__data__": {"id_": "0689e590-e22d-4f37-96a5-9a1f2c4f9d20", "embedding": null, "metadata": {"window": "Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124. ", "original_text": "We train the restoration module with a batch size of 96 for 150k iterations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "563511a7-2bf4-40af-9098-6a3009de89c5", "node_type": "1", "metadata": {"window": "In particular, CelebA-Test\ncontains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under\nthe same degradation range as our training settings.\n Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. ", "original_text": "The head number is set to 6 and the window\nsize is set to 8. "}, "hash": "2e9be561dd3c6820ff618744da407dc4ff3d87ebcb3d12ed60bec8e8e8c07964", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "345ea9a5-3f79-4e90-a55e-12994dde510e", "node_type": "1", "metadata": {"window": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. ", "original_text": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. "}, "hash": "784acd299e409dcc41623ae6fa47dd3f71e6d4f90811b99fdc439efbe9e91899", "class_name": "RelatedNodeInfo"}}, "hash": "5610cf2e9162ee34f907b2097939c0c19fd67ed9d69d1a8a5b4a371f1d592a30", "text": "We train the restoration module with a batch size of 96 for 150k iterations. ", "start_char_idx": 23986, "end_char_idx": 24063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "345ea9a5-3f79-4e90-a55e-12994dde510e": {"__data__": {"id_": "345ea9a5-3f79-4e90-a55e-12994dde510e", "embedding": null, "metadata": {"window": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. ", "original_text": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0689e590-e22d-4f37-96a5-9a1f2c4f9d20", "node_type": "1", "metadata": {"window": "Implementation.  The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124. ", "original_text": "We train the restoration module with a batch size of 96 for 150k iterations. "}, "hash": "5610cf2e9162ee34f907b2097939c0c19fd67ed9d69d1a8a5b4a371f1d592a30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1d86b3a-cc1c-4032-b806-174802bfc5c2", "node_type": "1", "metadata": {"window": "The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. ", "original_text": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124. "}, "hash": "f361896099ed9529a86fb873a1bcf7f074d032ba8be148e896838f0aa63ace42", "class_name": "RelatedNodeInfo"}}, "hash": "784acd299e409dcc41623ae6fa47dd3f71e6d4f90811b99fdc439efbe9e91899", "text": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. ", "start_char_idx": 24063, "end_char_idx": 24203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1d86b3a-cc1c-4032-b806-174802bfc5c2": {"__data__": {"id_": "b1d86b3a-cc1c-4032-b806-174802bfc5c2", "embedding": null, "metadata": {"window": "The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. ", "original_text": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "345ea9a5-3f79-4e90-a55e-12994dde510e", "node_type": "1", "metadata": {"window": "The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and\neach RSTB contains 6 Swin Transformer Layers (STL).  The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. ", "original_text": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192. "}, "hash": "784acd299e409dcc41623ae6fa47dd3f71e6d4f90811b99fdc439efbe9e91899", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da878723-76b7-4dd1-a7ba-960fac34eb88", "node_type": "1", "metadata": {"window": "We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. ", "original_text": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. "}, "hash": "2af9804e27117fa84b22df8c15afcb46e5922f46b459fcf60f7ff27744465c2f", "class_name": "RelatedNodeInfo"}}, "hash": "f361896099ed9529a86fb873a1bcf7f074d032ba8be148e896838f0aa63ace42", "text": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124. ", "start_char_idx": 24203, "end_char_idx": 24266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da878723-76b7-4dd1-a7ba-960fac34eb88": {"__data__": {"id_": "da878723-76b7-4dd1-a7ba-960fac34eb88", "embedding": null, "metadata": {"window": "We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. ", "original_text": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1d86b3a-cc1c-4032-b806-174802bfc5c2", "node_type": "1", "metadata": {"window": "The head number is set to 6 and the window\nsize is set to 8.  We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. ", "original_text": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124. "}, "hash": "f361896099ed9529a86fb873a1bcf7f074d032ba8be148e896838f0aa63ace42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d52bfb7-ed49-460f-856f-2aa1480dbf7f", "node_type": "1", "metadata": {"window": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n", "original_text": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. "}, "hash": "1df1797fb9dd4ccc4d47a3d2a2d1e6d2221b758286918a1e0f5d71d4fc124e07", "class_name": "RelatedNodeInfo"}}, "hash": "2af9804e27117fa84b22df8c15afcb46e5922f46b459fcf60f7ff27744465c2f", "text": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. ", "start_char_idx": 24266, "end_char_idx": 24346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d52bfb7-ed49-460f-856f-2aa1480dbf7f": {"__data__": {"id_": "4d52bfb7-ed49-460f-856f-2aa1480dbf7f", "embedding": null, "metadata": {"window": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n", "original_text": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da878723-76b7-4dd1-a7ba-960fac34eb88", "node_type": "1", "metadata": {"window": "We train the restoration module with a batch size of 96 for 150k iterations.  We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. ", "original_text": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs. "}, "hash": "2af9804e27117fa84b22df8c15afcb46e5922f46b459fcf60f7ff27744465c2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95852e4c-a347-4e78-82a1-ccd6af86e20e", "node_type": "1", "metadata": {"window": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics . ", "original_text": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. "}, "hash": "b7e96b4dc2330b0b810a316d87e69108c27a3d92d632f5b922e9951b8a066406", "class_name": "RelatedNodeInfo"}}, "hash": "1df1797fb9dd4ccc4d47a3d2a2d1e6d2221b758286918a1e0f5d71d4fc124e07", "text": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. ", "start_char_idx": 24346, "end_char_idx": 24416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95852e4c-a347-4e78-82a1-ccd6af86e20e": {"__data__": {"id_": "95852e4c-a347-4e78-82a1-ccd6af86e20e", "embedding": null, "metadata": {"window": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics . ", "original_text": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d52bfb7-ed49-460f-856f-2aa1480dbf7f", "node_type": "1", "metadata": {"window": "We utilize\nStable Diffusion 2.1-base3as the generative prior, and finetune the diffusion model for 25k iterations\nwith a batch size of 192.  We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n", "original_text": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps. "}, "hash": "1df1797fb9dd4ccc4d47a3d2a2d1e6d2221b758286918a1e0f5d71d4fc124e07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60695295-822d-4ba0-ba1e-6b7cb7a6f1f0", "node_type": "1", "metadata": {"window": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. ", "original_text": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n"}, "hash": "43341ceccb3d09c44f7031472fbf32474d1a175001bd919862c87f70c0b309f2", "class_name": "RelatedNodeInfo"}}, "hash": "b7e96b4dc2330b0b810a316d87e69108c27a3d92d632f5b922e9951b8a066406", "text": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. ", "start_char_idx": 24416, "end_char_idx": 24495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60695295-822d-4ba0-ba1e-6b7cb7a6f1f0": {"__data__": {"id_": "60695295-822d-4ba0-ba1e-6b7cb7a6f1f0", "embedding": null, "metadata": {"window": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. ", "original_text": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95852e4c-a347-4e78-82a1-ccd6af86e20e", "node_type": "1", "metadata": {"window": "We use Adam [ 28] optimizer and set the learning rate to 10\u22124.  The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics . ", "original_text": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512. "}, "hash": "b7e96b4dc2330b0b810a316d87e69108c27a3d92d632f5b922e9951b8a066406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "847dda23-caf9-49ad-b1ad-116461e72413", "node_type": "1", "metadata": {"window": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. ", "original_text": "Metrics . "}, "hash": "b9abcee9d9d17ed134bd8f3c4c71dec935f1e6e8ef98925a59b2e52b8efbbda7", "class_name": "RelatedNodeInfo"}}, "hash": "43341ceccb3d09c44f7031472fbf32474d1a175001bd919862c87f70c0b309f2", "text": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n", "start_char_idx": 24495, "end_char_idx": 24610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "847dda23-caf9-49ad-b1ad-116461e72413": {"__data__": {"id_": "847dda23-caf9-49ad-b1ad-116461e72413", "embedding": null, "metadata": {"window": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. ", "original_text": "Metrics . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60695295-822d-4ba0-ba1e-6b7cb7a6f1f0", "node_type": "1", "metadata": {"window": "The training\nprocess is conducted on 512\u00d7512resolution with 8 NVIDIA A100 GPUs.  For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. ", "original_text": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n"}, "hash": "43341ceccb3d09c44f7031472fbf32474d1a175001bd919862c87f70c0b309f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0da254b-15e7-4689-9f45-9ae50beb6c19", "node_type": "1", "metadata": {"window": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. ", "original_text": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. "}, "hash": "3cb3acf81c5523970c4b813ccb89f4b21ce01511cc6b8067c3c754477af430cd", "class_name": "RelatedNodeInfo"}}, "hash": "b9abcee9d9d17ed134bd8f3c4c71dec935f1e6e8ef98925a59b2e52b8efbbda7", "text": "Metrics . ", "start_char_idx": 24610, "end_char_idx": 24620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0da254b-15e7-4689-9f45-9ae50beb6c19": {"__data__": {"id_": "d0da254b-15e7-4689-9f45-9ae50beb6c19", "embedding": null, "metadata": {"window": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. ", "original_text": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "847dda23-caf9-49ad-b1ad-116461e72413", "node_type": "1", "metadata": {"window": "For inference, we adopt\nspaced DDPM sampling [ 43] with 50 timesteps.  Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. ", "original_text": "Metrics . "}, "hash": "b9abcee9d9d17ed134bd8f3c4c71dec935f1e6e8ef98925a59b2e52b8efbbda7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fee909e6-6eb8-4e74-a8f0-2159f2afca77", "node_type": "1", "metadata": {"window": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n", "original_text": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. "}, "hash": "bbb822fe285205b5987db021a9a5509cd2563f091f918f356c675ecd1fdbf068", "class_name": "RelatedNodeInfo"}}, "hash": "3cb3acf81c5523970c4b813ccb89f4b21ce01511cc6b8067c3c754477af430cd", "text": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. ", "start_char_idx": 24620, "end_char_idx": 24727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fee909e6-6eb8-4e74-a8f0-2159f2afca77": {"__data__": {"id_": "fee909e6-6eb8-4e74-a8f0-2159f2afca77", "embedding": null, "metadata": {"window": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n", "original_text": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0da254b-15e7-4689-9f45-9ae50beb6c19", "node_type": "1", "metadata": {"window": "Our DiffBIR is able to handle images with arbitrary\nsizes larger than 512\u00d7512.  For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. ", "original_text": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67]. "}, "hash": "3cb3acf81c5523970c4b813ccb89f4b21ce01511cc6b8067c3c754477af430cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee83116d-3f44-48d6-8687-6638fdeec590", "node_type": "1", "metadata": {"window": "Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. ", "original_text": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. "}, "hash": "3e0dc416cd6fdc4081b8cfc772c0e3afd01230d95e48511cd8fc812c35be82a1", "class_name": "RelatedNodeInfo"}}, "hash": "bbb822fe285205b5987db021a9a5509cd2563f091f918f356c675ecd1fdbf068", "text": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. ", "start_char_idx": 24727, "end_char_idx": 24872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee83116d-3f44-48d6-8687-6638fdeec590": {"__data__": {"id_": "ee83116d-3f44-48d6-8687-6638fdeec590", "embedding": null, "metadata": {"window": "Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. ", "original_text": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fee909e6-6eb8-4e74-a8f0-2159f2afca77", "node_type": "1", "metadata": {"window": "For images with sides <512, we first upsample them with the short side\nenlarged to 512, and then resize them back.\n Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n", "original_text": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE. "}, "hash": "bbb822fe285205b5987db021a9a5509cd2563f091f918f356c675ecd1fdbf068", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1eb57a0-43f8-4ab6-8c84-974e7d9f214b", "node_type": "1", "metadata": {"window": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. ", "original_text": "We also\ndeploy a user study for a more thorough comparison.\n"}, "hash": "e313fd657a477ac8e81018f0faa583cdd93fba07fa0848fb09639fdf11c98c19", "class_name": "RelatedNodeInfo"}}, "hash": "3e0dc416cd6fdc4081b8cfc772c0e3afd01230d95e48511cd8fc812c35be82a1", "text": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. ", "start_char_idx": 24872, "end_char_idx": 24988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1eb57a0-43f8-4ab6-8c84-974e7d9f214b": {"__data__": {"id_": "c1eb57a0-43f8-4ab6-8c84-974e7d9f214b", "embedding": null, "metadata": {"window": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. ", "original_text": "We also\ndeploy a user study for a more thorough comparison.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee83116d-3f44-48d6-8687-6638fdeec590", "node_type": "1", "metadata": {"window": "Metrics .  Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. ", "original_text": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20]. "}, "hash": "3e0dc416cd6fdc4081b8cfc772c0e3afd01230d95e48511cd8fc812c35be82a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69b8b300-58d7-45e2-8f75-4414861b9962", "node_type": "1", "metadata": {"window": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n", "original_text": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. "}, "hash": "4993ceff765b15e2b3276c62268a062eef293bb7160d220897324238dfa07380", "class_name": "RelatedNodeInfo"}}, "hash": "e313fd657a477ac8e81018f0faa583cdd93fba07fa0848fb09639fdf11c98c19", "text": "We also\ndeploy a user study for a more thorough comparison.\n", "start_char_idx": 24988, "end_char_idx": 25048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69b8b300-58d7-45e2-8f75-4414861b9962": {"__data__": {"id_": "69b8b300-58d7-45e2-8f75-4414861b9962", "embedding": null, "metadata": {"window": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n", "original_text": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1eb57a0-43f8-4ab6-8c84-974e7d9f214b", "node_type": "1", "metadata": {"window": "Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM,\nand LPIPS [ 67].  To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. ", "original_text": "We also\ndeploy a user study for a more thorough comparison.\n"}, "hash": "e313fd657a477ac8e81018f0faa583cdd93fba07fa0848fb09639fdf11c98c19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94e739f6-b404-442b-96b8-cdfa3a38bfca", "node_type": "1", "metadata": {"window": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset. ", "original_text": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. "}, "hash": "793657d95f87e49940ff3ef5d167e1f821fe8933e105dcb81e109ce76085f594", "class_name": "RelatedNodeInfo"}}, "hash": "4993ceff765b15e2b3276c62268a062eef293bb7160d220897324238dfa07380", "text": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. ", "start_char_idx": 25048, "end_char_idx": 25230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94e739f6-b404-442b-96b8-cdfa3a38bfca": {"__data__": {"id_": "94e739f6-b404-442b-96b8-cdfa3a38bfca", "embedding": null, "metadata": {"window": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset. ", "original_text": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69b8b300-58d7-45e2-8f75-4414861b9962", "node_type": "1", "metadata": {"window": "To better evaluate the realness for BIR task, we also include several no-reference\nimage quality assessment (IQA) metrics: MANIQA4[60] and NIQE.  For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n", "original_text": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6]. "}, "hash": "4993ceff765b15e2b3276c62268a062eef293bb7160d220897324238dfa07380", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e776fe8-5ab0-4ffa-abdf-c4c8001c8174", "node_type": "1", "metadata": {"window": "We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1. ", "original_text": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n"}, "hash": "007c0b9ca25e7aa59a5aab338735d99b0a16b38c978bd1a6bb3dcd73ad145fac", "class_name": "RelatedNodeInfo"}}, "hash": "793657d95f87e49940ff3ef5d167e1f821fe8933e105dcb81e109ce76085f594", "text": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. ", "start_char_idx": 25230, "end_char_idx": 25317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e776fe8-5ab0-4ffa-abdf-c4c8001c8174": {"__data__": {"id_": "0e776fe8-5ab0-4ffa-abdf-c4c8001c8174", "embedding": null, "metadata": {"window": "We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1. ", "original_text": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94e739f6-b404-442b-96b8-cdfa3a38bfca", "node_type": "1", "metadata": {"window": "For BFR, we evaluate the\nidentity preservation - IDS [ 68], and employ the widely used perceptual metric FID [ 20].  We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset. ", "original_text": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5. "}, "hash": "793657d95f87e49940ff3ef5d167e1f821fe8933e105dcb81e109ce76085f594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8551b4d-1563-4d08-8147-89bd31457d05", "node_type": "1", "metadata": {"window": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. ", "original_text": "BSR on real-world dataset. "}, "hash": "e54cf2394785b48f0595682802e2f1fd31e4817ff89f4aa8469cc43ac7e181f2", "class_name": "RelatedNodeInfo"}}, "hash": "007c0b9ca25e7aa59a5aab338735d99b0a16b38c978bd1a6bb3dcd73ad145fac", "text": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n", "start_char_idx": 25317, "end_char_idx": 25497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8551b4d-1563-4d08-8147-89bd31457d05": {"__data__": {"id_": "f8551b4d-1563-4d08-8147-89bd31457d05", "embedding": null, "metadata": {"window": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. ", "original_text": "BSR on real-world dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e776fe8-5ab0-4ffa-abdf-c4c8001c8174", "node_type": "1", "metadata": {"window": "We also\ndeploy a user study for a more thorough comparison.\n 4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1. ", "original_text": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n"}, "hash": "007c0b9ca25e7aa59a5aab338735d99b0a16b38c978bd1a6bb3dcd73ad145fac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa871314-45b4-471d-ba69-713bcec0b24b", "node_type": "1", "metadata": {"window": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. ", "original_text": "We provide the quantitative comparison on real-world datasets in Table\n1. "}, "hash": "fcefe71b0e61d63b55a3eec2e1d93d2e42f4dd73122cb94675c472f765937faa", "class_name": "RelatedNodeInfo"}}, "hash": "e54cf2394785b48f0595682802e2f1fd31e4817ff89f4aa8469cc43ac7e181f2", "text": "BSR on real-world dataset. ", "start_char_idx": 25497, "end_char_idx": 25524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa871314-45b4-471d-ba69-713bcec0b24b": {"__data__": {"id_": "fa871314-45b4-471d-ba69-713bcec0b24b", "embedding": null, "metadata": {"window": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. ", "original_text": "We provide the quantitative comparison on real-world datasets in Table\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8551b4d-1563-4d08-8147-89bd31457d05", "node_type": "1", "metadata": {"window": "4.2 Comparisons with State-of-the-Art Methods\nFor BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ [ 55],\nBSRGAN [ 64], SwinIR-GAN [ 36], and FeMaSR [ 6].  The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. ", "original_text": "BSR on real-world dataset. "}, "hash": "e54cf2394785b48f0595682802e2f1fd31e4817ff89f4aa8469cc43ac7e181f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e1ad868-4215-40b4-b059-cc80284ee86f", "node_type": "1", "metadata": {"window": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3. ", "original_text": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. "}, "hash": "7e42dd3e7577e38285ee76bb403ecf45c99d2c0d1709de9f782f60ced08c6feb", "class_name": "RelatedNodeInfo"}}, "hash": "fcefe71b0e61d63b55a3eec2e1d93d2e42f4dd73122cb94675c472f765937faa", "text": "We provide the quantitative comparison on real-world datasets in Table\n1. ", "start_char_idx": 25524, "end_char_idx": 25598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e1ad868-4215-40b4-b059-cc80284ee86f": {"__data__": {"id_": "3e1ad868-4215-40b4-b059-cc80284ee86f", "embedding": null, "metadata": {"window": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3. ", "original_text": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa871314-45b4-471d-ba69-713bcec0b24b", "node_type": "1", "metadata": {"window": "The recent state-of-the-art ZIR methods\n(DDNM [ 57] and GDP [ 16]) are also included5.  Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. ", "original_text": "We provide the quantitative comparison on real-world datasets in Table\n1. "}, "hash": "fcefe71b0e61d63b55a3eec2e1d93d2e42f4dd73122cb94675c472f765937faa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2c50208-89c6-4dea-bd2e-df578c65a5c0", "node_type": "1", "metadata": {"window": "BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. ", "original_text": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. "}, "hash": "2b026103b77ee0216eddbce6fb0de8e00aaad6371dcfdb248423012aa399bfaa", "class_name": "RelatedNodeInfo"}}, "hash": "7e42dd3e7577e38285ee76bb403ecf45c99d2c0d1709de9f782f60ced08c6feb", "text": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. ", "start_char_idx": 25598, "end_char_idx": 25730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2c50208-89c6-4dea-bd2e-df578c65a5c0": {"__data__": {"id_": "c2c50208-89c6-4dea-bd2e-df578c65a5c0", "embedding": null, "metadata": {"window": "BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. ", "original_text": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e1ad868-4215-40b4-b059-cc80284ee86f", "node_type": "1", "metadata": {"window": "Regarding BFR task, we compare with the most\nrecent state-of-the-art methods: DMDNet [ 35], GFP-GAN [ 54], GPEN [ 61], GCFSR [ 19], VQFR\n[18], CodeFormer [68], RestoreFormer [59].\n BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3. ", "original_text": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47. "}, "hash": "7e42dd3e7577e38285ee76bb403ecf45c99d2c0d1709de9f782f60ced08c6feb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0978c9ea-ffe5-49a1-ba91-57fe0489d955", "node_type": "1", "metadata": {"window": "We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. ", "original_text": "The visual comparison results are presented in Figure 3. "}, "hash": "10e0b3b80197c7709861924d7e5eb222e65ac743b607f14f02695908394fa07f", "class_name": "RelatedNodeInfo"}}, "hash": "2b026103b77ee0216eddbce6fb0de8e00aaad6371dcfdb248423012aa399bfaa", "text": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. ", "start_char_idx": 25730, "end_char_idx": 25820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0978c9ea-ffe5-49a1-ba91-57fe0489d955": {"__data__": {"id_": "0978c9ea-ffe5-49a1-ba91-57fe0489d955", "embedding": null, "metadata": {"window": "We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. ", "original_text": "The visual comparison results are presented in Figure 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2c50208-89c6-4dea-bd2e-df578c65a5c0", "node_type": "1", "metadata": {"window": "BSR on real-world dataset.  We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. ", "original_text": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets. "}, "hash": "2b026103b77ee0216eddbce6fb0de8e00aaad6371dcfdb248423012aa399bfaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf1be144-1eb0-4032-8f76-aec2553c4733", "node_type": "1", "metadata": {"window": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n", "original_text": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. "}, "hash": "0425730911b104d883905f3eefc22a97aeba1938fc1a2a87925e232b23911b5d", "class_name": "RelatedNodeInfo"}}, "hash": "10e0b3b80197c7709861924d7e5eb222e65ac743b607f14f02695908394fa07f", "text": "The visual comparison results are presented in Figure 3. ", "start_char_idx": 25820, "end_char_idx": 25877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf1be144-1eb0-4032-8f76-aec2553c4733": {"__data__": {"id_": "cf1be144-1eb0-4032-8f76-aec2553c4733", "embedding": null, "metadata": {"window": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n", "original_text": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0978c9ea-ffe5-49a1-ba91-57fe0489d955", "node_type": "1", "metadata": {"window": "We provide the quantitative comparison on real-world datasets in Table\n1.  It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. ", "original_text": "The visual comparison results are presented in Figure 3. "}, "hash": "10e0b3b80197c7709861924d7e5eb222e65ac743b607f14f02695908394fa07f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64a0fbb3-f32b-4316-96c2-9a17a9f13a06", "node_type": "1", "metadata": {"window": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. ", "original_text": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. "}, "hash": "c996ddb8df531fb94fd104a78612d80c8169760e2e96f8eb98f1bc3e66018b14", "class_name": "RelatedNodeInfo"}}, "hash": "0425730911b104d883905f3eefc22a97aeba1938fc1a2a87925e232b23911b5d", "text": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. ", "start_char_idx": 25877, "end_char_idx": 26034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64a0fbb3-f32b-4316-96c2-9a17a9f13a06": {"__data__": {"id_": "64a0fbb3-f32b-4316-96c2-9a17a9f13a06", "embedding": null, "metadata": {"window": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. ", "original_text": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf1be144-1eb0-4032-8f76-aec2553c4733", "node_type": "1", "metadata": {"window": "It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used\nRealSRSet [ 24] and our collected Real47.  While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n", "original_text": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output. "}, "hash": "0425730911b104d883905f3eefc22a97aeba1938fc1a2a87925e232b23911b5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce38a0d4-3458-4ead-8f4a-dbebc017863a", "node_type": "1", "metadata": {"window": "The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance. ", "original_text": "More\nvisualization results can be found in Figure 11 and Figure 12.\n"}, "hash": "0d168d6fc53d7820f9fc7c5f4bfdf6db162e25304b59d47f3a689e086f82f325", "class_name": "RelatedNodeInfo"}}, "hash": "c996ddb8df531fb94fd104a78612d80c8169760e2e96f8eb98f1bc3e66018b14", "text": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. ", "start_char_idx": 26034, "end_char_idx": 26180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce38a0d4-3458-4ead-8f4a-dbebc017863a": {"__data__": {"id_": "ce38a0d4-3458-4ead-8f4a-dbebc017863a", "embedding": null, "metadata": {"window": "The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance. ", "original_text": "More\nvisualization results can be found in Figure 11 and Figure 12.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64a0fbb3-f32b-4316-96c2-9a17a9f13a06", "node_type": "1", "metadata": {"window": "While BSRGAN and Real-ESRGAN+ could achieve top-3\nresults in MANIQA on both two datasets.  The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. ", "original_text": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results. "}, "hash": "c996ddb8df531fb94fd104a78612d80c8169760e2e96f8eb98f1bc3e66018b14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4e38081-d462-4a9d-9019-83a009d29477", "node_type": "1", "metadata": {"window": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n", "original_text": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. "}, "hash": "65a1e87c48d529d070a8fe3728a251895d310f70bcb48198297064a8d446edb0", "class_name": "RelatedNodeInfo"}}, "hash": "0d168d6fc53d7820f9fc7c5f4bfdf6db162e25304b59d47f3a689e086f82f325", "text": "More\nvisualization results can be found in Figure 11 and Figure 12.\n", "start_char_idx": 26180, "end_char_idx": 26248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4e38081-d462-4a9d-9019-83a009d29477": {"__data__": {"id_": "e4e38081-d462-4a9d-9019-83a009d29477", "embedding": null, "metadata": {"window": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n", "original_text": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce38a0d4-3458-4ead-8f4a-dbebc017863a", "node_type": "1", "metadata": {"window": "The visual comparison results are presented in Figure 3.  It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance. ", "original_text": "More\nvisualization results can be found in Figure 11 and Figure 12.\n"}, "hash": "0d168d6fc53d7820f9fc7c5f4bfdf6db162e25304b59d47f3a689e086f82f325", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60f8db2b-dc9a-4ea8-9f41-143b7c027685", "node_type": "1", "metadata": {"window": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n", "original_text": "Red and blue indicate the best and second best performance. "}, "hash": "1c63f78db8f316b952e45d29c7b352b1447e6975b84976e9913d921165e4d46e", "class_name": "RelatedNodeInfo"}}, "hash": "65a1e87c48d529d070a8fe3728a251895d310f70bcb48198297064a8d446edb0", "text": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. ", "start_char_idx": 26248, "end_char_idx": 26360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60f8db2b-dc9a-4ea8-9f41-143b7c027685": {"__data__": {"id_": "60f8db2b-dc9a-4ea8-9f41-143b7c027685", "embedding": null, "metadata": {"window": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n", "original_text": "Red and blue indicate the best and second best performance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4e38081-d462-4a9d-9019-83a009d29477", "node_type": "1", "metadata": {"window": "It\ncan be seen that DiffBIR is able to restore text information more naturally, while other methods tend\nto distort the characters or produce blurry output.  On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n", "original_text": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale. "}, "hash": "65a1e87c48d529d070a8fe3728a251895d310f70bcb48198297064a8d446edb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6c6399d-9912-4417-9a88-61005b0fef04", "node_type": "1", "metadata": {"window": "More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n", "original_text": "The top 3 results are marked as gray .\n"}, "hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "class_name": "RelatedNodeInfo"}}, "hash": "1c63f78db8f316b952e45d29c7b352b1447e6975b84976e9913d921165e4d46e", "text": "Red and blue indicate the best and second best performance. ", "start_char_idx": 26360, "end_char_idx": 26420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6c6399d-9912-4417-9a88-61005b0fef04": {"__data__": {"id_": "a6c6399d-9912-4417-9a88-61005b0fef04", "embedding": null, "metadata": {"window": "More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n", "original_text": "The top 3 results are marked as gray .\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60f8db2b-dc9a-4ea8-9f41-143b7c027685", "node_type": "1", "metadata": {"window": "On the other hand, our DiffBIR could also generate\nrealistic texture details for natural images, where other methods produce over-smooth results.  More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n", "original_text": "Red and blue indicate the best and second best performance. "}, "hash": "1c63f78db8f316b952e45d29c7b352b1447e6975b84976e9913d921165e4d46e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f5f4da7-7a40-41d6-931e-9ad2ef9062cc", "node_type": "1", "metadata": {"window": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4.", "original_text": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n"}, "hash": "37ce2e970133ed562edf72699b1f91aeca7b399ef5f4353bb9358071dea04c93", "class_name": "RelatedNodeInfo"}}, "hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "text": "The top 3 results are marked as gray .\n", "start_char_idx": 26420, "end_char_idx": 26459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f5f4da7-7a40-41d6-931e-9ad2ef9062cc": {"__data__": {"id_": "8f5f4da7-7a40-41d6-931e-9ad2ef9062cc", "embedding": null, "metadata": {"window": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4.", "original_text": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6c6399d-9912-4417-9a88-61005b0fef04", "node_type": "1", "metadata": {"window": "More\nvisualization results can be found in Figure 11 and Figure 12.\n Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n", "original_text": "The top 3 results are marked as gray .\n"}, "hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d50cd03e-a76a-4e3e-a98c-0b1f399ae8f5", "node_type": "1", "metadata": {"window": "Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. ", "original_text": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n"}, "hash": "483dc75e29288851fce34020880f940b2b39e6ee21d1e740712114607feb6ebb", "class_name": "RelatedNodeInfo"}}, "hash": "37ce2e970133ed562edf72699b1f91aeca7b399ef5f4353bb9358071dea04c93", "text": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n", "start_char_idx": 26459, "end_char_idx": 27012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d50cd03e-a76a-4e3e-a98c-0b1f399ae8f5": {"__data__": {"id_": "d50cd03e-a76a-4e3e-a98c-0b1f399ae8f5", "embedding": null, "metadata": {"window": "Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. ", "original_text": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f5f4da7-7a40-41d6-931e-9ad2ef9062cc", "node_type": "1", "metadata": {"window": "Table 1: Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a 4\u00d7upsampling\nscale.  Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4.", "original_text": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n"}, "hash": "37ce2e970133ed562edf72699b1f91aeca7b399ef5f4353bb9358071dea04c93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "109b8a70-88f3-49bc-a908-26e6ad69c290", "node_type": "1", "metadata": {"window": "The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. ", "original_text": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4."}, "hash": "d67d2cce53d53d9c74463cf9dc7552f88b9be190385ee2dee039697274761bc0", "class_name": "RelatedNodeInfo"}}, "hash": "483dc75e29288851fce34020880f940b2b39e6ee21d1e740712114607feb6ebb", "text": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n", "start_char_idx": 27012, "end_char_idx": 27112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "109b8a70-88f3-49bc-a908-26e6ad69c290": {"__data__": {"id_": "109b8a70-88f3-49bc-a908-26e6ad69c290", "embedding": null, "metadata": {"window": "The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. ", "original_text": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d50cd03e-a76a-4e3e-a98c-0b1f399ae8f5", "node_type": "1", "metadata": {"window": "Red and blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. ", "original_text": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n"}, "hash": "483dc75e29288851fce34020880f940b2b39e6ee21d1e740712114607feb6ebb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf9dee5c-8e10-4142-b1e6-3f8509f036dc", "node_type": "1", "metadata": {"window": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. ", "original_text": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. "}, "hash": "2163e2a2175aaa50d5a01cf3390e3d4c7440a3f95c8ada95363a5dbed098d501", "class_name": "RelatedNodeInfo"}}, "hash": "d67d2cce53d53d9c74463cf9dc7552f88b9be190385ee2dee039697274761bc0", "text": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4.", "start_char_idx": 27112, "end_char_idx": 27200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf9dee5c-8e10-4142-b1e6-3f8509f036dc": {"__data__": {"id_": "cf9dee5c-8e10-4142-b1e6-3f8509f036dc", "embedding": null, "metadata": {"window": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. ", "original_text": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "109b8a70-88f3-49bc-a908-26e6ad69c290", "node_type": "1", "metadata": {"window": "The top 3 results are marked as gray .\n Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. ", "original_text": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4."}, "hash": "d67d2cce53d53d9c74463cf9dc7552f88b9be190385ee2dee039697274761bc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "611eb5a8-c788-4d0d-9560-e0886aa41273", "node_type": "1", "metadata": {"window": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score. ", "original_text": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. "}, "hash": "af13ad44f77bfacba3372d2d2c0993b55e85039f460b3b461bfe5ad291ad3d16", "class_name": "RelatedNodeInfo"}}, "hash": "2163e2a2175aaa50d5a01cf3390e3d4c7440a3f95c8ada95363a5dbed098d501", "text": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. ", "start_char_idx": 27200, "end_char_idx": 27566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "611eb5a8-c788-4d0d-9560-e0886aa41273": {"__data__": {"id_": "611eb5a8-c788-4d0d-9560-e0886aa41273", "embedding": null, "metadata": {"window": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score. ", "original_text": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf9dee5c-8e10-4142-b1e6-3f8509f036dc", "node_type": "1", "metadata": {"window": "Dataset Metric DDNM [57] GDP [16] Real-ESRGAN+[55] BSRGAN [64] SwinIR-GAN [36] FeMaSR [6] DiffBIR(Ours)\nMANIQA \u21910.4535 0.4581 0.5376 0.5640 0.5295 0.5247 0.5906RealSRSetNIQE \u2193 6.8415 5.0626 5.7401 5.6074 5.6093 5.2353 6.0738\nMANIQA \u21910.4813 0.5237 0.5900 0.5889 0.5721 0.5718 0.6293Real47NIQE \u2193 6.4768 3.9866 3.9103 4.0338 3.9910 4.1731 3.9240\n3https://github.com/Stability-AI/stablediffusion\n4MANIQA ( https://github.com/IIGROUP/MANIQA ) won first place in the NTIRE2022 Perceptual Im-\nage Quality Assessment Challenge Track 2 No-Reference competition.\n 5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. ", "original_text": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset. "}, "hash": "2163e2a2175aaa50d5a01cf3390e3d4c7440a3f95c8ada95363a5dbed098d501", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afabe86c-a91b-42e2-837a-055c1fe4f43a", "node_type": "1", "metadata": {"window": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion. ", "original_text": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. "}, "hash": "959c96a1ae81ca0eb01884e22f5ed2c282a54d07503860446ea1b1e45851d270", "class_name": "RelatedNodeInfo"}}, "hash": "af13ad44f77bfacba3372d2d2c0993b55e85039f460b3b461bfe5ad291ad3d16", "text": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. ", "start_char_idx": 27566, "end_char_idx": 27637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afabe86c-a91b-42e2-837a-055c1fe4f43a": {"__data__": {"id_": "afabe86c-a91b-42e2-837a-055c1fe4f43a", "embedding": null, "metadata": {"window": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion. ", "original_text": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "611eb5a8-c788-4d0d-9560-e0886aa41273", "node_type": "1", "metadata": {"window": "5DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.\n 7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score. ", "original_text": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+. "}, "hash": "af13ad44f77bfacba3372d2d2c0993b55e85039f460b3b461bfe5ad291ad3d16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "264a7da6-5672-46cc-a9d9-2883e6cbab59", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4. ", "original_text": "To be more exact, better result\nobtains higher score. "}, "hash": "8ccd244e9646d60a223fa3891d58bb1a4d5861df17a124af15dde6ea9d952de5", "class_name": "RelatedNodeInfo"}}, "hash": "959c96a1ae81ca0eb01884e22f5ed2c282a54d07503860446ea1b1e45851d270", "text": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. ", "start_char_idx": 27637, "end_char_idx": 27775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "264a7da6-5672-46cc-a9d9-2883e6cbab59": {"__data__": {"id_": "264a7da6-5672-46cc-a9d9-2883e6cbab59", "embedding": null, "metadata": {"window": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4. ", "original_text": "To be more exact, better result\nobtains higher score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afabe86c-a91b-42e2-837a-055c1fe4f43a", "node_type": "1", "metadata": {"window": "7\n\nFigure 3: Visual comparison on real-world datasets with upsampling scale factor of 4. ( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion. ", "original_text": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order. "}, "hash": "959c96a1ae81ca0eb01884e22f5ed2c282a54d07503860446ea1b1e45851d270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a3c8b2e-333d-4985-84b9-c95d5d75658b", "node_type": "1", "metadata": {"window": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. ", "original_text": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion. "}, "hash": "ca7c0a44cbf1368b0b2c3d3ae6a0f4d7b3a22a8afda182c7124194fd9d86def2", "class_name": "RelatedNodeInfo"}}, "hash": "8ccd244e9646d60a223fa3891d58bb1a4d5861df17a124af15dde6ea9d952de5", "text": "To be more exact, better result\nobtains higher score. ", "start_char_idx": 27775, "end_char_idx": 27829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a3c8b2e-333d-4985-84b9-c95d5d75658b": {"__data__": {"id_": "0a3c8b2e-333d-4985-84b9-c95d5d75658b", "embedding": null, "metadata": {"window": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. ", "original_text": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "264a7da6-5672-46cc-a9d9-2883e6cbab59", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nDiffBIR SwinIR-GAN RealESRGAN+ BSRGAN\nmodels1.52.02.53.03.54.0scoreDiffBIR\nSwinIR-GAN\nRealESRGAN+\nBSRGAN\nFigure 4: The distribution of scores obtained by\nSwinIR-GAN, Real-ESRGAN+, BSRGAN, and our\nDiffBIR in user study.To further compare DiffBIR with other state-\nof-the-art methods, we conduct a user study on\nour collected Real47 dataset.  This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4. ", "original_text": "To be more exact, better result\nobtains higher score. "}, "hash": "8ccd244e9646d60a223fa3891d58bb1a4d5861df17a124af15dde6ea9d952de5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae8e34f0-f350-4f1e-ba4e-6e48e8ac8785", "node_type": "1", "metadata": {"window": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. ", "original_text": "The distribution of scores obtained by each\nmethod is shown in Figure 4. "}, "hash": "6d5646d60da7f4c1a0a98a3a0d372ae367032e51ba628f72cdcdaf9b02c64c9d", "class_name": "RelatedNodeInfo"}}, "hash": "ca7c0a44cbf1368b0b2c3d3ae6a0f4d7b3a22a8afda182c7124194fd9d86def2", "text": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion. ", "start_char_idx": 27829, "end_char_idx": 27909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae8e34f0-f350-4f1e-ba4e-6e48e8ac8785": {"__data__": {"id_": "ae8e34f0-f350-4f1e-ba4e-6e48e8ac8785", "embedding": null, "metadata": {"window": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. ", "original_text": "The distribution of scores obtained by each\nmethod is shown in Figure 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a3c8b2e-333d-4985-84b9-c95d5d75658b", "node_type": "1", "metadata": {"window": "This user study\ncompares DiffBIR, SwinIR-GAN, BSRGAN,\nand RealESRGAN+.  For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. ", "original_text": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion. "}, "hash": "ca7c0a44cbf1368b0b2c3d3ae6a0f4d7b3a22a8afda182c7124194fd9d86def2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea68df9c-cf46-496d-abf7-aa2a033ae811", "node_type": "1", "metadata": {"window": "To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n", "original_text": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. "}, "hash": "14cc4bcd8bee5216c265632cab4257cd5eeb4c972cd6b542959dc30a16c42f8b", "class_name": "RelatedNodeInfo"}}, "hash": "6d5646d60da7f4c1a0a98a3a0d372ae367032e51ba628f72cdcdaf9b02c64c9d", "text": "The distribution of scores obtained by each\nmethod is shown in Figure 4. ", "start_char_idx": 27909, "end_char_idx": 27982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea68df9c-cf46-496d-abf7-aa2a033ae811": {"__data__": {"id_": "ea68df9c-cf46-496d-abf7-aa2a033ae811", "embedding": null, "metadata": {"window": "To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n", "original_text": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae8e34f0-f350-4f1e-ba4e-6e48e8ac8785", "node_type": "1", "metadata": {"window": "For each image, users are\nasked to rank the results of the four methods\nand assign 1-4 points to different methods in an\nascending order.  To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. ", "original_text": "The distribution of scores obtained by each\nmethod is shown in Figure 4. "}, "hash": "6d5646d60da7f4c1a0a98a3a0d372ae367032e51ba628f72cdcdaf9b02c64c9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e28c622-61bc-4e76-ab50-d6b305eabb25", "node_type": "1", "metadata": {"window": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets. ", "original_text": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. "}, "hash": "c9b23760f72f391d74488867f06c365b32441329dbf10d4caad06c2cd9e15cd9", "class_name": "RelatedNodeInfo"}}, "hash": "14cc4bcd8bee5216c265632cab4257cd5eeb4c972cd6b542959dc30a16c42f8b", "text": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. ", "start_char_idx": 27982, "end_char_idx": 28083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e28c622-61bc-4e76-ab50-d6b305eabb25": {"__data__": {"id_": "5e28c622-61bc-4e76-ab50-d6b305eabb25", "embedding": null, "metadata": {"window": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets. ", "original_text": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea68df9c-cf46-496d-abf7-aa2a033ae811", "node_type": "1", "metadata": {"window": "To be more exact, better result\nobtains higher score.  31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n", "original_text": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3. "}, "hash": "14cc4bcd8bee5216c265632cab4257cd5eeb4c972cd6b542959dc30a16c42f8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcdc9722-a8b3-4f5e-98f0-7ac9a518358a", "node_type": "1", "metadata": {"window": "The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. ", "original_text": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n"}, "hash": "e1e4395111ab84e746a6a630863a56f1273831d4d3253435f09cf584b1ddb1cd", "class_name": "RelatedNodeInfo"}}, "hash": "c9b23760f72f391d74488867f06c365b32441329dbf10d4caad06c2cd9e15cd9", "text": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. ", "start_char_idx": 28083, "end_char_idx": 28160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcdc9722-a8b3-4f5e-98f0-7ac9a518358a": {"__data__": {"id_": "fcdc9722-a8b3-4f5e-98f0-7ac9a518358a", "embedding": null, "metadata": {"window": "The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. ", "original_text": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e28c622-61bc-4e76-ab50-d6b305eabb25", "node_type": "1", "metadata": {"window": "31 users are recruited to\nconduct this user study under detailed instruc-\ntion.  The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets. ", "original_text": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place. "}, "hash": "c9b23760f72f391d74488867f06c365b32441329dbf10d4caad06c2cd9e15cd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2bff15d-6780-40f2-941d-b2efe98e3358", "node_type": "1", "metadata": {"window": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. ", "original_text": "BFR on both synthetic and real-world datasets. "}, "hash": "e700f4fcdff8980241b46c56208f13762577b124ba2e3ea38a05a308eac0839a", "class_name": "RelatedNodeInfo"}}, "hash": "e1e4395111ab84e746a6a630863a56f1273831d4d3253435f09cf584b1ddb1cd", "text": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n", "start_char_idx": 28160, "end_char_idx": 28311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2bff15d-6780-40f2-941d-b2efe98e3358": {"__data__": {"id_": "b2bff15d-6780-40f2-941d-b2efe98e3358", "embedding": null, "metadata": {"window": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. ", "original_text": "BFR on both synthetic and real-world datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcdc9722-a8b3-4f5e-98f0-7ac9a518358a", "node_type": "1", "metadata": {"window": "The distribution of scores obtained by each\nmethod is shown in Figure 4.  It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. ", "original_text": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n"}, "hash": "e1e4395111ab84e746a6a630863a56f1273831d4d3253435f09cf584b1ddb1cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b4eee06-f1c4-48a2-b404-796a5a74790f", "node_type": "1", "metadata": {"window": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n", "original_text": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. "}, "hash": "63babde0a3206b85b854d66cff31e05ac0ef24e3507fe92c6569ab98783a384a", "class_name": "RelatedNodeInfo"}}, "hash": "e700f4fcdff8980241b46c56208f13762577b124ba2e3ea38a05a308eac0839a", "text": "BFR on both synthetic and real-world datasets. ", "start_char_idx": 28311, "end_char_idx": 28358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b4eee06-f1c4-48a2-b404-796a5a74790f": {"__data__": {"id_": "7b4eee06-f1c4-48a2-b404-796a5a74790f", "embedding": null, "metadata": {"window": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n", "original_text": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2bff15d-6780-40f2-941d-b2efe98e3358", "node_type": "1", "metadata": {"window": "It can be observed\nthat DiffBIR achieves the highest median score,\nand its upper quartile exceeds 3.  This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. ", "original_text": "BFR on both synthetic and real-world datasets. "}, "hash": "e700f4fcdff8980241b46c56208f13762577b124ba2e3ea38a05a308eac0839a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40b27d67-e31b-4212-b1b7-c835225f8bb3", "node_type": "1", "metadata": {"window": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . ", "original_text": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. "}, "hash": "40eba104d0c51174c5a6ca95ca0fb5f896cb3f7594de404f338a560c1238dedc", "class_name": "RelatedNodeInfo"}}, "hash": "63babde0a3206b85b854d66cff31e05ac0ef24e3507fe92c6569ab98783a384a", "text": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. ", "start_char_idx": 28358, "end_char_idx": 28448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40b27d67-e31b-4212-b1b7-c835225f8bb3": {"__data__": {"id_": "40b27d67-e31b-4212-b1b7-c835225f8bb3", "embedding": null, "metadata": {"window": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . ", "original_text": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b4eee06-f1c4-48a2-b404-796a5a74790f", "node_type": "1", "metadata": {"window": "This indicates\nthat users tend to rank DiffBIR\u2019s results in the\nfirst place.  The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n", "original_text": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2. "}, "hash": "63babde0a3206b85b854d66cff31e05ac0ef24e3507fe92c6569ab98783a384a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94ad96d7-0a9f-4ac1-b0de-1c5eb0c0e9b3", "node_type": "1", "metadata": {"window": "BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. ", "original_text": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n"}, "hash": "5d021ebca03b887a801ac51f74612d5e86cd4125970ee05361836de3f02d4835", "class_name": "RelatedNodeInfo"}}, "hash": "40eba104d0c51174c5a6ca95ca0fb5f896cb3f7594de404f338a560c1238dedc", "text": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. ", "start_char_idx": 28448, "end_char_idx": 28537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94ad96d7-0a9f-4ac1-b0de-1c5eb0c0e9b3": {"__data__": {"id_": "94ad96d7-0a9f-4ac1-b0de-1c5eb0c0e9b3", "embedding": null, "metadata": {"window": "BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. ", "original_text": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40b27d67-e31b-4212-b1b7-c835225f8bb3", "node_type": "1", "metadata": {"window": "The user study results again demon-\nstrate that DiffBIR\u2019s visual results are superior\nto other methods, which aligns with its highest score on MANIQA.\n BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . ", "original_text": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score. "}, "hash": "40eba104d0c51174c5a6ca95ca0fb5f896cb3f7594de404f338a560c1238dedc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d025fa7-b0f7-4787-b39d-ec299f44664d", "node_type": "1", "metadata": {"window": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. ", "original_text": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . "}, "hash": "5fd8d581b5aa92a295f7cb45affdcebd2822e92d56bba7a143a66948bed38bd3", "class_name": "RelatedNodeInfo"}}, "hash": "5d021ebca03b887a801ac51f74612d5e86cd4125970ee05361836de3f02d4835", "text": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n", "start_char_idx": 28537, "end_char_idx": 28601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d025fa7-b0f7-4787-b39d-ec299f44664d": {"__data__": {"id_": "3d025fa7-b0f7-4787-b39d-ec299f44664d", "embedding": null, "metadata": {"window": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. ", "original_text": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94ad96d7-0a9f-4ac1-b0de-1c5eb0c0e9b3", "node_type": "1", "metadata": {"window": "BFR on both synthetic and real-world datasets.  We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. ", "original_text": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n"}, "hash": "5d021ebca03b887a801ac51f74612d5e86cd4125970ee05361836de3f02d4835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8936bab0-ae00-43f7-b24e-b54d3fbbb340", "node_type": "1", "metadata": {"window": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. ", "original_text": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. "}, "hash": "20db08b4f27f07a783871a917fbab62108ca62b178a4e05e0995a9bde097df4e", "class_name": "RelatedNodeInfo"}}, "hash": "5fd8d581b5aa92a295f7cb45affdcebd2822e92d56bba7a143a66948bed38bd3", "text": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . ", "start_char_idx": 28601, "end_char_idx": 28717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8936bab0-ae00-43f7-b24e-b54d3fbbb340": {"__data__": {"id_": "8936bab0-ae00-43f7-b24e-b54d3fbbb340", "embedding": null, "metadata": {"window": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. ", "original_text": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d025fa7-b0f7-4787-b39d-ec299f44664d", "node_type": "1", "metadata": {"window": "We show the quantitative comparison on both\nsynthetic and real-world datasets in Table 2.  For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. ", "original_text": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity . "}, "hash": "5fd8d581b5aa92a295f7cb45affdcebd2822e92d56bba7a143a66948bed38bd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2201a452-7b9d-4dd8-8491-457e36161411", "node_type": "1", "metadata": {"window": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. ", "original_text": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. "}, "hash": "48e41d3743bb8ac90321fb23c52668bb5c8d39639ee477864ca809d149a389ef", "class_name": "RelatedNodeInfo"}}, "hash": "20db08b4f27f07a783871a917fbab62108ca62b178a4e05e0995a9bde097df4e", "text": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. ", "start_char_idx": 28717, "end_char_idx": 28942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2201a452-7b9d-4dd8-8491-457e36161411": {"__data__": {"id_": "2201a452-7b9d-4dd8-8491-457e36161411", "embedding": null, "metadata": {"window": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. ", "original_text": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8936bab0-ae00-43f7-b24e-b54d3fbbb340", "node_type": "1", "metadata": {"window": "For the synthetic dataset CelebA-Test [ 39], our DiffBIR\nachieves the highest FID score.  Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. ", "original_text": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test. "}, "hash": "20db08b4f27f07a783871a917fbab62108ca62b178a4e05e0995a9bde097df4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe3b84ff-a1ca-4325-8237-32d9390e302d", "node_type": "1", "metadata": {"window": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset. ", "original_text": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. "}, "hash": "d40522ef844cb7ea9c833b8c61337807cc67d51933351757841f14246314daf4", "class_name": "RelatedNodeInfo"}}, "hash": "48e41d3743bb8ac90321fb23c52668bb5c8d39639ee477864ca809d149a389ef", "text": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. ", "start_char_idx": 28942, "end_char_idx": 29020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe3b84ff-a1ca-4325-8237-32d9390e302d": {"__data__": {"id_": "fe3b84ff-a1ca-4325-8237-32d9390e302d", "embedding": null, "metadata": {"window": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset. ", "original_text": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2201a452-7b9d-4dd8-8491-457e36161411", "node_type": "1", "metadata": {"window": "Meanwhile, it is also the top-3 methods regarding PSNR and IDS.\n This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. ", "original_text": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset. "}, "hash": "48e41d3743bb8ac90321fb23c52668bb5c8d39639ee477864ca809d149a389ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3191d42-95b2-4a7a-ae67-f149bee5fd76", "node_type": "1", "metadata": {"window": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. ", "original_text": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. "}, "hash": "f5c5f10e593d374a89e7410aef793117bd15df6ba33639da37c412d3cbaf23d8", "class_name": "RelatedNodeInfo"}}, "hash": "d40522ef844cb7ea9c833b8c61337807cc67d51933351757841f14246314daf4", "text": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. ", "start_char_idx": 29020, "end_char_idx": 29142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3191d42-95b2-4a7a-ae67-f149bee5fd76": {"__data__": {"id_": "a3191d42-95b2-4a7a-ae67-f149bee5fd76", "embedding": null, "metadata": {"window": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. ", "original_text": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe3b84ff-a1ca-4325-8237-32d9390e302d", "node_type": "1", "metadata": {"window": "This reveals that the proposed DiffBIR can successfully produce results with both high realness and\nhigh fidelity .  For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset. ", "original_text": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail. "}, "hash": "d40522ef844cb7ea9c833b8c61337807cc67d51933351757841f14246314daf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22435e08-5926-4e7e-8367-8ca08d528a88", "node_type": "1", "metadata": {"window": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). ", "original_text": "Figure 6 presents a visual comparison on real-world\ndataset. "}, "hash": "9f1b0ca33b83b761e2360371a77d11d1be5ad8ef1d9a76e6b96d261572732aa1", "class_name": "RelatedNodeInfo"}}, "hash": "f5c5f10e593d374a89e7410aef793117bd15df6ba33639da37c412d3cbaf23d8", "text": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. ", "start_char_idx": 29142, "end_char_idx": 29250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22435e08-5926-4e7e-8367-8ca08d528a88": {"__data__": {"id_": "22435e08-5926-4e7e-8367-8ca08d528a88", "embedding": null, "metadata": {"window": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). ", "original_text": "Figure 6 presents a visual comparison on real-world\ndataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3191d42-95b2-4a7a-ae67-f149bee5fd76", "node_type": "1", "metadata": {"window": "For real-world datasets, DiffBIR obtains the best results on both LFW-Test [ 22] (mild\ndegradation) and WIDER-Test [ 68] (heavy degradation) datasets, and comparable results with state-\nof-the-art methods on CelebChild-Test.  Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. ", "original_text": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye. "}, "hash": "f5c5f10e593d374a89e7410aef793117bd15df6ba33639da37c412d3cbaf23d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0673045b-dbd3-442e-b459-5232f9143787", "node_type": "1", "metadata": {"window": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n", "original_text": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. "}, "hash": "5fa8e4b852e0eb42c91f63a7a483ba39c10fbde4d424575002e37afe7f5892de", "class_name": "RelatedNodeInfo"}}, "hash": "9f1b0ca33b83b761e2360371a77d11d1be5ad8ef1d9a76e6b96d261572732aa1", "text": "Figure 6 presents a visual comparison on real-world\ndataset. ", "start_char_idx": 29250, "end_char_idx": 29311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0673045b-dbd3-442e-b459-5232f9143787": {"__data__": {"id_": "0673045b-dbd3-442e-b459-5232f9143787", "embedding": null, "metadata": {"window": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n", "original_text": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22435e08-5926-4e7e-8367-8ca08d528a88", "node_type": "1", "metadata": {"window": "Figure 5 depicts a visual comparison of various methods on\nsynthetic dataset.  The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). ", "original_text": "Figure 6 presents a visual comparison on real-world\ndataset. "}, "hash": "9f1b0ca33b83b761e2360371a77d11d1be5ad8ef1d9a76e6b96d261572732aa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea357866-e618-4e1b-bb5f-36d216bba1ca", "node_type": "1", "metadata": {"window": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets.", "original_text": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). "}, "hash": "d06796566811fdd00ce6cdbfec451336adb07860b3f2f20c18615c31882ad4fb", "class_name": "RelatedNodeInfo"}}, "hash": "5fa8e4b852e0eb42c91f63a7a483ba39c10fbde4d424575002e37afe7f5892de", "text": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. ", "start_char_idx": 29311, "end_char_idx": 29474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea357866-e618-4e1b-bb5f-36d216bba1ca": {"__data__": {"id_": "ea357866-e618-4e1b-bb5f-36d216bba1ca", "embedding": null, "metadata": {"window": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets.", "original_text": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0673045b-dbd3-442e-b459-5232f9143787", "node_type": "1", "metadata": {"window": "The first example demonstrates that only DiffBIR succeeds in restoring extremely\ndegraded cases while other methods fail.  It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n", "original_text": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area. "}, "hash": "5fa8e4b852e0eb42c91f63a7a483ba39c10fbde4d424575002e37afe7f5892de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43bdd6e7-8794-49be-afc9-a85a9be48750", "node_type": "1", "metadata": {"window": "Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets.", "original_text": "More visualization results can be found in Figure 9 and Figure 10.\n"}, "hash": "cde76dd04175ec10f286a4ad0cf97627265448df6743f0e5ffb8b2cf7f3a61a3", "class_name": "RelatedNodeInfo"}}, "hash": "d06796566811fdd00ce6cdbfec451336adb07860b3f2f20c18615c31882ad4fb", "text": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). ", "start_char_idx": 29474, "end_char_idx": 29631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43bdd6e7-8794-49be-afc9-a85a9be48750": {"__data__": {"id_": "43bdd6e7-8794-49be-afc9-a85a9be48750", "embedding": null, "metadata": {"window": "Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets.", "original_text": "More visualization results can be found in Figure 9 and Figure 10.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea357866-e618-4e1b-bb5f-36d216bba1ca", "node_type": "1", "metadata": {"window": "It can be seen from the second example that only DiffBIR\n8\n\ncan successfully recover the occluded left eye.  Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets.", "original_text": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead). "}, "hash": "d06796566811fdd00ce6cdbfec451336adb07860b3f2f20c18615c31882ad4fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bae9b51-7b4f-48ec-9c18-0de7e4606164", "node_type": "1", "metadata": {"window": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. ", "original_text": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets."}, "hash": "f20d94801d511ebd40ac6fa6f6327682f8af34f0707bd08bced4795d332662d2", "class_name": "RelatedNodeInfo"}}, "hash": "cde76dd04175ec10f286a4ad0cf97627265448df6743f0e5ffb8b2cf7f3a61a3", "text": "More visualization results can be found in Figure 9 and Figure 10.\n", "start_char_idx": 29631, "end_char_idx": 29698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bae9b51-7b4f-48ec-9c18-0de7e4606164": {"__data__": {"id_": "5bae9b51-7b4f-48ec-9c18-0de7e4606164", "embedding": null, "metadata": {"window": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. ", "original_text": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43bdd6e7-8794-49be-afc9-a85a9be48750", "node_type": "1", "metadata": {"window": "Figure 6 presents a visual comparison on real-world\ndataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets.", "original_text": "More visualization results can be found in Figure 9 and Figure 10.\n"}, "hash": "cde76dd04175ec10f286a4ad0cf97627265448df6743f0e5ffb8b2cf7f3a61a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8afd5ff6-7d40-4bbd-9b77-8f22deff6342", "node_type": "1", "metadata": {"window": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance. ", "original_text": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets."}, "hash": "6725e4a0adee6cc59e5d72c317159cbae3bdf14b0032a977a12178cde38d0e5e", "class_name": "RelatedNodeInfo"}}, "hash": "f20d94801d511ebd40ac6fa6f6327682f8af34f0707bd08bced4795d332662d2", "text": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets.", "start_char_idx": 29698, "end_char_idx": 29778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8afd5ff6-7d40-4bbd-9b77-8f22deff6342": {"__data__": {"id_": "8afd5ff6-7d40-4bbd-9b77-8f22deff6342", "embedding": null, "metadata": {"window": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance. ", "original_text": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bae9b51-7b4f-48ec-9c18-0de7e4606164", "node_type": "1", "metadata": {"window": "It can be observed from the first example that DiffBIR is able to accurately restore the hair,\nwhile other methods mistake the hair for a part of the facial area.  The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. ", "original_text": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets."}, "hash": "f20d94801d511ebd40ac6fa6f6327682f8af34f0707bd08bced4795d332662d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0a1e8a3-324d-463c-afeb-c492ed9daa2f", "node_type": "1", "metadata": {"window": "More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n", "original_text": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. "}, "hash": "8e8c2c588dc9edca005f6226a9614def5523ab0cc15509dc3fa20393fb0dc7b0", "class_name": "RelatedNodeInfo"}}, "hash": "6725e4a0adee6cc59e5d72c317159cbae3bdf14b0032a977a12178cde38d0e5e", "text": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets.", "start_char_idx": 29778, "end_char_idx": 29885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0a1e8a3-324d-463c-afeb-c492ed9daa2f": {"__data__": {"id_": "f0a1e8a3-324d-463c-afeb-c492ed9daa2f", "embedding": null, "metadata": {"window": "More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n", "original_text": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8afd5ff6-7d40-4bbd-9b77-8f22deff6342", "node_type": "1", "metadata": {"window": "The second example suggests that\nour DiffBIR is the only method that can generate realistic details on non-face area ( i.e.,the decoration\nin the forehead).  More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance. ", "original_text": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets."}, "hash": "6725e4a0adee6cc59e5d72c317159cbae3bdf14b0032a977a12178cde38d0e5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e3548b1-1470-47c6-bec4-36389d703f92", "node_type": "1", "metadata": {"window": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. ", "original_text": "Red\nand blue indicate the best and second best performance. "}, "hash": "e7eaffd1170028a0e34b61a70dfd371b5c07c05771505295e20617413927365a", "class_name": "RelatedNodeInfo"}}, "hash": "8e8c2c588dc9edca005f6226a9614def5523ab0cc15509dc3fa20393fb0dc7b0", "text": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. ", "start_char_idx": 29885, "end_char_idx": 30020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e3548b1-1470-47c6-bec4-36389d703f92": {"__data__": {"id_": "5e3548b1-1470-47c6-bec4-36389d703f92", "embedding": null, "metadata": {"window": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. ", "original_text": "Red\nand blue indicate the best and second best performance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0a1e8a3-324d-463c-afeb-c492ed9daa2f", "node_type": "1", "metadata": {"window": "More visualization results can be found in Figure 9 and Figure 10.\n Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n", "original_text": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. "}, "hash": "8e8c2c588dc9edca005f6226a9614def5523ab0cc15509dc3fa20393fb0dc7b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da45dcc2-adc2-42dc-8ceb-e72a331052d1", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline. ", "original_text": "The top 3 results are marked as gray .\n"}, "hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "class_name": "RelatedNodeInfo"}}, "hash": "e7eaffd1170028a0e34b61a70dfd371b5c07c05771505295e20617413927365a", "text": "Red\nand blue indicate the best and second best performance. ", "start_char_idx": 30020, "end_char_idx": 30080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da45dcc2-adc2-42dc-8ceb-e72a331052d1": {"__data__": {"id_": "da45dcc2-adc2-42dc-8ceb-e72a331052d1", "embedding": null, "metadata": {"window": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline. ", "original_text": "The top 3 results are marked as gray .\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e3548b1-1470-47c6-bec4-36389d703f92", "node_type": "1", "metadata": {"window": "Figure 5: Qualitative comparison of different BFR methods on synthetic datasets. ( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. ", "original_text": "Red\nand blue indicate the best and second best performance. "}, "hash": "e7eaffd1170028a0e34b61a70dfd371b5c07c05771505295e20617413927365a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b37ddbcd-97ea-4547-b4a6-a68f00ef1057", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. ", "original_text": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. "}, "hash": "60fd1fae8cb3b307b132cc2d4c0958bd3f3e07e78ae1cbde161dd1843a5440ab", "class_name": "RelatedNodeInfo"}}, "hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "text": "The top 3 results are marked as gray .\n", "start_char_idx": 26420, "end_char_idx": 26459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b37ddbcd-97ea-4547-b4a6-a68f00ef1057": {"__data__": {"id_": "b37ddbcd-97ea-4547-b4a6-a68f00ef1057", "embedding": null, "metadata": {"window": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. ", "original_text": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da45dcc2-adc2-42dc-8ceb-e72a331052d1", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nFigure 6: Qualitative comparison of different BFR methods on real-world datasets. ( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline. ", "original_text": "The top 3 results are marked as gray .\n"}, "hash": "2d12dadd54a2151cf575707dfb10009fd53f4c6145efc1faba0271dbf61d9a88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfb5de91-db01-4939-bc33-3616592cd6c2", "node_type": "1", "metadata": {"window": "Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). ", "original_text": "In this part, we investigate the significance of our\nproposed two-stage pipeline. "}, "hash": "70853cb48b44678a839eaf615c355f5860e4c609c647995ab58d21c92ce79a99", "class_name": "RelatedNodeInfo"}}, "hash": "60fd1fae8cb3b307b132cc2d4c0958bd3f3e07e78ae1cbde161dd1843a5440ab", "text": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. ", "start_char_idx": 30119, "end_char_idx": 30809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfb5de91-db01-4939-bc33-3616592cd6c2": {"__data__": {"id_": "dfb5de91-db01-4939-bc33-3616592cd6c2", "embedding": null, "metadata": {"window": "Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). ", "original_text": "In this part, we investigate the significance of our\nproposed two-stage pipeline. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b37ddbcd-97ea-4547-b4a6-a68f00ef1057", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n9\n\nTable 2: Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets.  Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. ", "original_text": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module. "}, "hash": "60fd1fae8cb3b307b132cc2d4c0958bd3f3e07e78ae1cbde161dd1843a5440ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e5cefba-4137-4228-bc24-b083b010f990", "node_type": "1", "metadata": {"window": "The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a). ", "original_text": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. "}, "hash": "576676ac6935eeebd43f21898532f22fd47183b1820a7eef5dfb168d69d3e178", "class_name": "RelatedNodeInfo"}}, "hash": "70853cb48b44678a839eaf615c355f5860e4c609c647995ab58d21c92ce79a99", "text": "In this part, we investigate the significance of our\nproposed two-stage pipeline. ", "start_char_idx": 30809, "end_char_idx": 30891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e5cefba-4137-4228-bc24-b083b010f990": {"__data__": {"id_": "2e5cefba-4137-4228-bc24-b083b010f990", "embedding": null, "metadata": {"window": "The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a). ", "original_text": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfb5de91-db01-4939-bc33-3616592cd6c2", "node_type": "1", "metadata": {"window": "Red\nand blue indicate the best and second best performance.  The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). ", "original_text": "In this part, we investigate the significance of our\nproposed two-stage pipeline. "}, "hash": "70853cb48b44678a839eaf615c355f5860e4c609c647995ab58d21c92ce79a99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b281d00-d5f4-4aa5-8ecf-cc1969f2e781", "node_type": "1", "metadata": {"window": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. ", "original_text": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). "}, "hash": "1fc785f470e763c04158be3683264e0694f2dbb1259d0269f240c7c829ca60d7", "class_name": "RelatedNodeInfo"}}, "hash": "576676ac6935eeebd43f21898532f22fd47183b1820a7eef5dfb168d69d3e178", "text": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. ", "start_char_idx": 30891, "end_char_idx": 31011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b281d00-d5f4-4aa5-8ecf-cc1969f2e781": {"__data__": {"id_": "6b281d00-d5f4-4aa5-8ecf-cc1969f2e781", "embedding": null, "metadata": {"window": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. ", "original_text": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e5cefba-4137-4228-bc24-b083b010f990", "node_type": "1", "metadata": {"window": "The top 3 results are marked as gray .\n Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a). ", "original_text": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs. "}, "hash": "576676ac6935eeebd43f21898532f22fd47183b1820a7eef5dfb168d69d3e178", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25c39189-98a2-4e25-8fd1-d516a41830bd", "node_type": "1", "metadata": {"window": "In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity. ", "original_text": "The\nvisual comparison is presented in Figure 7(a). "}, "hash": "47dbc5044b14e01f72a75c441f87755b656d125d9edaee8113a7cd340dfd37d0", "class_name": "RelatedNodeInfo"}}, "hash": "1fc785f470e763c04158be3683264e0694f2dbb1259d0269f240c7c829ca60d7", "text": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). ", "start_char_idx": 31011, "end_char_idx": 31144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25c39189-98a2-4e25-8fd1-d516a41830bd": {"__data__": {"id_": "25c39189-98a2-4e25-8fd1-d516a41830bd", "embedding": null, "metadata": {"window": "In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity. ", "original_text": "The\nvisual comparison is presented in Figure 7(a). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b281d00-d5f4-4aa5-8ecf-cc1969f2e781", "node_type": "1", "metadata": {"window": "Synthetic WildDatasetCelebA-Test LFW-Test WIDER-Test CelebChild-Test\nMethod PSNR \u2191SSIM \u2191LPIPS \u2193FID\u2193IDS\u2191 FID\u2193 FID\u2193 FID\u2193\nGPEN [61] 21.3995 0.5742 0.4687 23.92 0.48 51.97 46.35 76.58\nGCFSR [19] 21.8791 0.6072 0.4577 35.49 0.44 52.20 40.86 76.29\nGFPGAN [54] 21.6953 0.6060 0.4304 21.69 0.49 52.11 41.70 80.69\nVQFR [18] 21.3014 0.6132 0.4116 20.30 0.48 49.88 37.87 74.76\nRestoreFormer [59] 21.0025 0.5283 0.4789 43.77 0.56 48.43 49.79 70.54\nDMDNet [35] 21.6617 0.6000 0.4828 64.79 0.67 43.36 40.51 79.38\nCodeFormer [68] 22.1519 0.5948 0.4055 22.19 0.47 52.37 38.78 79.54\nDiffBIR(Ours) 21.7509 0.5971 0.4573 20.02 0.51 39.58 32.35 75.94\n4.3 Ablation Studies\nThe Importance of Restoration Module.  In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. ", "original_text": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3). "}, "hash": "1fc785f470e763c04158be3683264e0694f2dbb1259d0269f240c7c829ca60d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a181d98-6a92-4c06-9794-d3230b963753", "node_type": "1", "metadata": {"window": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. ", "original_text": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. "}, "hash": "be57fee454954c13f05603b7e2c3a2952a43d0ca2a154caf25ce60f4e7cb7595", "class_name": "RelatedNodeInfo"}}, "hash": "47dbc5044b14e01f72a75c441f87755b656d125d9edaee8113a7cd340dfd37d0", "text": "The\nvisual comparison is presented in Figure 7(a). ", "start_char_idx": 31144, "end_char_idx": 31195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a181d98-6a92-4c06-9794-d3230b963753": {"__data__": {"id_": "1a181d98-6a92-4c06-9794-d3230b963753", "embedding": null, "metadata": {"window": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. ", "original_text": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25c39189-98a2-4e25-8fd1-d516a41830bd", "node_type": "1", "metadata": {"window": "In this part, we investigate the significance of our\nproposed two-stage pipeline.  Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity. ", "original_text": "The\nvisual comparison is presented in Figure 7(a). "}, "hash": "47dbc5044b14e01f72a75c441f87755b656d125d9edaee8113a7cd340dfd37d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dbda129-b751-4b3a-9fe5-e73b0c20b721", "node_type": "1", "metadata": {"window": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n", "original_text": "This demonstrates that the\nrestoration module contributes to preserving fidelity. "}, "hash": "6b4b624f24909c99922939744b0813b720e268bbcc75c97b380283e69026e15b", "class_name": "RelatedNodeInfo"}}, "hash": "be57fee454954c13f05603b7e2c3a2952a43d0ca2a154caf25ce60f4e7cb7595", "text": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. ", "start_char_idx": 31195, "end_char_idx": 31317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dbda129-b751-4b3a-9fe5-e73b0c20b721": {"__data__": {"id_": "3dbda129-b751-4b3a-9fe5-e73b0c20b721", "embedding": null, "metadata": {"window": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n", "original_text": "This demonstrates that the\nrestoration module contributes to preserving fidelity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a181d98-6a92-4c06-9794-d3230b963753", "node_type": "1", "metadata": {"window": "Here, we remove the Restoration Module (RM), and directly finetune\nthe diffusion model with synthesized training pairs.  The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. ", "original_text": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake. "}, "hash": "be57fee454954c13f05603b7e2c3a2952a43d0ca2a154caf25ce60f4e7cb7595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "423c5cf0-a213-4c56-b664-06053a2ebe72", "node_type": "1", "metadata": {"window": "The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. ", "original_text": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. "}, "hash": "0594554950f3046526d6562f83ae0c8f690ec49b91aa5fb561a16cdc8ca52db7", "class_name": "RelatedNodeInfo"}}, "hash": "6b4b624f24909c99922939744b0813b720e268bbcc75c97b380283e69026e15b", "text": "This demonstrates that the\nrestoration module contributes to preserving fidelity. ", "start_char_idx": 31317, "end_char_idx": 31399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "423c5cf0-a213-4c56-b664-06053a2ebe72": {"__data__": {"id_": "423c5cf0-a213-4c56-b664-06053a2ebe72", "embedding": null, "metadata": {"window": "The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. ", "original_text": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dbda129-b751-4b3a-9fe5-e73b0c20b721", "node_type": "1", "metadata": {"window": "The removal of restoration module leads to\na noticeable performance drop in FID/MANIQA across all real-world datasets (see Table 3).  The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n", "original_text": "This demonstrates that the\nrestoration module contributes to preserving fidelity. "}, "hash": "6b4b624f24909c99922939744b0813b720e268bbcc75c97b380283e69026e15b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f76bebaf-e95b-4e88-87ca-1a3c4d44c887", "node_type": "1", "metadata": {"window": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. ", "original_text": "This indicates that the RM is indispensable in degradation removal.\n"}, "hash": "8d639083af966c25ed15f51044d70daf83429968c548ed3d0f4da34568449992", "class_name": "RelatedNodeInfo"}}, "hash": "0594554950f3046526d6562f83ae0c8f690ec49b91aa5fb561a16cdc8ca52db7", "text": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. ", "start_char_idx": 31399, "end_char_idx": 31558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f76bebaf-e95b-4e88-87ca-1a3c4d44c887": {"__data__": {"id_": "f76bebaf-e95b-4e88-87ca-1a3c4d44c887", "embedding": null, "metadata": {"window": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. ", "original_text": "This indicates that the RM is indispensable in degradation removal.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423c5cf0-a213-4c56-b664-06053a2ebe72", "node_type": "1", "metadata": {"window": "The\nvisual comparison is presented in Figure 7(a).  As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. ", "original_text": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts. "}, "hash": "0594554950f3046526d6562f83ae0c8f690ec49b91aa5fb561a16cdc8ca52db7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a865b445-e410-4f5c-9381-4c3ee63d043b", "node_type": "1", "metadata": {"window": "This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet.", "original_text": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. "}, "hash": "5232ddeadf08a010ec91992fae4b997dffe035800204af740fede47ff1eedd89", "class_name": "RelatedNodeInfo"}}, "hash": "8d639083af966c25ed15f51044d70daf83429968c548ed3d0f4da34568449992", "text": "This indicates that the RM is indispensable in degradation removal.\n", "start_char_idx": 31558, "end_char_idx": 31626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a865b445-e410-4f5c-9381-4c3ee63d043b": {"__data__": {"id_": "a865b445-e410-4f5c-9381-4c3ee63d043b", "embedding": null, "metadata": {"window": "This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet.", "original_text": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f76bebaf-e95b-4e88-87ca-1a3c4d44c887", "node_type": "1", "metadata": {"window": "As seen from the first example, the one-stage model\n(w/o RM) regards the degradations as semantic information by mistake.  This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. ", "original_text": "This indicates that the RM is indispensable in degradation removal.\n"}, "hash": "8d639083af966c25ed15f51044d70daf83429968c548ed3d0f4da34568449992", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "180a0f06-0e26-46c9-acdc-a6710785befe", "node_type": "1", "metadata": {"window": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module. ", "original_text": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. "}, "hash": "1e97b5a84b84480ef2fd784897c1cae416d84325515f0d08521a1ba99fc9d7c3", "class_name": "RelatedNodeInfo"}}, "hash": "5232ddeadf08a010ec91992fae4b997dffe035800204af740fede47ff1eedd89", "text": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. ", "start_char_idx": 31626, "end_char_idx": 31768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "180a0f06-0e26-46c9-acdc-a6710785befe": {"__data__": {"id_": "180a0f06-0e26-46c9-acdc-a6710785befe", "embedding": null, "metadata": {"window": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module. ", "original_text": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a865b445-e410-4f5c-9381-4c3ee63d043b", "node_type": "1", "metadata": {"window": "This demonstrates that the\nrestoration module contributes to preserving fidelity.  The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet.", "original_text": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies. "}, "hash": "5232ddeadf08a010ec91992fae4b997dffe035800204af740fede47ff1eedd89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8c50c35-4031-4542-a6a1-b6986dccffe8", "node_type": "1", "metadata": {"window": "This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n", "original_text": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet."}, "hash": "9b37982ba9fcdd36f4fd7b88228de6d545fa729f65a32c20c323cf943ff20cef", "class_name": "RelatedNodeInfo"}}, "hash": "1e97b5a84b84480ef2fd784897c1cae416d84325515f0d08521a1ba99fc9d7c3", "text": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. ", "start_char_idx": 31768, "end_char_idx": 32033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8c50c35-4031-4542-a6a1-b6986dccffe8": {"__data__": {"id_": "e8c50c35-4031-4542-a6a1-b6986dccffe8", "embedding": null, "metadata": {"window": "This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n", "original_text": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "180a0f06-0e26-46c9-acdc-a6710785befe", "node_type": "1", "metadata": {"window": "The second example clearly illustrates that\nsolely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world\nnoise/artifacts.  This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module. ", "original_text": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results. "}, "hash": "1e97b5a84b84480ef2fd784897c1cae416d84325515f0d08521a1ba99fc9d7c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a1490e0-96ca-4506-acaf-953269b8d3db", "node_type": "1", "metadata": {"window": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. ", "original_text": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module. "}, "hash": "531040073953866a72cf046ebf6fa6e1311fbd0773265792fb7411a165d4cd07", "class_name": "RelatedNodeInfo"}}, "hash": "9b37982ba9fcdd36f4fd7b88228de6d545fa729f65a32c20c323cf943ff20cef", "text": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet.", "start_char_idx": 32033, "end_char_idx": 32122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a1490e0-96ca-4506-acaf-953269b8d3db": {"__data__": {"id_": "7a1490e0-96ca-4506-acaf-953269b8d3db", "embedding": null, "metadata": {"window": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. ", "original_text": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8c50c35-4031-4542-a6a1-b6986dccffe8", "node_type": "1", "metadata": {"window": "This indicates that the RM is indispensable in degradation removal.\n LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n", "original_text": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet."}, "hash": "9b37982ba9fcdd36f4fd7b88228de6d545fa729f65a32c20c323cf943ff20cef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c1158d7-90e4-468e-82cb-473f18fe992f", "node_type": "1", "metadata": {"window": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model. ", "original_text": "The best results are denoted as Red.\n"}, "hash": "11e2857b4903f1637c56ae67f607f2ec5234346345dbee604fc13a3a399ec87a", "class_name": "RelatedNodeInfo"}}, "hash": "531040073953866a72cf046ebf6fa6e1311fbd0773265792fb7411a165d4cd07", "text": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module. ", "start_char_idx": 32122, "end_char_idx": 32198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c1158d7-90e4-468e-82cb-473f18fe992f": {"__data__": {"id_": "8c1158d7-90e4-468e-82cb-473f18fe992f", "embedding": null, "metadata": {"window": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model. ", "original_text": "The best results are denoted as Red.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a1490e0-96ca-4506-acaf-953269b8d3db", "node_type": "1", "metadata": {"window": "LQ w/ RM w/o RM\n(a)\nLQ w/ finetuning w/o finetuning (b)\nLQ w/ LAControlNet w/ ControlNet (c)\nFigure 7: Visual comparison of ablation studies.  (a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. ", "original_text": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module. "}, "hash": "531040073953866a72cf046ebf6fa6e1311fbd0773265792fb7411a165d4cd07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fc0df74-0385-4391-b4f9-0eb20b6f1b64", "node_type": "1", "metadata": {"window": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. ", "original_text": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. "}, "hash": "c26aea36dc6bffc34748af3fa4ece5d51303d1f4513dd6650802d55f457adf4f", "class_name": "RelatedNodeInfo"}}, "hash": "11e2857b4903f1637c56ae67f607f2ec5234346345dbee604fc13a3a399ec87a", "text": "The best results are denoted as Red.\n", "start_char_idx": 32198, "end_char_idx": 32235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fc0df74-0385-4391-b4f9-0eb20b6f1b64": {"__data__": {"id_": "9fc0df74-0385-4391-b4f9-0eb20b6f1b64", "embedding": null, "metadata": {"window": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. ", "original_text": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c1158d7-90e4-468e-82cb-473f18fe992f", "node_type": "1", "metadata": {"window": "(a) DiffBIR w/o restoration module performs poorly in both\nfidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion,\ndirectly applying the image guidance technique [ 57;16] is not able to produce realistic results.  (c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model. ", "original_text": "The best results are denoted as Red.\n"}, "hash": "11e2857b4903f1637c56ae67f607f2ec5234346345dbee604fc13a3a399ec87a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "842d6c2c-8db7-4e4b-aa8e-e1f6c557335b", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n", "original_text": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model. "}, "hash": "7985fa01290fee918ce91716b78d46ec20d88c77c2b8a7fbdd0e967271440f8d", "class_name": "RelatedNodeInfo"}}, "hash": "c26aea36dc6bffc34748af3fa4ece5d51303d1f4513dd6650802d55f457adf4f", "text": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. ", "start_char_idx": 32235, "end_char_idx": 32484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "842d6c2c-8db7-4e4b-aa8e-e1f6c557335b": {"__data__": {"id_": "842d6c2c-8db7-4e4b-aa8e-e1f6c557335b", "embedding": null, "metadata": {"window": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n", "original_text": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fc0df74-0385-4391-b4f9-0eb20b6f1b64", "node_type": "1", "metadata": {"window": "(c) ControlNet\n[66] has a color shift problem which can be addressed by our LAControlNet. ( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. ", "original_text": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion. "}, "hash": "c26aea36dc6bffc34748af3fa4ece5d51303d1f4513dd6650802d55f457adf4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57cd9472-f5c5-45a8-9a29-d8d2123ad685", "node_type": "1", "metadata": {"window": "The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). ", "original_text": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. "}, "hash": "46353d9aa42833381af75d5c224e257e57cde2ee88213b27ad9a21edb1d173ac", "class_name": "RelatedNodeInfo"}}, "hash": "7985fa01290fee918ce91716b78d46ec20d88c77c2b8a7fbdd0e967271440f8d", "text": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model. ", "start_char_idx": 32484, "end_char_idx": 32560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57cd9472-f5c5-45a8-9a29-d8d2123ad685": {"__data__": {"id_": "57cd9472-f5c5-45a8-9a29-d8d2123ad685", "embedding": null, "metadata": {"window": "The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). ", "original_text": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "842d6c2c-8db7-4e4b-aa8e-e1f6c557335b", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nTable 3: The effectiveness of restoration module.  The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n", "original_text": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model. "}, "hash": "7985fa01290fee918ce91716b78d46ec20d88c77c2b8a7fbdd0e967271440f8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd3bda7a-cbff-47b5-8188-eb636b7d35ac", "node_type": "1", "metadata": {"window": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n", "original_text": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n"}, "hash": "6d713a879eedf9b8ff358835bf0091c1158c78db7f990314e85c941c69e3280b", "class_name": "RelatedNodeInfo"}}, "hash": "46353d9aa42833381af75d5c224e257e57cde2ee88213b27ad9a21edb1d173ac", "text": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. ", "start_char_idx": 32560, "end_char_idx": 32707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd3bda7a-cbff-47b5-8188-eb636b7d35ac": {"__data__": {"id_": "dd3bda7a-cbff-47b5-8188-eb636b7d35ac", "embedding": null, "metadata": {"window": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n", "original_text": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57cd9472-f5c5-45a8-9a29-d8d2123ad685", "node_type": "1", "metadata": {"window": "The best results are denoted as Red.\n Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). ", "original_text": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space. "}, "hash": "46353d9aa42833381af75d5c224e257e57cde2ee88213b27ad9a21edb1d173ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c81086e-54a6-412c-9b4c-c1e893ee279a", "node_type": "1", "metadata": {"window": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet. ", "original_text": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). "}, "hash": "ef2e09588509719b2bbe8958aadfa29b2971c5670f538bb41baf82ba00507d6d", "class_name": "RelatedNodeInfo"}}, "hash": "6d713a879eedf9b8ff358835bf0091c1158c78db7f990314e85c941c69e3280b", "text": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n", "start_char_idx": 32707, "end_char_idx": 32829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c81086e-54a6-412c-9b4c-c1e893ee279a": {"__data__": {"id_": "9c81086e-54a6-412c-9b4c-c1e893ee279a", "embedding": null, "metadata": {"window": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet. ", "original_text": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd3bda7a-cbff-47b5-8188-eb636b7d35ac", "node_type": "1", "metadata": {"window": "Face GeneralDatasetLFW-Test WIDER-Test CelebChild-Test RealSRSet Real47\nMethod FID \u2193 FID\u2193 FID\u2193 MANIQA \u2191MANIQA \u2191\nDiffBIR(w/o RM) 40.78 33.22 75.98 0.582 0.624\nDiffBIR(w/ RM) 39.58 32.35 75.94 0.591 0.629\nThe Necessity of Finetuning Stable Diffusion.  Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n", "original_text": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n"}, "hash": "6d713a879eedf9b8ff358835bf0091c1158c78db7f990314e85c941c69e3280b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1021aa89-6d09-48da-89ed-d9933d98a680", "node_type": "1", "metadata": {"window": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. ", "original_text": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n"}, "hash": "c61de3fa3ec87595c80f5da52915b4b102e315737514f975b0118f6f6b1badd5", "class_name": "RelatedNodeInfo"}}, "hash": "ef2e09588509719b2bbe8958aadfa29b2971c5670f538bb41baf82ba00507d6d", "text": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). ", "start_char_idx": 32829, "end_char_idx": 32965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1021aa89-6d09-48da-89ed-d9933d98a680": {"__data__": {"id_": "1021aa89-6d09-48da-89ed-d9933d98a680", "embedding": null, "metadata": {"window": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. ", "original_text": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c81086e-54a6-412c-9b4c-c1e893ee279a", "node_type": "1", "metadata": {"window": "Next, we illustrate the necessity of finetuning the\nlatent diffusion model.  Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet. ", "original_text": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing). "}, "hash": "ef2e09588509719b2bbe8958aadfa29b2971c5670f538bb41baf82ba00507d6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dbb3559-30eb-4045-bc5d-b26036e0e7fb", "node_type": "1", "metadata": {"window": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n", "original_text": "The Effectiveness of LAControlNet. "}, "hash": "69408ed392ba8395e1cb9a998bcea474ac771375f9cd21a734a24117c22ead41", "class_name": "RelatedNodeInfo"}}, "hash": "c61de3fa3ec87595c80f5da52915b4b102e315737514f975b0118f6f6b1badd5", "text": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n", "start_char_idx": 32965, "end_char_idx": 33171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dbb3559-30eb-4045-bc5d-b26036e0e7fb": {"__data__": {"id_": "8dbb3559-30eb-4045-bc5d-b26036e0e7fb", "embedding": null, "metadata": {"window": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n", "original_text": "The Effectiveness of LAControlNet. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1021aa89-6d09-48da-89ed-d9933d98a680", "node_type": "1", "metadata": {"window": "Zero-shot IR methods [57; 16] provide an effective approach that guides the\nreverse diffusion process using the degraded image in the image space.  Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. ", "original_text": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n"}, "hash": "c61de3fa3ec87595c80f5da52915b4b102e315737514f975b0118f6f6b1badd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f894422d-3968-47b3-91b2-e2cb16415f3b", "node_type": "1", "metadata": {"window": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. ", "original_text": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. "}, "hash": "25e7f73e0e5c3806cd5d038b53136406f9ffc73439fd2f8c812d67106eba3a9c", "class_name": "RelatedNodeInfo"}}, "hash": "69408ed392ba8395e1cb9a998bcea474ac771375f9cd21a734a24117c22ead41", "text": "The Effectiveness of LAControlNet. ", "start_char_idx": 33171, "end_char_idx": 33206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f894422d-3968-47b3-91b2-e2cb16415f3b": {"__data__": {"id_": "f894422d-3968-47b3-91b2-e2cb16415f3b", "embedding": null, "metadata": {"window": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. ", "original_text": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dbb3559-30eb-4045-bc5d-b26036e0e7fb", "node_type": "1", "metadata": {"window": "Following their methodology,\nwe employ the smoothed result Iregto guide the original Stable Diffusion without finetuning.\n However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n", "original_text": "The Effectiveness of LAControlNet. "}, "hash": "69408ed392ba8395e1cb9a998bcea474ac771375f9cd21a734a24117c22ead41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e78df41f-e7b1-4c05-b561-bfdb4dfc2c50", "node_type": "1", "metadata": {"window": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n", "original_text": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n"}, "hash": "4c6d0b2fa03c2b5975cde39d038779b2a469d4fce31a2afc34c2db9f5bedeaf6", "class_name": "RelatedNodeInfo"}}, "hash": "25e7f73e0e5c3806cd5d038b53136406f9ffc73439fd2f8c812d67106eba3a9c", "text": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. ", "start_char_idx": 33206, "end_char_idx": 33316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e78df41f-e7b1-4c05-b561-bfdb4dfc2c50": {"__data__": {"id_": "e78df41f-e7b1-4c05-b561-bfdb4dfc2c50", "embedding": null, "metadata": {"window": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n", "original_text": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f894422d-3968-47b3-91b2-e2cb16415f3b", "node_type": "1", "metadata": {"window": "However, as depicted in Figure 7(b), this guidance strategy tends to generate unrealistic content ( i.e.,\na bird with one leg missing).  This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. ", "original_text": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space. "}, "hash": "25e7f73e0e5c3806cd5d038b53136406f9ffc73439fd2f8c812d67106eba3a9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "058156a7-ba76-4939-8605-49d9089f2432", "node_type": "1", "metadata": {"window": "The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n", "original_text": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. "}, "hash": "d9a447f174c56bd7152ec908a55a0113d3a96e7de61b92e38b8b217beb20a7b0", "class_name": "RelatedNodeInfo"}}, "hash": "4c6d0b2fa03c2b5975cde39d038779b2a469d4fce31a2afc34c2db9f5bedeaf6", "text": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n", "start_char_idx": 33316, "end_char_idx": 33461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "058156a7-ba76-4939-8605-49d9089f2432": {"__data__": {"id_": "058156a7-ba76-4939-8605-49d9089f2432", "embedding": null, "metadata": {"window": "The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n", "original_text": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e78df41f-e7b1-4c05-b561-bfdb4dfc2c50", "node_type": "1", "metadata": {"window": "This demonstrates that the widely used guidance in image space may\nnot effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable\nfor this image reconstruction task.\n The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n", "original_text": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n"}, "hash": "4c6d0b2fa03c2b5975cde39d038779b2a469d4fce31a2afc34c2db9f5bedeaf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57f06cc8-c3b3-4108-8fdb-fef473001cb0", "node_type": "1", "metadata": {"window": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module. ", "original_text": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n"}, "hash": "52d576d59637edd1986727b76988e1596362562f28a20b27a59a2c7c468b367a", "class_name": "RelatedNodeInfo"}}, "hash": "d9a447f174c56bd7152ec908a55a0113d3a96e7de61b92e38b8b217beb20a7b0", "text": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. ", "start_char_idx": 33461, "end_char_idx": 33621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57f06cc8-c3b3-4108-8fdb-fef473001cb0": {"__data__": {"id_": "57f06cc8-c3b3-4108-8fdb-fef473001cb0", "embedding": null, "metadata": {"window": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module. ", "original_text": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058156a7-ba76-4939-8605-49d9089f2432", "node_type": "1", "metadata": {"window": "The Effectiveness of LAControlNet.  Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n", "original_text": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training. "}, "hash": "d9a447f174c56bd7152ec908a55a0113d3a96e7de61b92e38b8b217beb20a7b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a90fac1-9e61-4a8e-9638-e3d03abaa1b6", "node_type": "1", "metadata": {"window": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. ", "original_text": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n"}, "hash": "a72fc1747e2182af2b950ff621e1520e3249c3e7e2757c6b9e1f6f87a64a6c85", "class_name": "RelatedNodeInfo"}}, "hash": "52d576d59637edd1986727b76988e1596362562f28a20b27a59a2c7c468b367a", "text": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n", "start_char_idx": 33621, "end_char_idx": 33773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a90fac1-9e61-4a8e-9638-e3d03abaa1b6": {"__data__": {"id_": "2a90fac1-9e61-4a8e-9638-e3d03abaa1b6", "embedding": null, "metadata": {"window": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. ", "original_text": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57f06cc8-c3b3-4108-8fdb-fef473001cb0", "node_type": "1", "metadata": {"window": "Then we aim to emphasize the effectiveness of our proposed\nLAControlNet that encodes Iregto the latent space.  Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module. ", "original_text": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n"}, "hash": "52d576d59637edd1986727b76988e1596362562f28a20b27a59a2c7c468b367a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7fa000e-ae3f-4d09-96a6-e19138952711", "node_type": "1", "metadata": {"window": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8. ", "original_text": "The Flexibility of Controllable Module. "}, "hash": "234f3086c6d378f68d6af5064e0336f793038ccf8eb8ee2d698926f56a9bfbf7", "class_name": "RelatedNodeInfo"}}, "hash": "a72fc1747e2182af2b950ff621e1520e3249c3e7e2757c6b9e1f6f87a64a6c85", "text": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n", "start_char_idx": 33773, "end_char_idx": 33874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7fa000e-ae3f-4d09-96a6-e19138952711": {"__data__": {"id_": "e7fa000e-ae3f-4d09-96a6-e19138952711", "embedding": null, "metadata": {"window": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8. ", "original_text": "The Flexibility of Controllable Module. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a90fac1-9e61-4a8e-9638-e3d03abaa1b6", "node_type": "1", "metadata": {"window": "Here we compare with ControlNet [ 66], which\nadopts an additional condition network trained from scratch for conditioning the input information.\n As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. ", "original_text": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n"}, "hash": "a72fc1747e2182af2b950ff621e1520e3249c3e7e2757c6b9e1f6f87a64a6c85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63bf8bc5-309e-4e5e-97e5-cb4698f3f0b4", "node_type": "1", "metadata": {"window": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. ", "original_text": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. "}, "hash": "8cf4b86199ce3416efea9f9128997bbfe74fe5e09ef0702352766cfaca696fac", "class_name": "RelatedNodeInfo"}}, "hash": "234f3086c6d378f68d6af5064e0336f793038ccf8eb8ee2d698926f56a9bfbf7", "text": "The Flexibility of Controllable Module. ", "start_char_idx": 33874, "end_char_idx": 33914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63bf8bc5-309e-4e5e-97e5-cb4698f3f0b4": {"__data__": {"id_": "63bf8bc5-309e-4e5e-97e5-cb4698f3f0b4", "embedding": null, "metadata": {"window": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. ", "original_text": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7fa000e-ae3f-4d09-96a6-e19138952711", "node_type": "1", "metadata": {"window": "As shown in Figure 7(c), ControlNet tends to output results with color shifts, as there is no explicit\n10\n\nregularization on color consistency during training.  One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8. ", "original_text": "The Flexibility of Controllable Module. "}, "hash": "234f3086c6d378f68d6af5064e0336f793038ccf8eb8ee2d698926f56a9bfbf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbeefa32-1443-4b83-959e-12764f6a48a7", "node_type": "1", "metadata": {"window": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. ", "original_text": "The visualization result is shown in Figure 8. "}, "hash": "a8d3d5acfb279edbd865d0857502288f6a132253c53eb45c9031009e7af1587c", "class_name": "RelatedNodeInfo"}}, "hash": "8cf4b86199ce3416efea9f9128997bbfe74fe5e09ef0702352766cfaca696fac", "text": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. ", "start_char_idx": 33914, "end_char_idx": 34093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbeefa32-1443-4b83-959e-12764f6a48a7": {"__data__": {"id_": "bbeefa32-1443-4b83-959e-12764f6a48a7", "embedding": null, "metadata": {"window": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. ", "original_text": "The visualization result is shown in Figure 8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63bf8bc5-309e-4e5e-97e5-cb4698f3f0b4", "node_type": "1", "metadata": {"window": "One might use non-uniform sampling to increase\nthe probability of optimization in the early sampling stage and achieves better color controlling [ 42].\n Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. ", "original_text": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences. "}, "hash": "8cf4b86199ce3416efea9f9128997bbfe74fe5e09ef0702352766cfaca696fac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5539238-3d99-4cd0-8356-000c20717f18", "node_type": "1", "metadata": {"window": "The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n", "original_text": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. "}, "hash": "ee849f190e8b0c38a0c9d943c28423d51c30172a99331626ce58f0b6197c932d", "class_name": "RelatedNodeInfo"}}, "hash": "a8d3d5acfb279edbd865d0857502288f6a132253c53eb45c9031009e7af1587c", "text": "The visualization result is shown in Figure 8. ", "start_char_idx": 34093, "end_char_idx": 34140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5539238-3d99-4cd0-8356-000c20717f18": {"__data__": {"id_": "c5539238-3d99-4cd0-8356-000c20717f18", "embedding": null, "metadata": {"window": "The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n", "original_text": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbeefa32-1443-4b83-959e-12764f6a48a7", "node_type": "1", "metadata": {"window": "Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.\n The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. ", "original_text": "The visualization result is shown in Figure 8. "}, "hash": "a8d3d5acfb279edbd865d0857502288f6a132253c53eb45c9031009e7af1587c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b01e6783-bcdd-4cb1-95ab-d81319d2e058", "node_type": "1", "metadata": {"window": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. ", "original_text": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. "}, "hash": "715ab7170f6a8cb5c5c6b3391411b919abebbcddaac3ace7a6d0c087bbecb44f", "class_name": "RelatedNodeInfo"}}, "hash": "ee849f190e8b0c38a0c9d943c28423d51c30172a99331626ce58f0b6197c932d", "text": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. ", "start_char_idx": 34140, "end_char_idx": 34265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b01e6783-bcdd-4cb1-95ab-d81319d2e058": {"__data__": {"id_": "b01e6783-bcdd-4cb1-95ab-d81319d2e058", "embedding": null, "metadata": {"window": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. ", "original_text": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5539238-3d99-4cd0-8356-000c20717f18", "node_type": "1", "metadata": {"window": "The Flexibility of Controllable Module.  Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n", "original_text": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg. "}, "hash": "ee849f190e8b0c38a0c9d943c28423d51c30172a99331626ce58f0b6197c932d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3ecce72-200e-4c51-bbb3-72403063afea", "node_type": "1", "metadata": {"window": "The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg.", "original_text": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n"}, "hash": "bd22df58152c8bf65f2948eb14715d85a5e7cbc2d492f89b4139905e44f8739b", "class_name": "RelatedNodeInfo"}}, "hash": "715ab7170f6a8cb5c5c6b3391411b919abebbcddaac3ace7a6d0c087bbecb44f", "text": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. ", "start_char_idx": 34265, "end_char_idx": 34430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3ecce72-200e-4c51-bbb3-72403063afea": {"__data__": {"id_": "c3ecce72-200e-4c51-bbb3-72403063afea", "embedding": null, "metadata": {"window": "The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg.", "original_text": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b01e6783-bcdd-4cb1-95ab-d81319d2e058", "node_type": "1", "metadata": {"window": "Considering that generative restoration models may\nproduce unexpected details, here we provide a controllable module for users to explore according to\ntheir personal preferences.  The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. ", "original_text": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result. "}, "hash": "715ab7170f6a8cb5c5c6b3391411b919abebbcddaac3ace7a6d0c087bbecb44f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17bd6cbc-9deb-4f49-b774-c62157218fe3", "node_type": "1", "metadata": {"window": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. ", "original_text": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. "}, "hash": "a1d0c0ff7debcd74b9482abf135aa547f90f9f736e8c9ed2cdf24c6004da853d", "class_name": "RelatedNodeInfo"}}, "hash": "bd22df58152c8bf65f2948eb14715d85a5e7cbc2d492f89b4139905e44f8739b", "text": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n", "start_char_idx": 34430, "end_char_idx": 34528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17bd6cbc-9deb-4f49-b774-c62157218fe3": {"__data__": {"id_": "17bd6cbc-9deb-4f49-b774-c62157218fe3", "embedding": null, "metadata": {"window": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. ", "original_text": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3ecce72-200e-4c51-bbb3-72403063afea", "node_type": "1", "metadata": {"window": "The visualization result is shown in Figure 8.  Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg.", "original_text": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n"}, "hash": "bd22df58152c8bf65f2948eb14715d85a5e7cbc2d492f89b4139905e44f8739b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf7530db-91fa-4d03-ab84-d2127ebdd1c8", "node_type": "1", "metadata": {"window": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n", "original_text": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg."}, "hash": "7400563ede7f412689efed1c13e4f435902f52419f4e7ccc40ec9b0d6e748d6a", "class_name": "RelatedNodeInfo"}}, "hash": "a1d0c0ff7debcd74b9482abf135aa547f90f9f736e8c9ed2cdf24c6004da853d", "text": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. ", "start_char_idx": 34528, "end_char_idx": 34625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf7530db-91fa-4d03-ab84-d2127ebdd1c8": {"__data__": {"id_": "bf7530db-91fa-4d03-ab84-d2127ebdd1c8", "embedding": null, "metadata": {"window": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n", "original_text": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17bd6cbc-9deb-4f49-b774-c62157218fe3", "node_type": "1", "metadata": {"window": "Our experiments suggest\nthat a larger gradient scale stends to produce a high-fidelity smooth result which is close to Ireg.  As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. ", "original_text": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity. "}, "hash": "a1d0c0ff7debcd74b9482abf135aa547f90f9f736e8c9ed2cdf24c6004da853d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4691b7b8-6508-4d11-97a9-e132e3bc0359", "node_type": "1", "metadata": {"window": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. ", "original_text": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. "}, "hash": "e7128932285006fd6fc8f114455e66362dd5257c41384b559f290c633a6d9bef", "class_name": "RelatedNodeInfo"}}, "hash": "7400563ede7f412689efed1c13e4f435902f52419f4e7ccc40ec9b0d6e748d6a", "text": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg.", "start_char_idx": 34625, "end_char_idx": 34722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4691b7b8-6508-4d11-97a9-e132e3bc0359": {"__data__": {"id_": "4691b7b8-6508-4d11-97a9-e132e3bc0359", "embedding": null, "metadata": {"window": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. ", "original_text": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf7530db-91fa-4d03-ab84-d2127ebdd1c8", "node_type": "1", "metadata": {"window": "As\nseen from the first row, DiffBIR\u2019s output Idiff has some blue artifacts in the dog\u2019s eyes, thus we set s\nto 200 and higher as well for obtaining a better result.  Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n", "original_text": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg."}, "hash": "7400563ede7f412689efed1c13e4f435902f52419f4e7ccc40ec9b0d6e748d6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f615dd0e-f8e5-4440-8a6e-f0384f4f52d0", "node_type": "1", "metadata": {"window": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. ", "original_text": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n"}, "hash": "cda892b79fa1a0c02e26fe318d22ac1f281c596cf8511fac2b6b0b34d4a8cca5", "class_name": "RelatedNodeInfo"}}, "hash": "e7128932285006fd6fc8f114455e66362dd5257c41384b559f290c633a6d9bef", "text": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. ", "start_char_idx": 34722, "end_char_idx": 34968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f615dd0e-f8e5-4440-8a6e-f0384f4f52d0": {"__data__": {"id_": "f615dd0e-f8e5-4440-8a6e-f0384f4f52d0", "embedding": null, "metadata": {"window": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. ", "original_text": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4691b7b8-6508-4d11-97a9-e132e3bc0359", "node_type": "1", "metadata": {"window": "Moreover, the background is also changing\n(tends to be more blurry) as the gradient scale sgrows.\n Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. ", "original_text": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. "}, "hash": "e7128932285006fd6fc8f114455e66362dd5257c41384b559f290c633a6d9bef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0511131a-c1cc-492d-9989-a243d5878fdf", "node_type": "1", "metadata": {"window": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. ", "original_text": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. "}, "hash": "2c9e92bb417ce644f4e9540ce0c8601e21630c3c900c16f62949199fab4029d0", "class_name": "RelatedNodeInfo"}}, "hash": "cda892b79fa1a0c02e26fe318d22ac1f281c596cf8511fac2b6b0b34d4a8cca5", "text": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n", "start_char_idx": 34968, "end_char_idx": 35075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0511131a-c1cc-492d-9989-a243d5878fdf": {"__data__": {"id_": "0511131a-c1cc-492d-9989-a243d5878fdf", "embedding": null, "metadata": {"window": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. ", "original_text": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f615dd0e-f8e5-4440-8a6e-f0384f4f52d0", "node_type": "1", "metadata": {"window": "Figure 8: Our latent image guidance is able to achieve a trade-off between quality and fidelity.  The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. ", "original_text": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n"}, "hash": "cda892b79fa1a0c02e26fe318d22ac1f281c596cf8511fac2b6b0b34d4a8cca5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1499dfec-f332-4037-8fa4-78e5fd12c2a7", "node_type": "1", "metadata": {"window": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n", "original_text": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. "}, "hash": "1cd0635c7ff15ea72bac0462ebe1128f7d5a1962b9e67a9e409861aa2d916068", "class_name": "RelatedNodeInfo"}}, "hash": "2c9e92bb417ce644f4e9540ce0c8601e21630c3c900c16f62949199fab4029d0", "text": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. ", "start_char_idx": 35075, "end_char_idx": 35206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1499dfec-f332-4037-8fa4-78e5fd12c2a7": {"__data__": {"id_": "1499dfec-f332-4037-8fa4-78e5fd12c2a7", "embedding": null, "metadata": {"window": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n", "original_text": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0511131a-c1cc-492d-9989-a243d5878fdf", "node_type": "1", "metadata": {"window": "The gradient\nscale can be tuned to obtain transition effects between sharp Idiff and smooth Ireg. (Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. ", "original_text": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks. "}, "hash": "2c9e92bb417ce644f4e9540ce0c8601e21630c3c900c16f62949199fab4029d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8746bcd0-ba02-4e73-9775-453b211a040a", "node_type": "1", "metadata": {"window": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. ", "original_text": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. "}, "hash": "3ca130eee86c160d1910e5dc3b2917f144c6618c6d8632599164815682a3a441", "class_name": "RelatedNodeInfo"}}, "hash": "1cd0635c7ff15ea72bac0462ebe1128f7d5a1962b9e67a9e409861aa2d916068", "text": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. ", "start_char_idx": 35206, "end_char_idx": 35329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8746bcd0-ba02-4e73-9775-453b211a040a": {"__data__": {"id_": "8746bcd0-ba02-4e73-9775-453b211a040a", "embedding": null, "metadata": {"window": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. ", "original_text": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1499dfec-f332-4037-8fa4-78e5fd12c2a7", "node_type": "1", "metadata": {"window": "(Zoom in for best view )\n5 Conclusion and Limitations\nWe propose a unified framework for blind image restoration, named DiffBIR, which could achieve\nrealistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion.  It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n", "original_text": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored. "}, "hash": "1cd0635c7ff15ea72bac0462ebe1128f7d5a1962b9e67a9e409861aa2d916068", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a8fb800-b1db-4b6c-9952-0ddcd1a246e0", "node_type": "1", "metadata": {"window": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages. ", "original_text": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n"}, "hash": "792b791001e8869d4798409419c7955f861f7aefec920fbd6a4f4363c736ef5c", "class_name": "RelatedNodeInfo"}}, "hash": "3ca130eee86c160d1910e5dc3b2917f144c6618c6d8632599164815682a3a441", "text": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. ", "start_char_idx": 35329, "end_char_idx": 35412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a8fb800-b1db-4b6c-9952-0ddcd1a246e0": {"__data__": {"id_": "0a8fb800-b1db-4b6c-9952-0ddcd1a246e0", "embedding": null, "metadata": {"window": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages. ", "original_text": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8746bcd0-ba02-4e73-9775-453b211a040a", "node_type": "1", "metadata": {"window": "It\nconsists of two stages: the restoration and generation stage, which ensures both fidelity and realness.\n Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. ", "original_text": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged. "}, "hash": "3ca130eee86c160d1910e5dc3b2917f144c6618c6d8632599164815682a3a441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "beacdb7c-03e6-4f71-9462-8d7e86c9cdd4", "node_type": "1", "metadata": {"window": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n", "original_text": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. "}, "hash": "656d52da5b2549de986c8ffe2ffa1f642cca69f5d88456e3122b8f9eb5d97469", "class_name": "RelatedNodeInfo"}}, "hash": "792b791001e8869d4798409419c7955f861f7aefec920fbd6a4f4363c736ef5c", "text": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n", "start_char_idx": 35412, "end_char_idx": 35638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "beacdb7c-03e6-4f71-9462-8d7e86c9cdd4": {"__data__": {"id_": "beacdb7c-03e6-4f71-9462-8d7e86c9cdd4", "embedding": null, "metadata": {"window": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n", "original_text": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a8fb800-b1db-4b6c-9952-0ddcd1a246e0", "node_type": "1", "metadata": {"window": "Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods\nfor both BSR and BFR tasks.  Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages. ", "original_text": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n"}, "hash": "792b791001e8869d4798409419c7955f861f7aefec920fbd6a4f4363c736ef5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43c44ed8-29ca-44e8-ac59-be54635cfe89", "node_type": "1", "metadata": {"window": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. ", "original_text": "Blended diffusion for text-driven editing of natural\nimages. "}, "hash": "02ab0ba34cf748fa9d9f94ecbfe5fd831274f664fb446c791ae37e0f3addce51", "class_name": "RelatedNodeInfo"}}, "hash": "656d52da5b2549de986c8ffe2ffa1f642cca69f5d88456e3122b8f9eb5d97469", "text": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. ", "start_char_idx": 35638, "end_char_idx": 35700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43c44ed8-29ca-44e8-ac59-be54635cfe89": {"__data__": {"id_": "43c44ed8-29ca-44e8-ac59-be54635cfe89", "embedding": null, "metadata": {"window": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. ", "original_text": "Blended diffusion for text-driven editing of natural\nimages. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "beacdb7c-03e6-4f71-9462-8d7e86c9cdd4", "node_type": "1", "metadata": {"window": "Although our proposed DiffBIR has shown promising results, the\npotential of text-driven image restoration is not explored.  Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n", "original_text": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried. "}, "hash": "656d52da5b2549de986c8ffe2ffa1f642cca69f5d88456e3122b8f9eb5d97469", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2520686-96e4-4af7-b4db-e38b9a52308e", "node_type": "1", "metadata": {"window": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n"}, "hash": "dabaa13b9f99255db5f336b28ac42b019dcbe74bbfde79e5f614785a5f70ccee", "class_name": "RelatedNodeInfo"}}, "hash": "02ab0ba34cf748fa9d9f94ecbfe5fd831274f664fb446c791ae37e0f3addce51", "text": "Blended diffusion for text-driven editing of natural\nimages. ", "start_char_idx": 35700, "end_char_idx": 35761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2520686-96e4-4af7-b4db-e38b9a52308e": {"__data__": {"id_": "a2520686-96e4-4af7-b4db-e38b9a52308e", "embedding": null, "metadata": {"window": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43c44ed8-29ca-44e8-ac59-be54635cfe89", "node_type": "1", "metadata": {"window": "Further exploitation in Stable Diffusion for\nimage restoration task is encouraged.  On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. ", "original_text": "Blended diffusion for text-driven editing of natural\nimages. "}, "hash": "02ab0ba34cf748fa9d9f94ecbfe5fd831274f664fb446c791ae37e0f3addce51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c87ffe8-6fb5-469c-9add-7192476da0d3", "node_type": "1", "metadata": {"window": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546. ", "original_text": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. "}, "hash": "2cd3808500a00bf286009c3bb7528ea321c53a4f402ca62749790f8a034ea721", "class_name": "RelatedNodeInfo"}}, "hash": "dabaa13b9f99255db5f336b28ac42b019dcbe74bbfde79e5f614785a5f70ccee", "text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n", "start_char_idx": 35761, "end_char_idx": 35873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c87ffe8-6fb5-469c-9add-7192476da0d3": {"__data__": {"id_": "4c87ffe8-6fb5-469c-9add-7192476da0d3", "embedding": null, "metadata": {"window": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546. ", "original_text": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2520686-96e4-4af7-b4db-e38b9a52308e", "node_type": "1", "metadata": {"window": "On the other hand, our DiffBIR method requires 50 sampling\nsteps to restore a low-quality image, resulting in much higher computational resource consumption\nand more inference time compared to other image restoration methods.\n References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n"}, "hash": "dabaa13b9f99255db5f336b28ac42b019dcbe74bbfde79e5f614785a5f70ccee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "067a3710-de15-438a-9556-5429e720fc7a", "node_type": "1", "metadata": {"window": "Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n", "original_text": "Compressed sensing using generative\nmodels. "}, "hash": "b876e82814cc481fe45f1d2a946020f4d6c1cdca784a60888a41bc78d8b86f8f", "class_name": "RelatedNodeInfo"}}, "hash": "2cd3808500a00bf286009c3bb7528ea321c53a4f402ca62749790f8a034ea721", "text": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. ", "start_char_idx": 35873, "end_char_idx": 35939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "067a3710-de15-438a-9556-5429e720fc7a": {"__data__": {"id_": "067a3710-de15-438a-9556-5429e720fc7a", "embedding": null, "metadata": {"window": "Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n", "original_text": "Compressed sensing using generative\nmodels. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c87ffe8-6fb5-469c-9add-7192476da0d3", "node_type": "1", "metadata": {"window": "References\n[1]Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546. ", "original_text": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. "}, "hash": "2cd3808500a00bf286009c3bb7528ea321c53a4f402ca62749790f8a034ea721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "171df040-b833-491e-bb1b-547e6c62a3ef", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. ", "original_text": "In International Conference on Machine Learning , pages 537\u2013546. "}, "hash": "d731158785815d2a27bb4368f71107c50510cd8b06698c5744af33b7c8cd9fc0", "class_name": "RelatedNodeInfo"}}, "hash": "b876e82814cc481fe45f1d2a946020f4d6c1cdca784a60888a41bc78d8b86f8f", "text": "Compressed sensing using generative\nmodels. ", "start_char_idx": 35939, "end_char_idx": 35983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "171df040-b833-491e-bb1b-547e6c62a3ef": {"__data__": {"id_": "171df040-b833-491e-bb1b-547e6c62a3ef", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. ", "original_text": "In International Conference on Machine Learning , pages 537\u2013546. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "067a3710-de15-438a-9556-5429e720fc7a", "node_type": "1", "metadata": {"window": "Blended diffusion for text-driven editing of natural\nimages.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n", "original_text": "Compressed sensing using generative\nmodels. "}, "hash": "b876e82814cc481fe45f1d2a946020f4d6c1cdca784a60888a41bc78d8b86f8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba348eff-24eb-4092-8f3e-8d1be3f45724", "node_type": "1", "metadata": {"window": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model. ", "original_text": "PMLR, 2017.\n"}, "hash": "67890edd2596397a2652640fc023266637534178ff3dbf8c8b9603ea126b0586", "class_name": "RelatedNodeInfo"}}, "hash": "d731158785815d2a27bb4368f71107c50510cd8b06698c5744af33b7c8cd9fc0", "text": "In International Conference on Machine Learning , pages 537\u2013546. ", "start_char_idx": 35983, "end_char_idx": 36048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba348eff-24eb-4092-8f3e-8d1be3f45724": {"__data__": {"id_": "ba348eff-24eb-4092-8f3e-8d1be3f45724", "embedding": null, "metadata": {"window": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model. ", "original_text": "PMLR, 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "171df040-b833-491e-bb1b-547e6c62a3ef", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n18208\u201318218, 2022.\n [2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. ", "original_text": "In International Conference on Machine Learning , pages 537\u2013546. "}, "hash": "d731158785815d2a27bb4368f71107c50510cd8b06698c5744af33b7c8cd9fc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b97be60-e83e-402f-9995-b3778d387316", "node_type": "1", "metadata": {"window": "Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n", "original_text": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. "}, "hash": "de76b10f33c8c1aa049ddef4a4d33bc47f8cd22e4f545989d0242bf100a0e511", "class_name": "RelatedNodeInfo"}}, "hash": "67890edd2596397a2652640fc023266637534178ff3dbf8c8b9603ea126b0586", "text": "PMLR, 2017.\n", "start_char_idx": 36048, "end_char_idx": 36060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b97be60-e83e-402f-9995-b3778d387316": {"__data__": {"id_": "0b97be60-e83e-402f-9995-b3778d387316", "embedding": null, "metadata": {"window": "Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n", "original_text": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba348eff-24eb-4092-8f3e-8d1be3f45724", "node_type": "1", "metadata": {"window": "[2]Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis.  Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model. ", "original_text": "PMLR, 2017.\n"}, "hash": "67890edd2596397a2652640fc023266637534178ff3dbf8c8b9603ea126b0586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0a51584-18b0-4486-8860-71e75ba39ca5", "node_type": "1", "metadata": {"window": "In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. ", "original_text": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model. "}, "hash": "5041385b0620a29e1d936ae63267ab0bb01ad4f0e4366e290468a5cb0f63396d", "class_name": "RelatedNodeInfo"}}, "hash": "de76b10f33c8c1aa049ddef4a4d33bc47f8cd22e4f545989d0242bf100a0e511", "text": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. ", "start_char_idx": 36060, "end_char_idx": 36132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0a51584-18b0-4486-8860-71e75ba39ca5": {"__data__": {"id_": "f0a51584-18b0-4486-8860-71e75ba39ca5", "embedding": null, "metadata": {"window": "In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. ", "original_text": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b97be60-e83e-402f-9995-b3778d387316", "node_type": "1", "metadata": {"window": "Compressed sensing using generative\nmodels.  In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n", "original_text": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. "}, "hash": "de76b10f33c8c1aa049ddef4a4d33bc47f8cd22e4f545989d0242bf100a0e511", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df40838d-7f0f-43ea-86f7-57120e7e3000", "node_type": "1", "metadata": {"window": "PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution. ", "original_text": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n"}, "hash": "d13e98ac7f8cf2b74d5b394e84805e9e767b141df06ea29046582dafbd7d2e43", "class_name": "RelatedNodeInfo"}}, "hash": "5041385b0620a29e1d936ae63267ab0bb01ad4f0e4366e290468a5cb0f63396d", "text": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model. ", "start_char_idx": 36132, "end_char_idx": 36214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df40838d-7f0f-43ea-86f7-57120e7e3000": {"__data__": {"id_": "df40838d-7f0f-43ea-86f7-57120e7e3000", "embedding": null, "metadata": {"window": "PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution. ", "original_text": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0a51584-18b0-4486-8860-71e75ba39ca5", "node_type": "1", "metadata": {"window": "In International Conference on Machine Learning , pages 537\u2013546.  PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. ", "original_text": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model. "}, "hash": "5041385b0620a29e1d936ae63267ab0bb01ad4f0e4366e290468a5cb0f63396d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "542995bd-ee2f-447e-b444-68c20e2f84f0", "node_type": "1", "metadata": {"window": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n", "original_text": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. "}, "hash": "aea3e2dc78f45f37f9272a836de1e0635b8e0f9b0aec9c6bbe577f048538f300", "class_name": "RelatedNodeInfo"}}, "hash": "d13e98ac7f8cf2b74d5b394e84805e9e767b141df06ea29046582dafbd7d2e43", "text": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n", "start_char_idx": 36214, "end_char_idx": 36314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "542995bd-ee2f-447e-b444-68c20e2f84f0": {"__data__": {"id_": "542995bd-ee2f-447e-b444-68c20e2f84f0", "embedding": null, "metadata": {"window": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n", "original_text": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df40838d-7f0f-43ea-86f7-57120e7e3000", "node_type": "1", "metadata": {"window": "PMLR, 2017.\n 11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution. ", "original_text": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n"}, "hash": "d13e98ac7f8cf2b74d5b394e84805e9e767b141df06ea29046582dafbd7d2e43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f78bf349-1623-4609-8018-5d562a67df47", "node_type": "1", "metadata": {"window": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. ", "original_text": "Glean: Generative latent\nbank for large-factor image super-resolution. "}, "hash": "b855b6bc09a88bcdf1fce84bc6f2b700c99decd8300789769c7fe038e8973b8c", "class_name": "RelatedNodeInfo"}}, "hash": "aea3e2dc78f45f37f9272a836de1e0635b8e0f9b0aec9c6bbe577f048538f300", "text": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. ", "start_char_idx": 36314, "end_char_idx": 36390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f78bf349-1623-4609-8018-5d562a67df47": {"__data__": {"id_": "f78bf349-1623-4609-8018-5d562a67df47", "embedding": null, "metadata": {"window": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. ", "original_text": "Glean: Generative latent\nbank for large-factor image super-resolution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "542995bd-ee2f-447e-b444-68c20e2f84f0", "node_type": "1", "metadata": {"window": "11\n\n[3]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.  Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n", "original_text": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. "}, "hash": "aea3e2dc78f45f37f9272a836de1e0635b8e0f9b0aec9c6bbe577f048538f300", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f02815e-b849-476c-8aad-e3f34f4a23bb", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n"}, "hash": "f39a6f6459d1ec20314d7ec367d2a5eafd24bea0bd2f7841bf7c815410b07c1b", "class_name": "RelatedNodeInfo"}}, "hash": "b855b6bc09a88bcdf1fce84bc6f2b700c99decd8300789769c7fe038e8973b8c", "text": "Glean: Generative latent\nbank for large-factor image super-resolution. ", "start_char_idx": 36390, "end_char_idx": 36461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f02815e-b849-476c-8aad-e3f34f4a23bb": {"__data__": {"id_": "7f02815e-b849-476c-8aad-e3f34f4a23bb", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f78bf349-1623-4609-8018-5d562a67df47", "node_type": "1", "metadata": {"window": "Toward real-world single image\nsuper-resolution: A new benchmark and a new model.  In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. ", "original_text": "Glean: Generative latent\nbank for large-factor image super-resolution. "}, "hash": "b855b6bc09a88bcdf1fce84bc6f2b700c99decd8300789769c7fe038e8973b8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4780c862-03a1-4ba5-8121-01b3c23731a1", "node_type": "1", "metadata": {"window": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n", "original_text": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. "}, "hash": "e76c63cad29bee4e9c1c9be77c995266f16308d6d91755b4f3cab8dd001e0019", "class_name": "RelatedNodeInfo"}}, "hash": "f39a6f6459d1ec20314d7ec367d2a5eafd24bea0bd2f7841bf7c815410b07c1b", "text": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n", "start_char_idx": 36461, "end_char_idx": 36573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4780c862-03a1-4ba5-8121-01b3c23731a1": {"__data__": {"id_": "4780c862-03a1-4ba5-8121-01b3c23731a1", "embedding": null, "metadata": {"window": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n", "original_text": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f02815e-b849-476c-8aad-e3f34f4a23bb", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3086\u20133095, 2019.\n [4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n"}, "hash": "f39a6f6459d1ec20314d7ec367d2a5eafd24bea0bd2f7841bf7c815410b07c1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86e8c524-ca71-4d9e-b471-d14965af5916", "node_type": "1", "metadata": {"window": "Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n", "original_text": "Progressive\nsemantic-aware style transformation for blind face restoration. "}, "hash": "90750e887afc3085e7622891b51727f0bf316e3cad88e8e9e628ac1416781725", "class_name": "RelatedNodeInfo"}}, "hash": "e76c63cad29bee4e9c1c9be77c995266f16308d6d91755b4f3cab8dd001e0019", "text": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. ", "start_char_idx": 36573, "end_char_idx": 36662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86e8c524-ca71-4d9e-b471-d14965af5916": {"__data__": {"id_": "86e8c524-ca71-4d9e-b471-d14965af5916", "embedding": null, "metadata": {"window": "Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n", "original_text": "Progressive\nsemantic-aware style transformation for blind face restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4780c862-03a1-4ba5-8121-01b3c23731a1", "node_type": "1", "metadata": {"window": "[4]Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.  Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n", "original_text": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong. "}, "hash": "e76c63cad29bee4e9c1c9be77c995266f16308d6d91755b4f3cab8dd001e0019", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfbb1950-5a50-46db-8ff7-aaec2905ea27", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors. ", "original_text": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n"}, "hash": "b121ea0ba2fbab09848550dfbe4f5868889c0f8a89e67431831efa49f62852a2", "class_name": "RelatedNodeInfo"}}, "hash": "90750e887afc3085e7622891b51727f0bf316e3cad88e8e9e628ac1416781725", "text": "Progressive\nsemantic-aware style transformation for blind face restoration. ", "start_char_idx": 36662, "end_char_idx": 36738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfbb1950-5a50-46db-8ff7-aaec2905ea27": {"__data__": {"id_": "cfbb1950-5a50-46db-8ff7-aaec2905ea27", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors. ", "original_text": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86e8c524-ca71-4d9e-b471-d14965af5916", "node_type": "1", "metadata": {"window": "Glean: Generative latent\nbank for large-factor image super-resolution.  In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n", "original_text": "Progressive\nsemantic-aware style transformation for blind face restoration. "}, "hash": "90750e887afc3085e7622891b51727f0bf316e3cad88e8e9e628ac1416781725", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f0c6007-01fc-4a1f-a45a-0530a6948c5d", "node_type": "1", "metadata": {"window": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n", "original_text": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n"}, "hash": "05e6db237f50cbfb08c6b9a93955847e4dd01f9164ceec0aaec9712aa3d78074", "class_name": "RelatedNodeInfo"}}, "hash": "b121ea0ba2fbab09848550dfbe4f5868889c0f8a89e67431831efa49f62852a2", "text": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n", "start_char_idx": 36738, "end_char_idx": 36850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f0c6007-01fc-4a1f-a45a-0530a6948c5d": {"__data__": {"id_": "9f0c6007-01fc-4a1f-a45a-0530a6948c5d", "embedding": null, "metadata": {"window": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n", "original_text": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfbb1950-5a50-46db-8ff7-aaec2905ea27", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 14245\u201314254, 2021.\n [5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors. ", "original_text": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n"}, "hash": "b121ea0ba2fbab09848550dfbe4f5868889c0f8a89e67431831efa49f62852a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc6a7f0c-e602-4534-bae7-a17ac229b32a", "node_type": "1", "metadata": {"window": "Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. ", "original_text": "Real-world blind super-resolution via feature matching with implicit high-resolution priors. "}, "hash": "d44fc9cc77d3e5275147d871c592a67132b1d19d4d41421e9ee9ef0daeb0c447", "class_name": "RelatedNodeInfo"}}, "hash": "05e6db237f50cbfb08c6b9a93955847e4dd01f9164ceec0aaec9712aa3d78074", "text": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n", "start_char_idx": 36850, "end_char_idx": 36945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc6a7f0c-e602-4534-bae7-a17ac229b32a": {"__data__": {"id_": "bc6a7f0c-e602-4534-bae7-a17ac229b32a", "embedding": null, "metadata": {"window": "Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. ", "original_text": "Real-world blind super-resolution via feature matching with implicit high-resolution priors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f0c6007-01fc-4a1f-a45a-0530a6948c5d", "node_type": "1", "metadata": {"window": "[5]Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K Wong.  Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n", "original_text": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n"}, "hash": "05e6db237f50cbfb08c6b9a93955847e4dd01f9164ceec0aaec9712aa3d78074", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2003c2fe-2558-44f8-b4d1-e83e586c839a", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer. ", "original_text": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n"}, "hash": "018301978f6b9a30b63221ad5f46d99cdba8a0dbc63b21785b25eeb0bc840c5a", "class_name": "RelatedNodeInfo"}}, "hash": "d44fc9cc77d3e5275147d871c592a67132b1d19d4d41421e9ee9ef0daeb0c447", "text": "Real-world blind super-resolution via feature matching with implicit high-resolution priors. ", "start_char_idx": 36945, "end_char_idx": 37038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2003c2fe-2558-44f8-b4d1-e83e586c839a": {"__data__": {"id_": "2003c2fe-2558-44f8-b4d1-e83e586c839a", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer. ", "original_text": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc6a7f0c-e602-4534-bae7-a17ac229b32a", "node_type": "1", "metadata": {"window": "Progressive\nsemantic-aware style transformation for blind face restoration.  In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. ", "original_text": "Real-world blind super-resolution via feature matching with implicit high-resolution priors. "}, "hash": "d44fc9cc77d3e5275147d871c592a67132b1d19d4d41421e9ee9ef0daeb0c447", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39b7232a-0cdc-4f7d-9ce0-55c058fc8ef6", "node_type": "1", "metadata": {"window": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n", "original_text": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. "}, "hash": "3208cabbcd218c0a08237f9e85420af22528868e151f22cdb4e25dedc01d59aa", "class_name": "RelatedNodeInfo"}}, "hash": "018301978f6b9a30b63221ad5f46d99cdba8a0dbc63b21785b25eeb0bc840c5a", "text": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n", "start_char_idx": 37038, "end_char_idx": 37133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39b7232a-0cdc-4f7d-9ce0-55c058fc8ef6": {"__data__": {"id_": "39b7232a-0cdc-4f7d-9ce0-55c058fc8ef6", "embedding": null, "metadata": {"window": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n", "original_text": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2003c2fe-2558-44f8-b4d1-e83e586c839a", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 11896\u201311905, 2021.\n [6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer. ", "original_text": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n"}, "hash": "018301978f6b9a30b63221ad5f46d99cdba8a0dbc63b21785b25eeb0bc840c5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "481de39b-d027-41c3-b85b-eaadf3dd1940", "node_type": "1", "metadata": {"window": "Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. ", "original_text": "Pre-trained image processing transformer. "}, "hash": "e40d0e976bbb8a6b814c36f2c89df0f51ecc14be1fa7b9454c8249f23c2d6e66", "class_name": "RelatedNodeInfo"}}, "hash": "3208cabbcd218c0a08237f9e85420af22528868e151f22cdb4e25dedc01d59aa", "text": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. ", "start_char_idx": 37133, "end_char_idx": 37255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "481de39b-d027-41c3-b85b-eaadf3dd1940": {"__data__": {"id_": "481de39b-d027-41c3-b85b-eaadf3dd1940", "embedding": null, "metadata": {"window": "Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. ", "original_text": "Pre-trained image processing transformer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39b7232a-0cdc-4f7d-9ce0-55c058fc8ef6", "node_type": "1", "metadata": {"window": "[6]Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo.\n Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n", "original_text": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. "}, "hash": "3208cabbcd218c0a08237f9e85420af22528868e151f22cdb4e25dedc01d59aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ecd8886-1272-4eec-9501-fb62bd91ef9d", "node_type": "1", "metadata": {"window": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer. ", "original_text": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n"}, "hash": "674022e6282d0aad4ed6c848f9d3cfd854df90bab0efcfa32ced569b07f3eb82", "class_name": "RelatedNodeInfo"}}, "hash": "e40d0e976bbb8a6b814c36f2c89df0f51ecc14be1fa7b9454c8249f23c2d6e66", "text": "Pre-trained image processing transformer. ", "start_char_idx": 37255, "end_char_idx": 37297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ecd8886-1272-4eec-9501-fb62bd91ef9d": {"__data__": {"id_": "3ecd8886-1272-4eec-9501-fb62bd91ef9d", "embedding": null, "metadata": {"window": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer. ", "original_text": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "481de39b-d027-41c3-b85b-eaadf3dd1940", "node_type": "1", "metadata": {"window": "Real-world blind super-resolution via feature matching with implicit high-resolution priors.  In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. ", "original_text": "Pre-trained image processing transformer. "}, "hash": "e40d0e976bbb8a6b814c36f2c89df0f51ecc14be1fa7b9454c8249f23c2d6e66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab8c725a-40f4-425a-bdaa-db1e62da05d5", "node_type": "1", "metadata": {"window": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n", "original_text": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. "}, "hash": "7b89d87e994ab0f58bb09e0942b0fcd4a7f22bac6ce9eeffd2793a347dedff61", "class_name": "RelatedNodeInfo"}}, "hash": "674022e6282d0aad4ed6c848f9d3cfd854df90bab0efcfa32ced569b07f3eb82", "text": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n", "start_char_idx": 37297, "end_char_idx": 37409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab8c725a-40f4-425a-bdaa-db1e62da05d5": {"__data__": {"id_": "ab8c725a-40f4-425a-bdaa-db1e62da05d5", "embedding": null, "metadata": {"window": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n", "original_text": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ecd8886-1272-4eec-9501-fb62bd91ef9d", "node_type": "1", "metadata": {"window": "In Proceedings\nof the 30th ACM International Conference on Multimedia , pages 1329\u20131338, 2022.\n [7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer. ", "original_text": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n"}, "hash": "674022e6282d0aad4ed6c848f9d3cfd854df90bab0efcfa32ced569b07f3eb82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ce6abe5-93a3-4c94-9970-0f8a1977241e", "node_type": "1", "metadata": {"window": "Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. ", "original_text": "Activating more pixels in image\nsuper-resolution transformer. "}, "hash": "aaf0643e543e91f8db46fdb627081393a5940349ba4158e14f0a7ed7ccef690d", "class_name": "RelatedNodeInfo"}}, "hash": "7b89d87e994ab0f58bb09e0942b0fcd4a7f22bac6ce9eeffd2793a347dedff61", "text": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. ", "start_char_idx": 37409, "end_char_idx": 37477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ce6abe5-93a3-4c94-9970-0f8a1977241e": {"__data__": {"id_": "0ce6abe5-93a3-4c94-9970-0f8a1977241e", "embedding": null, "metadata": {"window": "Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. ", "original_text": "Activating more pixels in image\nsuper-resolution transformer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab8c725a-40f4-425a-bdaa-db1e62da05d5", "node_type": "1", "metadata": {"window": "[7]Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao.  Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n", "original_text": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. "}, "hash": "7b89d87e994ab0f58bb09e0942b0fcd4a7f22bac6ce9eeffd2793a347dedff61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e6842f3-9e9a-4811-9956-01a82c9d9b08", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n"}, "hash": "323caae4d6af72a793f3a653acc3a813a49e10835aa67d343c2661a214ed3068", "class_name": "RelatedNodeInfo"}}, "hash": "aaf0643e543e91f8db46fdb627081393a5940349ba4158e14f0a7ed7ccef690d", "text": "Activating more pixels in image\nsuper-resolution transformer. ", "start_char_idx": 37477, "end_char_idx": 37539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e6842f3-9e9a-4811-9956-01a82c9d9b08": {"__data__": {"id_": "9e6842f3-9e9a-4811-9956-01a82c9d9b08", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ce6abe5-93a3-4c94-9970-0f8a1977241e", "node_type": "1", "metadata": {"window": "Pre-trained image processing transformer.  In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. ", "original_text": "Activating more pixels in image\nsuper-resolution transformer. "}, "hash": "aaf0643e543e91f8db46fdb627081393a5940349ba4158e14f0a7ed7ccef690d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ae979fc-107a-42bb-9167-5838cc143aac", "node_type": "1", "metadata": {"window": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n", "original_text": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. "}, "hash": "241b032e9c642e98bcaadc85c1b6136b2760604af8380562a1b1af7846152b18", "class_name": "RelatedNodeInfo"}}, "hash": "323caae4d6af72a793f3a653acc3a813a49e10835aa67d343c2661a214ed3068", "text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n", "start_char_idx": 37539, "end_char_idx": 37651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ae979fc-107a-42bb-9167-5838cc143aac": {"__data__": {"id_": "9ae979fc-107a-42bb-9167-5838cc143aac", "embedding": null, "metadata": {"window": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n", "original_text": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e6842f3-9e9a-4811-9956-01a82c9d9b08", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 12299\u201312310, 2021.\n [8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n"}, "hash": "323caae4d6af72a793f3a653acc3a813a49e10835aa67d343c2661a214ed3068", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f92ca317-b8bf-4c0d-b273-71978efd65a9", "node_type": "1", "metadata": {"window": "Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. ", "original_text": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. "}, "hash": "e8cbcb3f2590a99ca28615188a15e3b600e9dbe62b2fbbc94de1244645ea3427", "class_name": "RelatedNodeInfo"}}, "hash": "241b032e9c642e98bcaadc85c1b6136b2760604af8380562a1b1af7846152b18", "text": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. ", "start_char_idx": 37651, "end_char_idx": 37716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f92ca317-b8bf-4c0d-b273-71978efd65a9": {"__data__": {"id_": "f92ca317-b8bf-4c0d-b273-71978efd65a9", "embedding": null, "metadata": {"window": "Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. ", "original_text": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ae979fc-107a-42bb-9167-5838cc143aac", "node_type": "1", "metadata": {"window": "[8]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong.  Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n", "original_text": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. "}, "hash": "241b032e9c642e98bcaadc85c1b6136b2760604af8380562a1b1af7846152b18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89e970b2-a1e6-42e1-9052-3f3e9f191d38", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models. ", "original_text": "Cornell University - arXiv , 2017.\n"}, "hash": "96b48b9c1f7f9dd5f204881c2c993086991654ed5b9e9b8f46353b27fbacead8", "class_name": "RelatedNodeInfo"}}, "hash": "e8cbcb3f2590a99ca28615188a15e3b600e9dbe62b2fbbc94de1244645ea3427", "text": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. ", "start_char_idx": 37716, "end_char_idx": 37786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89e970b2-a1e6-42e1-9052-3f3e9f191d38": {"__data__": {"id_": "89e970b2-a1e6-42e1-9052-3f3e9f191d38", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models. ", "original_text": "Cornell University - arXiv , 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f92ca317-b8bf-4c0d-b273-71978efd65a9", "node_type": "1", "metadata": {"window": "Activating more pixels in image\nsuper-resolution transformer.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. ", "original_text": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. "}, "hash": "e8cbcb3f2590a99ca28615188a15e3b600e9dbe62b2fbbc94de1244645ea3427", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a88373f-7594-4d54-8723-0b289aadae1e", "node_type": "1", "metadata": {"window": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n", "original_text": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. "}, "hash": "9a1d5c50bdaa4490ad33b87820fb305097bbae711491ae083f366970363583c0", "class_name": "RelatedNodeInfo"}}, "hash": "96b48b9c1f7f9dd5f204881c2c993086991654ed5b9e9b8f46353b27fbacead8", "text": "Cornell University - arXiv , 2017.\n", "start_char_idx": 37786, "end_char_idx": 37821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a88373f-7594-4d54-8723-0b289aadae1e": {"__data__": {"id_": "5a88373f-7594-4d54-8723-0b289aadae1e", "embedding": null, "metadata": {"window": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n", "original_text": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89e970b2-a1e6-42e1-9052-3f3e9f191d38", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22367\u201322377, 2023.\n [9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models. ", "original_text": "Cornell University - arXiv , 2017.\n"}, "hash": "96b48b9c1f7f9dd5f204881c2c993086991654ed5b9e9b8f46353b27fbacead8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbee968b-8c24-4dbb-9328-b89cc9ba46d6", "node_type": "1", "metadata": {"window": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ", "original_text": "Intermediate layer optimization for\ninverse problems using deep generative models. "}, "hash": "490d6b2cba32dba893f99c563279d7d2020c990091f3233bc67575d15e38b9bd", "class_name": "RelatedNodeInfo"}}, "hash": "9a1d5c50bdaa4490ad33b87820fb305097bbae711491ae083f366970363583c0", "text": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. ", "start_char_idx": 37821, "end_char_idx": 37892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbee968b-8c24-4dbb-9328-b89cc9ba46d6": {"__data__": {"id_": "dbee968b-8c24-4dbb-9328-b89cc9ba46d6", "embedding": null, "metadata": {"window": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ", "original_text": "Intermediate layer optimization for\ninverse problems using deep generative models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a88373f-7594-4d54-8723-0b289aadae1e", "node_type": "1", "metadata": {"window": "[9]Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang.  Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n", "original_text": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. "}, "hash": "9a1d5c50bdaa4490ad33b87820fb305097bbae711491ae083f366970363583c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d56bbb8c-0194-4dbe-bae4-33558d95dc57", "node_type": "1", "metadata": {"window": "Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database. ", "original_text": "arXiv preprint arXiv:2102.07364 , 2021.\n"}, "hash": "9231316e85446fb963d8f93977f0f40ebc4a424f6ee4d96fbe5fa1c152ee1da8", "class_name": "RelatedNodeInfo"}}, "hash": "490d6b2cba32dba893f99c563279d7d2020c990091f3233bc67575d15e38b9bd", "text": "Intermediate layer optimization for\ninverse problems using deep generative models. ", "start_char_idx": 37892, "end_char_idx": 37975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d56bbb8c-0194-4dbe-bae4-33558d95dc57": {"__data__": {"id_": "d56bbb8c-0194-4dbe-bae4-33558d95dc57", "embedding": null, "metadata": {"window": "Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database. ", "original_text": "arXiv preprint arXiv:2102.07364 , 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbee968b-8c24-4dbb-9328-b89cc9ba46d6", "node_type": "1", "metadata": {"window": "Fsrnet: End-to-end learning face\nsuper-resolution with facial priors.  Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ", "original_text": "Intermediate layer optimization for\ninverse problems using deep generative models. "}, "hash": "490d6b2cba32dba893f99c563279d7d2020c990091f3233bc67575d15e38b9bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab591579-badf-42ad-be19-510df44dc6a4", "node_type": "1", "metadata": {"window": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n", "original_text": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "}, "hash": "9eb360f159fe4b75e04d85131332d1037d2a8af63f943024b85ea355f5e23132", "class_name": "RelatedNodeInfo"}}, "hash": "9231316e85446fb963d8f93977f0f40ebc4a424f6ee4d96fbe5fa1c152ee1da8", "text": "arXiv preprint arXiv:2102.07364 , 2021.\n", "start_char_idx": 37975, "end_char_idx": 38015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab591579-badf-42ad-be19-510df44dc6a4": {"__data__": {"id_": "ab591579-badf-42ad-be19-510df44dc6a4", "embedding": null, "metadata": {"window": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n", "original_text": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d56bbb8c-0194-4dbe-bae4-33558d95dc57", "node_type": "1", "metadata": {"window": "Cornell University - arXiv , 2017.\n [10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database. ", "original_text": "arXiv preprint arXiv:2102.07364 , 2021.\n"}, "hash": "9231316e85446fb963d8f93977f0f40ebc4a424f6ee4d96fbe5fa1c152ee1da8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29ac1e64-35f1-4c69-a63a-6516ce992690", "node_type": "1", "metadata": {"window": "Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n", "original_text": "Imagenet: A large-scale hierarchical\nimage database. "}, "hash": "ddfad7892f372f866c87478d934b3e3fc3e9610ea19ae262cb99ed0782e648d1", "class_name": "RelatedNodeInfo"}}, "hash": "9eb360f159fe4b75e04d85131332d1037d2a8af63f943024b85ea355f5e23132", "text": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ", "start_char_idx": 38015, "end_char_idx": 38091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29ac1e64-35f1-4c69-a63a-6516ce992690": {"__data__": {"id_": "29ac1e64-35f1-4c69-a63a-6516ce992690", "embedding": null, "metadata": {"window": "Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n", "original_text": "Imagenet: A large-scale hierarchical\nimage database. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab591579-badf-42ad-be19-510df44dc6a4", "node_type": "1", "metadata": {"window": "[10] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis.  Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n", "original_text": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "}, "hash": "9eb360f159fe4b75e04d85131332d1037d2a8af63f943024b85ea355f5e23132", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75f64e11-631b-468e-9822-0989d5c9c4c5", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol. ", "original_text": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n"}, "hash": "158ad385a2a8db3740f2ba473e86a1fbcf9c29f1eaf2c4a13ca8a560e04c98b7", "class_name": "RelatedNodeInfo"}}, "hash": "ddfad7892f372f866c87478d934b3e3fc3e9610ea19ae262cb99ed0782e648d1", "text": "Imagenet: A large-scale hierarchical\nimage database. ", "start_char_idx": 38091, "end_char_idx": 38144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75f64e11-631b-468e-9822-0989d5c9c4c5": {"__data__": {"id_": "75f64e11-631b-468e-9822-0989d5c9c4c5", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol. ", "original_text": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29ac1e64-35f1-4c69-a63a-6516ce992690", "node_type": "1", "metadata": {"window": "Intermediate layer optimization for\ninverse problems using deep generative models.  arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n", "original_text": "Imagenet: A large-scale hierarchical\nimage database. "}, "hash": "ddfad7892f372f866c87478d934b3e3fc3e9610ea19ae262cb99ed0782e648d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "752a51c8-af7b-4d36-899f-a86fb53b9158", "node_type": "1", "metadata": {"window": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis. ", "original_text": "Ieee, 2009.\n"}, "hash": "247521667bf7cb1a464210009a2e5985f7523ad90d68acf09765e5052cadd72a", "class_name": "RelatedNodeInfo"}}, "hash": "158ad385a2a8db3740f2ba473e86a1fbcf9c29f1eaf2c4a13ca8a560e04c98b7", "text": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n", "start_char_idx": 38144, "end_char_idx": 38228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "752a51c8-af7b-4d36-899f-a86fb53b9158": {"__data__": {"id_": "752a51c8-af7b-4d36-899f-a86fb53b9158", "embedding": null, "metadata": {"window": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis. ", "original_text": "Ieee, 2009.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75f64e11-631b-468e-9822-0989d5c9c4c5", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2102.07364 , 2021.\n [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol. ", "original_text": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n"}, "hash": "158ad385a2a8db3740f2ba473e86a1fbcf9c29f1eaf2c4a13ca8a560e04c98b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f27634f0-b139-425f-8819-6a333b789fd1", "node_type": "1", "metadata": {"window": "Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n", "original_text": "[12] Prafulla Dhariwal and Alexander Nichol. "}, "hash": "eb859d14df00e6d9a4e9c325e6efdb35284cec62b548bdb9a39536c9c1ca0334", "class_name": "RelatedNodeInfo"}}, "hash": "247521667bf7cb1a464210009a2e5985f7523ad90d68acf09765e5052cadd72a", "text": "Ieee, 2009.\n", "start_char_idx": 38228, "end_char_idx": 38240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f27634f0-b139-425f-8819-6a333b789fd1": {"__data__": {"id_": "f27634f0-b139-425f-8819-6a333b789fd1", "embedding": null, "metadata": {"window": "Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n", "original_text": "[12] Prafulla Dhariwal and Alexander Nichol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "752a51c8-af7b-4d36-899f-a86fb53b9158", "node_type": "1", "metadata": {"window": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis. ", "original_text": "Ieee, 2009.\n"}, "hash": "247521667bf7cb1a464210009a2e5985f7523ad90d68acf09765e5052cadd72a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ad198db-25a9-40e0-a305-0913364c1249", "node_type": "1", "metadata": {"window": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte. ", "original_text": "Diffusion models beat gans on image synthesis. "}, "hash": "62ca5bd9410311924d60b907003df025060a20ed8f6230bb125e63215df9aa09", "class_name": "RelatedNodeInfo"}}, "hash": "eb859d14df00e6d9a4e9c325e6efdb35284cec62b548bdb9a39536c9c1ca0334", "text": "[12] Prafulla Dhariwal and Alexander Nichol. ", "start_char_idx": 38240, "end_char_idx": 38285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ad198db-25a9-40e0-a305-0913364c1249": {"__data__": {"id_": "9ad198db-25a9-40e0-a305-0913364c1249", "embedding": null, "metadata": {"window": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte. ", "original_text": "Diffusion models beat gans on image synthesis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f27634f0-b139-425f-8819-6a333b789fd1", "node_type": "1", "metadata": {"window": "Imagenet: A large-scale hierarchical\nimage database.  In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n", "original_text": "[12] Prafulla Dhariwal and Alexander Nichol. "}, "hash": "eb859d14df00e6d9a4e9c325e6efdb35284cec62b548bdb9a39536c9c1ca0334", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c18168d-60e3-4882-b0aa-85c4ecc7728e", "node_type": "1", "metadata": {"window": "Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks. ", "original_text": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n"}, "hash": "bb668ebb78dbbbf76f6d52e047e8c092277ba38b49572db01b7ffe7d02ca04f6", "class_name": "RelatedNodeInfo"}}, "hash": "62ca5bd9410311924d60b907003df025060a20ed8f6230bb125e63215df9aa09", "text": "Diffusion models beat gans on image synthesis. ", "start_char_idx": 38285, "end_char_idx": 38332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c18168d-60e3-4882-b0aa-85c4ecc7728e": {"__data__": {"id_": "8c18168d-60e3-4882-b0aa-85c4ecc7728e", "embedding": null, "metadata": {"window": "Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks. ", "original_text": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ad198db-25a9-40e0-a305-0913364c1249", "node_type": "1", "metadata": {"window": "In 2009 IEEE conference on computer vision and pattern recognition , pages 248\u2013255.\n Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte. ", "original_text": "Diffusion models beat gans on image synthesis. "}, "hash": "62ca5bd9410311924d60b907003df025060a20ed8f6230bb125e63215df9aa09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bca1016-0aa5-4ee2-baa0-9aa22b6d5163", "node_type": "1", "metadata": {"window": "[12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n", "original_text": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte. "}, "hash": "4646d0091f53103d41b762505b6d757fae9c0ab9743c6a53f3a4b171a955a235", "class_name": "RelatedNodeInfo"}}, "hash": "bb668ebb78dbbbf76f6d52e047e8c092277ba38b49572db01b7ffe7d02ca04f6", "text": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n", "start_char_idx": 38332, "end_char_idx": 38404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bca1016-0aa5-4ee2-baa0-9aa22b6d5163": {"__data__": {"id_": "1bca1016-0aa5-4ee2-baa0-9aa22b6d5163", "embedding": null, "metadata": {"window": "[12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n", "original_text": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c18168d-60e3-4882-b0aa-85c4ecc7728e", "node_type": "1", "metadata": {"window": "Ieee, 2009.\n [12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks. ", "original_text": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n"}, "hash": "bb668ebb78dbbbf76f6d52e047e8c092277ba38b49572db01b7ffe7d02ca04f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "792ce173-7bf2-4f65-bfb8-73d159878d98", "node_type": "1", "metadata": {"window": "Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. ", "original_text": "Exemplar guided face image super-resolution without facial\nlandmarks. "}, "hash": "dfbd34730a78c2f1d3359ca3582e0bab313af2756095970923c9656a60eab737", "class_name": "RelatedNodeInfo"}}, "hash": "4646d0091f53103d41b762505b6d757fae9c0ab9743c6a53f3a4b171a955a235", "text": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte. ", "start_char_idx": 38404, "end_char_idx": 38451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "792ce173-7bf2-4f65-bfb8-73d159878d98": {"__data__": {"id_": "792ce173-7bf2-4f65-bfb8-73d159878d98", "embedding": null, "metadata": {"window": "Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. ", "original_text": "Exemplar guided face image super-resolution without facial\nlandmarks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bca1016-0aa5-4ee2-baa0-9aa22b6d5163", "node_type": "1", "metadata": {"window": "[12] Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n", "original_text": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte. "}, "hash": "4646d0091f53103d41b762505b6d757fae9c0ab9743c6a53f3a4b171a955a235", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0325e13-59cc-4da1-ac86-769fcd2281c6", "node_type": "1", "metadata": {"window": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution. ", "original_text": "Computer Vision and Pattern Recognition , 2019.\n"}, "hash": "cae6eab0bf81d999ebea48a8efb95653af583daf3755d5fa87e2794fbb23994a", "class_name": "RelatedNodeInfo"}}, "hash": "dfbd34730a78c2f1d3359ca3582e0bab313af2756095970923c9656a60eab737", "text": "Exemplar guided face image super-resolution without facial\nlandmarks. ", "start_char_idx": 38451, "end_char_idx": 38521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0325e13-59cc-4da1-ac86-769fcd2281c6": {"__data__": {"id_": "b0325e13-59cc-4da1-ac86-769fcd2281c6", "embedding": null, "metadata": {"window": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution. ", "original_text": "Computer Vision and Pattern Recognition , 2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "792ce173-7bf2-4f65-bfb8-73d159878d98", "node_type": "1", "metadata": {"window": "Diffusion models beat gans on image synthesis.  Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. ", "original_text": "Exemplar guided face image super-resolution without facial\nlandmarks. "}, "hash": "dfbd34730a78c2f1d3359ca3582e0bab313af2756095970923c9656a60eab737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e170db10-d971-40ae-a928-50b69e24655c", "node_type": "1", "metadata": {"window": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. ", "original_text": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. "}, "hash": "6fdd216062c1fcb90a2a000a5737dec54af301c0e274770d46aa5434fe0801f3", "class_name": "RelatedNodeInfo"}}, "hash": "cae6eab0bf81d999ebea48a8efb95653af583daf3755d5fa87e2794fbb23994a", "text": "Computer Vision and Pattern Recognition , 2019.\n", "start_char_idx": 38521, "end_char_idx": 38569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e170db10-d971-40ae-a928-50b69e24655c": {"__data__": {"id_": "e170db10-d971-40ae-a928-50b69e24655c", "embedding": null, "metadata": {"window": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. ", "original_text": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0325e13-59cc-4da1-ac86-769fcd2281c6", "node_type": "1", "metadata": {"window": "Advances in\nNeural Information Processing Systems , 34:8780\u20138794, 2021.\n [13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution. ", "original_text": "Computer Vision and Pattern Recognition , 2019.\n"}, "hash": "cae6eab0bf81d999ebea48a8efb95653af583daf3755d5fa87e2794fbb23994a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a39d37af-c459-43cd-b017-939502f7f398", "node_type": "1", "metadata": {"window": "Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n", "original_text": "Learning a deep convolutional network for\nimage super-resolution. "}, "hash": "89e5748017e90c3fd376ef9befd24f812a7d6377f96a7548939b2d56cc29d092", "class_name": "RelatedNodeInfo"}}, "hash": "6fdd216062c1fcb90a2a000a5737dec54af301c0e274770d46aa5434fe0801f3", "text": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. ", "start_char_idx": 38569, "end_char_idx": 38631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a39d37af-c459-43cd-b017-939502f7f398": {"__data__": {"id_": "a39d37af-c459-43cd-b017-939502f7f398", "embedding": null, "metadata": {"window": "Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n", "original_text": "Learning a deep convolutional network for\nimage super-resolution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e170db10-d971-40ae-a928-50b69e24655c", "node_type": "1", "metadata": {"window": "[13] Berk Dogan, Shuhang Gu, and Radu Timofte.  Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. ", "original_text": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. "}, "hash": "6fdd216062c1fcb90a2a000a5737dec54af301c0e274770d46aa5434fe0801f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1372150-0ba3-4928-84de-e428a2609731", "node_type": "1", "metadata": {"window": "Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. ", "original_text": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. "}, "hash": "b70617a86fded2e941e422dc4a51ca1991e1b0d76950899b02f22b8d5c9778b4", "class_name": "RelatedNodeInfo"}}, "hash": "89e5748017e90c3fd376ef9befd24f812a7d6377f96a7548939b2d56cc29d092", "text": "Learning a deep convolutional network for\nimage super-resolution. ", "start_char_idx": 38631, "end_char_idx": 38697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1372150-0ba3-4928-84de-e428a2609731": {"__data__": {"id_": "c1372150-0ba3-4928-84de-e428a2609731", "embedding": null, "metadata": {"window": "Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. ", "original_text": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a39d37af-c459-43cd-b017-939502f7f398", "node_type": "1", "metadata": {"window": "Exemplar guided face image super-resolution without facial\nlandmarks.  Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n", "original_text": "Learning a deep convolutional network for\nimage super-resolution. "}, "hash": "89e5748017e90c3fd376ef9befd24f812a7d6377f96a7548939b2d56cc29d092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "806df6a3-c505-45f5-86bd-60333f9cf57e", "node_type": "1", "metadata": {"window": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis. ", "original_text": "Springer, 2014.\n"}, "hash": "4279a00854644d467c7d18c9f49e13a5c696f3bbf475916f78d9b0481bfef070", "class_name": "RelatedNodeInfo"}}, "hash": "b70617a86fded2e941e422dc4a51ca1991e1b0d76950899b02f22b8d5c9778b4", "text": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. ", "start_char_idx": 38697, "end_char_idx": 38837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "806df6a3-c505-45f5-86bd-60333f9cf57e": {"__data__": {"id_": "806df6a3-c505-45f5-86bd-60333f9cf57e", "embedding": null, "metadata": {"window": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis. ", "original_text": "Springer, 2014.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1372150-0ba3-4928-84de-e428a2609731", "node_type": "1", "metadata": {"window": "Computer Vision and Pattern Recognition , 2019.\n [14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. ", "original_text": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199. "}, "hash": "b70617a86fded2e941e422dc4a51ca1991e1b0d76950899b02f22b8d5c9778b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "285936fd-25dd-494c-b9ee-ed8bf9a98ec0", "node_type": "1", "metadata": {"window": "Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n", "original_text": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. "}, "hash": "dc9a0d7aa60abbeb53dd314bfb305e325721a115a63e53a0f2e477481826be18", "class_name": "RelatedNodeInfo"}}, "hash": "4279a00854644d467c7d18c9f49e13a5c696f3bbf475916f78d9b0481bfef070", "text": "Springer, 2014.\n", "start_char_idx": 38837, "end_char_idx": 38853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "285936fd-25dd-494c-b9ee-ed8bf9a98ec0": {"__data__": {"id_": "285936fd-25dd-494c-b9ee-ed8bf9a98ec0", "embedding": null, "metadata": {"window": "Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n", "original_text": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "806df6a3-c505-45f5-86bd-60333f9cf57e", "node_type": "1", "metadata": {"window": "[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.  Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis. ", "original_text": "Springer, 2014.\n"}, "hash": "4279a00854644d467c7d18c9f49e13a5c696f3bbf475916f78d9b0481bfef070", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1daa56d6-9690-47e3-8e9c-6aa6c1f21d4c", "node_type": "1", "metadata": {"window": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n", "original_text": "Taming transformers for high-resolution image\nsynthesis. "}, "hash": "6e3cb1aeccfad63590721bed8bfe9aa05f9a98c053bf833965009a27cafbb416", "class_name": "RelatedNodeInfo"}}, "hash": "dc9a0d7aa60abbeb53dd314bfb305e325721a115a63e53a0f2e477481826be18", "text": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. ", "start_char_idx": 38853, "end_char_idx": 38905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1daa56d6-9690-47e3-8e9c-6aa6c1f21d4c": {"__data__": {"id_": "1daa56d6-9690-47e3-8e9c-6aa6c1f21d4c", "embedding": null, "metadata": {"window": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n", "original_text": "Taming transformers for high-resolution image\nsynthesis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "285936fd-25dd-494c-b9ee-ed8bf9a98ec0", "node_type": "1", "metadata": {"window": "Learning a deep convolutional network for\nimage super-resolution.  In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n", "original_text": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. "}, "hash": "dc9a0d7aa60abbeb53dd314bfb305e325721a115a63e53a0f2e477481826be18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac7565be-0c0c-4d04-9342-3b262b91e175", "node_type": "1", "metadata": {"window": "Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n"}, "hash": "1a662a059e7aa46cb1e0bfb2cd71553ab6b2e0b764fd2560f0278bebe995d836", "class_name": "RelatedNodeInfo"}}, "hash": "6e3cb1aeccfad63590721bed8bfe9aa05f9a98c053bf833965009a27cafbb416", "text": "Taming transformers for high-resolution image\nsynthesis. ", "start_char_idx": 38905, "end_char_idx": 38962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac7565be-0c0c-4d04-9342-3b262b91e175": {"__data__": {"id_": "ac7565be-0c0c-4d04-9342-3b262b91e175", "embedding": null, "metadata": {"window": "Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1daa56d6-9690-47e3-8e9c-6aa6c1f21d4c", "node_type": "1", "metadata": {"window": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part IV 13 , pages 184\u2013199.  Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n", "original_text": "Taming transformers for high-resolution image\nsynthesis. "}, "hash": "6e3cb1aeccfad63590721bed8bfe9aa05f9a98c053bf833965009a27cafbb416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1195bcc-cbaf-4627-8892-6f6ee09307c0", "node_type": "1", "metadata": {"window": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n", "original_text": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n"}, "hash": "76aea4a28a56732b73b9d2f9261f21f6e826e0aadd64b53cf35b3354aeab4613", "class_name": "RelatedNodeInfo"}}, "hash": "1a662a059e7aa46cb1e0bfb2cd71553ab6b2e0b764fd2560f0278bebe995d836", "text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n", "start_char_idx": 38962, "end_char_idx": 39074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1195bcc-cbaf-4627-8892-6f6ee09307c0": {"__data__": {"id_": "e1195bcc-cbaf-4627-8892-6f6ee09307c0", "embedding": null, "metadata": {"window": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n", "original_text": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac7565be-0c0c-4d04-9342-3b262b91e175", "node_type": "1", "metadata": {"window": "Springer, 2014.\n [15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n"}, "hash": "1a662a059e7aa46cb1e0bfb2cd71553ab6b2e0b764fd2560f0278bebe995d836", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49084dda-ce96-4f45-9014-8719182723e5", "node_type": "1", "metadata": {"window": "Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. ", "original_text": "Generative diffusion prior for unified image restoration and enhancement. "}, "hash": "2c6eb9f515f398cf30030c6fcfe45076d5d5c8960b5113a7bc4c4743ec455a04", "class_name": "RelatedNodeInfo"}}, "hash": "76aea4a28a56732b73b9d2f9261f21f6e826e0aadd64b53cf35b3354aeab4613", "text": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n", "start_char_idx": 39074, "end_char_idx": 39176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49084dda-ce96-4f45-9014-8719182723e5": {"__data__": {"id_": "49084dda-ce96-4f45-9014-8719182723e5", "embedding": null, "metadata": {"window": "Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. ", "original_text": "Generative diffusion prior for unified image restoration and enhancement. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1195bcc-cbaf-4627-8892-6f6ee09307c0", "node_type": "1", "metadata": {"window": "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer.  Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n", "original_text": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n"}, "hash": "76aea4a28a56732b73b9d2f9261f21f6e826e0aadd64b53cf35b3354aeab4613", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8abec02-9ded-4377-8c86-3a6a7747c570", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks. ", "original_text": "arXiv preprint arXiv:2304.01247 ,\n2023.\n"}, "hash": "567da208505672bd08f8928ac8074398c3a5e046f95a482a96a1ad7e28876d28", "class_name": "RelatedNodeInfo"}}, "hash": "2c6eb9f515f398cf30030c6fcfe45076d5d5c8960b5113a7bc4c4743ec455a04", "text": "Generative diffusion prior for unified image restoration and enhancement. ", "start_char_idx": 39176, "end_char_idx": 39250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8abec02-9ded-4377-8c86-3a6a7747c570": {"__data__": {"id_": "c8abec02-9ded-4377-8c86-3a6a7747c570", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks. ", "original_text": "arXiv preprint arXiv:2304.01247 ,\n2023.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49084dda-ce96-4f45-9014-8719182723e5", "node_type": "1", "metadata": {"window": "Taming transformers for high-resolution image\nsynthesis.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. ", "original_text": "Generative diffusion prior for unified image restoration and enhancement. "}, "hash": "2c6eb9f515f398cf30030c6fcfe45076d5d5c8960b5113a7bc4c4743ec455a04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2ba2529-2602-4843-838f-21477aed9997", "node_type": "1", "metadata": {"window": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n", "original_text": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. "}, "hash": "1dff34d44f5943e5f41ab123dfcfca59c71cad38e553b16f076781d7a9aafc54", "class_name": "RelatedNodeInfo"}}, "hash": "567da208505672bd08f8928ac8074398c3a5e046f95a482a96a1ad7e28876d28", "text": "arXiv preprint arXiv:2304.01247 ,\n2023.\n", "start_char_idx": 39250, "end_char_idx": 39290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2ba2529-2602-4843-838f-21477aed9997": {"__data__": {"id_": "d2ba2529-2602-4843-838f-21477aed9997", "embedding": null, "metadata": {"window": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n", "original_text": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8abec02-9ded-4377-8c86-3a6a7747c570", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873\u201312883, 2021.\n [16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks. ", "original_text": "arXiv preprint arXiv:2304.01247 ,\n2023.\n"}, "hash": "567da208505672bd08f8928ac8074398c3a5e046f95a482a96a1ad7e28876d28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47417187-8f04-4fae-beb3-6ad8b18c9344", "node_type": "1", "metadata": {"window": "Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. ", "original_text": "Generative adversarial networks. "}, "hash": "2b54c009d8ca72fc1d7bbd7984f4f1cf08cbca8a995a7d6f283d722d70e0a485", "class_name": "RelatedNodeInfo"}}, "hash": "1dff34d44f5943e5f41ab123dfcfca59c71cad38e553b16f076781d7a9aafc54", "text": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. ", "start_char_idx": 39290, "end_char_idx": 39424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47417187-8f04-4fae-beb3-6ad8b18c9344": {"__data__": {"id_": "47417187-8f04-4fae-beb3-6ad8b18c9344", "embedding": null, "metadata": {"window": "Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. ", "original_text": "Generative adversarial networks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2ba2529-2602-4843-838f-21477aed9997", "node_type": "1", "metadata": {"window": "[16] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai.\n Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n", "original_text": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. "}, "hash": "1dff34d44f5943e5f41ab123dfcfca59c71cad38e553b16f076781d7a9aafc54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c82d3052-6d33-4a32-8fbf-1fc0bd6ce166", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. ", "original_text": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n"}, "hash": "233925aa3d7b528109fc5785ce61ef997f0f39be3d6f596ab98cd2fe1ffab84d", "class_name": "RelatedNodeInfo"}}, "hash": "2b54c009d8ca72fc1d7bbd7984f4f1cf08cbca8a995a7d6f283d722d70e0a485", "text": "Generative adversarial networks. ", "start_char_idx": 39424, "end_char_idx": 39457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c82d3052-6d33-4a32-8fbf-1fc0bd6ce166": {"__data__": {"id_": "c82d3052-6d33-4a32-8fbf-1fc0bd6ce166", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. ", "original_text": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47417187-8f04-4fae-beb3-6ad8b18c9344", "node_type": "1", "metadata": {"window": "Generative diffusion prior for unified image restoration and enhancement.  arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. ", "original_text": "Generative adversarial networks. "}, "hash": "2b54c009d8ca72fc1d7bbd7984f4f1cf08cbca8a995a7d6f283d722d70e0a485", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2d63ff3-af2b-4bc3-9338-fdd9c5209d41", "node_type": "1", "metadata": {"window": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. ", "original_text": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. "}, "hash": "b9edbde122e40d5e3880ee321669fd9c07c43d4ab1ef59c02fcf3a75724a4917", "class_name": "RelatedNodeInfo"}}, "hash": "233925aa3d7b528109fc5785ce61ef997f0f39be3d6f596ab98cd2fe1ffab84d", "text": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n", "start_char_idx": 39457, "end_char_idx": 39508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2d63ff3-af2b-4bc3-9338-fdd9c5209d41": {"__data__": {"id_": "a2d63ff3-af2b-4bc3-9338-fdd9c5209d41", "embedding": null, "metadata": {"window": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. ", "original_text": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c82d3052-6d33-4a32-8fbf-1fc0bd6ce166", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2304.01247 ,\n2023.\n [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. ", "original_text": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n"}, "hash": "233925aa3d7b528109fc5785ce61ef997f0f39be3d6f596ab98cd2fe1ffab84d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91869c96-9f89-44c7-a052-63380c48d539", "node_type": "1", "metadata": {"window": "Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n", "original_text": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. "}, "hash": "45663114d5ad395261c40c02054439dd1e21166aae1f962596616b03831ab9fc", "class_name": "RelatedNodeInfo"}}, "hash": "b9edbde122e40d5e3880ee321669fd9c07c43d4ab1ef59c02fcf3a75724a4917", "text": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. ", "start_char_idx": 39508, "end_char_idx": 39602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91869c96-9f89-44c7-a052-63380c48d539": {"__data__": {"id_": "91869c96-9f89-44c7-a052-63380c48d539", "embedding": null, "metadata": {"window": "Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n", "original_text": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2d63ff3-af2b-4bc3-9338-fdd9c5209d41", "node_type": "1", "metadata": {"window": "[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.  Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. ", "original_text": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng. "}, "hash": "b9edbde122e40d5e3880ee321669fd9c07c43d4ab1ef59c02fcf3a75724a4917", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25b440f2-b925-42a6-bce5-27beede2d1d5", "node_type": "1", "metadata": {"window": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. ", "original_text": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. "}, "hash": "591bc8bc86854d64bc9e4266b44e81a8d9772c42048123069e8e08298f2d909f", "class_name": "RelatedNodeInfo"}}, "hash": "45663114d5ad395261c40c02054439dd1e21166aae1f962596616b03831ab9fc", "text": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. ", "start_char_idx": 39602, "end_char_idx": 39686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25b440f2-b925-42a6-bce5-27beede2d1d5": {"__data__": {"id_": "25b440f2-b925-42a6-bce5-27beede2d1d5", "embedding": null, "metadata": {"window": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. ", "original_text": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91869c96-9f89-44c7-a052-63380c48d539", "node_type": "1", "metadata": {"window": "Generative adversarial networks.  Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n", "original_text": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder. "}, "hash": "45663114d5ad395261c40c02054439dd1e21166aae1f962596616b03831ab9fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "816ace9f-2828-449e-a3f3-a6955c8c266b", "node_type": "1", "metadata": {"window": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. ", "original_text": "Springer, 2022.\n"}, "hash": "aba588f1dfab8775cbd435a65007b2e79ef45194500f9044ecd0cdc33fd1d191", "class_name": "RelatedNodeInfo"}}, "hash": "591bc8bc86854d64bc9e4266b44e81a8d9772c42048123069e8e08298f2d909f", "text": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. ", "start_char_idx": 39686, "end_char_idx": 39822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "816ace9f-2828-449e-a3f3-a6955c8c266b": {"__data__": {"id_": "816ace9f-2828-449e-a3f3-a6955c8c266b", "embedding": null, "metadata": {"window": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. ", "original_text": "Springer, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25b440f2-b925-42a6-bce5-27beede2d1d5", "node_type": "1", "metadata": {"window": "Communications of the ACM , 63(11):139\u2013\n144, 2020.\n [18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. ", "original_text": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143. "}, "hash": "591bc8bc86854d64bc9e4266b44e81a8d9772c42048123069e8e08298f2d909f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66840cf7-148c-45bf-aca9-9a5dbd806432", "node_type": "1", "metadata": {"window": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n", "original_text": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. "}, "hash": "33c988f5b23d73b8d209ff89ca84476b6af6e3a0fc796b93367971509efd2f83", "class_name": "RelatedNodeInfo"}}, "hash": "aba588f1dfab8775cbd435a65007b2e79ef45194500f9044ecd0cdc33fd1d191", "text": "Springer, 2022.\n", "start_char_idx": 39822, "end_char_idx": 39838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66840cf7-148c-45bf-aca9-9a5dbd806432": {"__data__": {"id_": "66840cf7-148c-45bf-aca9-9a5dbd806432", "embedding": null, "metadata": {"window": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n", "original_text": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "816ace9f-2828-449e-a3f3-a6955c8c266b", "node_type": "1", "metadata": {"window": "[18] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng.  Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. ", "original_text": "Springer, 2022.\n"}, "hash": "aba588f1dfab8775cbd435a65007b2e79ef45194500f9044ecd0cdc33fd1d191", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d36b11d-9ee5-4487-bb44-3681b58dc315", "node_type": "1", "metadata": {"window": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. ", "original_text": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. "}, "hash": "8ce89bde05a7964779552c6eb8dc69eba402b9ecd78eec6605355f35042a8f24", "class_name": "RelatedNodeInfo"}}, "hash": "33c988f5b23d73b8d209ff89ca84476b6af6e3a0fc796b93367971509efd2f83", "text": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. ", "start_char_idx": 39838, "end_char_idx": 39897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d36b11d-9ee5-4487-bb44-3681b58dc315": {"__data__": {"id_": "2d36b11d-9ee5-4487-bb44-3681b58dc315", "embedding": null, "metadata": {"window": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. ", "original_text": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66840cf7-148c-45bf-aca9-9a5dbd806432", "node_type": "1", "metadata": {"window": "Vqfr:\nBlind face restoration with vector-quantized dictionary and parallel decoder.  In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n", "original_text": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. "}, "hash": "33c988f5b23d73b8d209ff89ca84476b6af6e3a0fc796b93367971509efd2f83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acec621d-59d2-44fb-a813-80db16e8ace8", "node_type": "1", "metadata": {"window": "Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n"}, "hash": "1d24d755429ac26c00103ba8d5b6679c635f7c741b5f947d6ecce262c4ba93f5", "class_name": "RelatedNodeInfo"}}, "hash": "8ce89bde05a7964779552c6eb8dc69eba402b9ecd78eec6605355f35042a8f24", "text": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. ", "start_char_idx": 39897, "end_char_idx": 39994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acec621d-59d2-44fb-a813-80db16e8ace8": {"__data__": {"id_": "acec621d-59d2-44fb-a813-80db16e8ace8", "embedding": null, "metadata": {"window": "Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d36b11d-9ee5-4487-bb44-3681b58dc315", "node_type": "1", "metadata": {"window": "In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII , pages\n126\u2013143.  Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. ", "original_text": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors. "}, "hash": "8ce89bde05a7964779552c6eb8dc69eba402b9ecd78eec6605355f35042a8f24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc5c50c7-eda7-4da1-b97e-9e02ccab4aa0", "node_type": "1", "metadata": {"window": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n", "original_text": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. "}, "hash": "991c2bb2e9eca337c9916d13205b25e3668ab5e1ce0b09e4b1530b228511516b", "class_name": "RelatedNodeInfo"}}, "hash": "1d24d755429ac26c00103ba8d5b6679c635f7c741b5f947d6ecce262c4ba93f5", "text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n", "start_char_idx": 39994, "end_char_idx": 40104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc5c50c7-eda7-4da1-b97e-9e02ccab4aa0": {"__data__": {"id_": "dc5c50c7-eda7-4da1-b97e-9e02ccab4aa0", "embedding": null, "metadata": {"window": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n", "original_text": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acec621d-59d2-44fb-a813-80db16e8ace8", "node_type": "1", "metadata": {"window": "Springer, 2022.\n [19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n"}, "hash": "1d24d755429ac26c00103ba8d5b6679c635f7c741b5f947d6ecce262c4ba93f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c38de91a-6c43-452d-826d-867902c36353", "node_type": "1", "metadata": {"window": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. ", "original_text": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. "}, "hash": "850fbc61e5098b52ca0b84613530f97c78a5a6a5d865db9068edb35e47cde996", "class_name": "RelatedNodeInfo"}}, "hash": "991c2bb2e9eca337c9916d13205b25e3668ab5e1ce0b09e4b1530b228511516b", "text": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. ", "start_char_idx": 40104, "end_char_idx": 40200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c38de91a-6c43-452d-826d-867902c36353": {"__data__": {"id_": "c38de91a-6c43-452d-826d-867902c36353", "embedding": null, "metadata": {"window": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. ", "original_text": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc5c50c7-eda7-4da1-b97e-9e02ccab4aa0", "node_type": "1", "metadata": {"window": "[19] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong.  Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n", "original_text": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. "}, "hash": "991c2bb2e9eca337c9916d13205b25e3668ab5e1ce0b09e4b1530b228511516b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32a357c5-58b1-4a29-8426-03894718d024", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models. ", "original_text": "Advances in neural information\nprocessing systems , 30, 2017.\n"}, "hash": "a242ae6cb5616f1a9f6f64ef258ee7a09f1af624207a948622dd29509ca7ef8f", "class_name": "RelatedNodeInfo"}}, "hash": "850fbc61e5098b52ca0b84613530f97c78a5a6a5d865db9068edb35e47cde996", "text": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. ", "start_char_idx": 40200, "end_char_idx": 40283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32a357c5-58b1-4a29-8426-03894718d024": {"__data__": {"id_": "32a357c5-58b1-4a29-8426-03894718d024", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models. ", "original_text": "Advances in neural information\nprocessing systems , 30, 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c38de91a-6c43-452d-826d-867902c36353", "node_type": "1", "metadata": {"window": "Gcfsr: a generative and controllable face super\nresolution method without facial and gan priors.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. ", "original_text": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. "}, "hash": "850fbc61e5098b52ca0b84613530f97c78a5a6a5d865db9068edb35e47cde996", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6959b7cd-4fbd-47e2-8de1-f22ccb8251f9", "node_type": "1", "metadata": {"window": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n", "original_text": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "}, "hash": "b00c680cda2ccb7b185be6fdc81bbbc2ae3d3768eea36f5fcb84f7c7aee76bfa", "class_name": "RelatedNodeInfo"}}, "hash": "a242ae6cb5616f1a9f6f64ef258ee7a09f1af624207a948622dd29509ca7ef8f", "text": "Advances in neural information\nprocessing systems , 30, 2017.\n", "start_char_idx": 40283, "end_char_idx": 40345, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6959b7cd-4fbd-47e2-8de1-f22ccb8251f9": {"__data__": {"id_": "6959b7cd-4fbd-47e2-8de1-f22ccb8251f9", "embedding": null, "metadata": {"window": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n", "original_text": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32a357c5-58b1-4a29-8426-03894718d024", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1889\u20131898, 2022.\n [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models. ", "original_text": "Advances in neural information\nprocessing systems , 30, 2017.\n"}, "hash": "a242ae6cb5616f1a9f6f64ef258ee7a09f1af624207a948622dd29509ca7ef8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e15cf3f2-b63a-428a-be0b-8dfb48516116", "node_type": "1", "metadata": {"window": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. ", "original_text": "Denoising diffusion probabilistic models. "}, "hash": "4c3cbdced025828967dc19e4eb7666a8e8d5fdaa6e55e21795c3292fc0b18b87", "class_name": "RelatedNodeInfo"}}, "hash": "b00c680cda2ccb7b185be6fdc81bbbc2ae3d3768eea36f5fcb84f7c7aee76bfa", "text": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. ", "start_char_idx": 40345, "end_char_idx": 40397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e15cf3f2-b63a-428a-be0b-8dfb48516116": {"__data__": {"id_": "e15cf3f2-b63a-428a-be0b-8dfb48516116", "embedding": null, "metadata": {"window": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. ", "original_text": "Denoising diffusion probabilistic models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6959b7cd-4fbd-47e2-8de1-f22ccb8251f9", "node_type": "1", "metadata": {"window": "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.  Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n", "original_text": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "}, "hash": "b00c680cda2ccb7b185be6fdc81bbbc2ae3d3768eea36f5fcb84f7c7aee76bfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f79ca10-8897-460d-90df-c55c59c37dda", "node_type": "1", "metadata": {"window": "Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. ", "original_text": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n"}, "hash": "f1fd569515ac7b8a5bc3cac69052a3976d35b5f74257ed3fa0b864c66f418e20", "class_name": "RelatedNodeInfo"}}, "hash": "4c3cbdced025828967dc19e4eb7666a8e8d5fdaa6e55e21795c3292fc0b18b87", "text": "Denoising diffusion probabilistic models. ", "start_char_idx": 40397, "end_char_idx": 40439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f79ca10-8897-460d-90df-c55c59c37dda": {"__data__": {"id_": "8f79ca10-8897-460d-90df-c55c59c37dda", "embedding": null, "metadata": {"window": "Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. ", "original_text": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e15cf3f2-b63a-428a-be0b-8dfb48516116", "node_type": "1", "metadata": {"window": "Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.  Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. ", "original_text": "Denoising diffusion probabilistic models. "}, "hash": "4c3cbdced025828967dc19e4eb7666a8e8d5fdaa6e55e21795c3292fc0b18b87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d587bb29-a55b-437d-a72a-0290fb6ac4e8", "node_type": "1", "metadata": {"window": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n", "original_text": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. "}, "hash": "6f7a9d05fd935e15a2c122396936f84a5ba5cb2f613663dcaa101721b8994750", "class_name": "RelatedNodeInfo"}}, "hash": "f1fd569515ac7b8a5bc3cac69052a3976d35b5f74257ed3fa0b864c66f418e20", "text": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n", "start_char_idx": 40439, "end_char_idx": 40511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d587bb29-a55b-437d-a72a-0290fb6ac4e8": {"__data__": {"id_": "d587bb29-a55b-437d-a72a-0290fb6ac4e8", "embedding": null, "metadata": {"window": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n", "original_text": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f79ca10-8897-460d-90df-c55c59c37dda", "node_type": "1", "metadata": {"window": "Advances in neural information\nprocessing systems , 30, 2017.\n 12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. ", "original_text": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n"}, "hash": "f1fd569515ac7b8a5bc3cac69052a3976d35b5f74257ed3fa0b864c66f418e20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a7fc208-ce88-4a72-b599-57f8af2280a2", "node_type": "1", "metadata": {"window": "Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. ", "original_text": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. "}, "hash": "2e56aa25f2bbd75f066e4c279d8b73aae60f076b7643c24e9f7a4ee7df881c19", "class_name": "RelatedNodeInfo"}}, "hash": "6f7a9d05fd935e15a2c122396936f84a5ba5cb2f613663dcaa101721b8994750", "text": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. ", "start_char_idx": 40511, "end_char_idx": 40582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a7fc208-ce88-4a72-b599-57f8af2280a2": {"__data__": {"id_": "0a7fc208-ce88-4a72-b599-57f8af2280a2", "embedding": null, "metadata": {"window": "Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. ", "original_text": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d587bb29-a55b-437d-a72a-0290fb6ac4e8", "node_type": "1", "metadata": {"window": "12\n\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n", "original_text": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. "}, "hash": "6f7a9d05fd935e15a2c122396936f84a5ba5cb2f613663dcaa101721b8994750", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d8740bf-f083-424b-9cab-ad8231b15f0c", "node_type": "1", "metadata": {"window": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks. ", "original_text": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n"}, "hash": "26e0390cffe7220a6b67b3af1c5615746b944fe456eb5ce2f1311c08dbedb11f", "class_name": "RelatedNodeInfo"}}, "hash": "2e56aa25f2bbd75f066e4c279d8b73aae60f076b7643c24e9f7a4ee7df881c19", "text": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. ", "start_char_idx": 40582, "end_char_idx": 40681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d8740bf-f083-424b-9cab-ad8231b15f0c": {"__data__": {"id_": "9d8740bf-f083-424b-9cab-ad8231b15f0c", "embedding": null, "metadata": {"window": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks. ", "original_text": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a7fc208-ce88-4a72-b599-57f8af2280a2", "node_type": "1", "metadata": {"window": "Denoising diffusion probabilistic models.  Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. ", "original_text": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments. "}, "hash": "2e56aa25f2bbd75f066e4c279d8b73aae60f076b7643c24e9f7a4ee7df881c19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3032879-67a0-4ba1-804d-7fe94803578e", "node_type": "1", "metadata": {"window": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n", "original_text": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. "}, "hash": "444a54d37d01f19b5faa365f51b3868adb195c9faed7c669ad28b78f6dfcab5d", "class_name": "RelatedNodeInfo"}}, "hash": "26e0390cffe7220a6b67b3af1c5615746b944fe456eb5ce2f1311c08dbedb11f", "text": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n", "start_char_idx": 40681, "end_char_idx": 40757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3032879-67a0-4ba1-804d-7fe94803578e": {"__data__": {"id_": "d3032879-67a0-4ba1-804d-7fe94803578e", "embedding": null, "metadata": {"window": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n", "original_text": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d8740bf-f083-424b-9cab-ad8231b15f0c", "node_type": "1", "metadata": {"window": "Advances in Neural\nInformation Processing Systems , 33:6840\u20136851, 2020.\n [22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks. ", "original_text": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n"}, "hash": "26e0390cffe7220a6b67b3af1c5615746b944fe456eb5ce2f1311c08dbedb11f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53dc86a5-ec44-4298-8c23-0d4125cbcd59", "node_type": "1", "metadata": {"window": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. ", "original_text": "Image-to-image translation with conditional\nadversarial networks. "}, "hash": "8f34a77130a3a272ba4f9b2ed567768fa52cab6d7653f213c30f6957f99876a5", "class_name": "RelatedNodeInfo"}}, "hash": "444a54d37d01f19b5faa365f51b3868adb195c9faed7c669ad28b78f6dfcab5d", "text": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. ", "start_char_idx": 40757, "end_char_idx": 40824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53dc86a5-ec44-4298-8c23-0d4125cbcd59": {"__data__": {"id_": "53dc86a5-ec44-4298-8c23-0d4125cbcd59", "embedding": null, "metadata": {"window": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. ", "original_text": "Image-to-image translation with conditional\nadversarial networks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3032879-67a0-4ba1-804d-7fe94803578e", "node_type": "1", "metadata": {"window": "[22] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.  Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n", "original_text": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. "}, "hash": "444a54d37d01f19b5faa365f51b3868adb195c9faed7c669ad28b78f6dfcab5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aece8fb3-763e-4fc8-81fd-73234ff42a15", "node_type": "1", "metadata": {"window": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection. ", "original_text": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n"}, "hash": "6612489c77f399ce676a60cbef607f89c43de9a3c79c9829053c01d4ba382025", "class_name": "RelatedNodeInfo"}}, "hash": "8f34a77130a3a272ba4f9b2ed567768fa52cab6d7653f213c30f6957f99876a5", "text": "Image-to-image translation with conditional\nadversarial networks. ", "start_char_idx": 40824, "end_char_idx": 40890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aece8fb3-763e-4fc8-81fd-73234ff42a15": {"__data__": {"id_": "aece8fb3-763e-4fc8-81fd-73234ff42a15", "embedding": null, "metadata": {"window": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection. ", "original_text": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53dc86a5-ec44-4298-8c23-0d4125cbcd59", "node_type": "1", "metadata": {"window": "Labeled faces in the wild: A\ndatabase for studying face recognition in unconstrained environments.  Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. ", "original_text": "Image-to-image translation with conditional\nadversarial networks. "}, "hash": "8f34a77130a3a272ba4f9b2ed567768fa52cab6d7653f213c30f6957f99876a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cd3b4ca-8188-4fbd-9901-ee552334f981", "node_type": "1", "metadata": {"window": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n", "original_text": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. "}, "hash": "1ad0912f09b1647569e889fcce3cdc9a2e521387bbda4b77dc747700680b9efd", "class_name": "RelatedNodeInfo"}}, "hash": "6612489c77f399ce676a60cbef607f89c43de9a3c79c9829053c01d4ba382025", "text": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n", "start_char_idx": 40890, "end_char_idx": 40996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cd3b4ca-8188-4fbd-9901-ee552334f981": {"__data__": {"id_": "6cd3b4ca-8188-4fbd-9901-ee552334f981", "embedding": null, "metadata": {"window": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n", "original_text": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aece8fb3-763e-4fc8-81fd-73234ff42a15", "node_type": "1", "metadata": {"window": "Technical Report 07-49, University\nof Massachusetts, Amherst, October 2007.\n [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection. ", "original_text": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n"}, "hash": "6612489c77f399ce676a60cbef607f89c43de9a3c79c9829053c01d4ba382025", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdcde95e-abcb-4a48-add9-0000b9cedc63", "node_type": "1", "metadata": {"window": "Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila. ", "original_text": "Real-world super-resolution\nvia kernel estimation and noise injection. "}, "hash": "ffb60acc0f03b45da4fa16e1b70f8d9635272bbb14ccf474e87c08c0da63dff5", "class_name": "RelatedNodeInfo"}}, "hash": "1ad0912f09b1647569e889fcce3cdc9a2e521387bbda4b77dc747700680b9efd", "text": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. ", "start_char_idx": 40996, "end_char_idx": 41077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdcde95e-abcb-4a48-add9-0000b9cedc63": {"__data__": {"id_": "fdcde95e-abcb-4a48-add9-0000b9cedc63", "embedding": null, "metadata": {"window": "Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila. ", "original_text": "Real-world super-resolution\nvia kernel estimation and noise injection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cd3b4ca-8188-4fbd-9901-ee552334f981", "node_type": "1", "metadata": {"window": "[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.  Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n", "original_text": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. "}, "hash": "1ad0912f09b1647569e889fcce3cdc9a2e521387bbda4b77dc747700680b9efd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c5ce62d-9b5f-4375-a25b-31539cbb019e", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks. ", "original_text": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n"}, "hash": "ee0df9621073f4048002bc3a196147e72d093dbecfc689574a6640eab625b3fb", "class_name": "RelatedNodeInfo"}}, "hash": "ffb60acc0f03b45da4fa16e1b70f8d9635272bbb14ccf474e87c08c0da63dff5", "text": "Real-world super-resolution\nvia kernel estimation and noise injection. ", "start_char_idx": 41077, "end_char_idx": 41148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c5ce62d-9b5f-4375-a25b-31539cbb019e": {"__data__": {"id_": "0c5ce62d-9b5f-4375-a25b-31539cbb019e", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks. ", "original_text": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdcde95e-abcb-4a48-add9-0000b9cedc63", "node_type": "1", "metadata": {"window": "Image-to-image translation with conditional\nadversarial networks.  In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila. ", "original_text": "Real-world super-resolution\nvia kernel estimation and noise injection. "}, "hash": "ffb60acc0f03b45da4fa16e1b70f8d9635272bbb14ccf474e87c08c0da63dff5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe5dbf49-332b-4c36-8d5c-3f49fcbbeb7e", "node_type": "1", "metadata": {"window": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n", "original_text": "[25] Tero Karras, Samuli Laine, and Timo Aila. "}, "hash": "d2397f995736389051d2dfbfb7349e925a37b05930a5342538fea15a943aa8e0", "class_name": "RelatedNodeInfo"}}, "hash": "ee0df9621073f4048002bc3a196147e72d093dbecfc689574a6640eab625b3fb", "text": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n", "start_char_idx": 41148, "end_char_idx": 41266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe5dbf49-332b-4c36-8d5c-3f49fcbbeb7e": {"__data__": {"id_": "fe5dbf49-332b-4c36-8d5c-3f49fcbbeb7e", "embedding": null, "metadata": {"window": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n", "original_text": "[25] Tero Karras, Samuli Laine, and Timo Aila. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c5ce62d-9b5f-4375-a25b-31539cbb019e", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 1125\u20131134, 2017.\n [24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks. ", "original_text": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n"}, "hash": "ee0df9621073f4048002bc3a196147e72d093dbecfc689574a6640eab625b3fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab021809-1cfa-4693-9ae9-3ac5471a6c96", "node_type": "1", "metadata": {"window": "Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. ", "original_text": "A style-based generator architecture for generative adversarial\nnetworks. "}, "hash": "acfa404f99857cd9b87709a46f0ee2056f8b85666c38818b6ec751c24662d634", "class_name": "RelatedNodeInfo"}}, "hash": "d2397f995736389051d2dfbfb7349e925a37b05930a5342538fea15a943aa8e0", "text": "[25] Tero Karras, Samuli Laine, and Timo Aila. ", "start_char_idx": 41266, "end_char_idx": 41313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab021809-1cfa-4693-9ae9-3ac5471a6c96": {"__data__": {"id_": "ab021809-1cfa-4693-9ae9-3ac5471a6c96", "embedding": null, "metadata": {"window": "Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. ", "original_text": "A style-based generator architecture for generative adversarial\nnetworks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe5dbf49-332b-4c36-8d5c-3f49fcbbeb7e", "node_type": "1", "metadata": {"window": "[24] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.  Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n", "original_text": "[25] Tero Karras, Samuli Laine, and Timo Aila. "}, "hash": "d2397f995736389051d2dfbfb7349e925a37b05930a5342538fea15a943aa8e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac64eb8e-e4e9-43eb-ba76-62082fafd7a1", "node_type": "1", "metadata": {"window": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n"}, "hash": "de52f4aa5171d66b71d90eba09465469716177e75106d6c7b847a1237124d751", "class_name": "RelatedNodeInfo"}}, "hash": "acfa404f99857cd9b87709a46f0ee2056f8b85666c38818b6ec751c24662d634", "text": "A style-based generator architecture for generative adversarial\nnetworks. ", "start_char_idx": 41313, "end_char_idx": 41387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac64eb8e-e4e9-43eb-ba76-62082fafd7a1": {"__data__": {"id_": "ac64eb8e-e4e9-43eb-ba76-62082fafd7a1", "embedding": null, "metadata": {"window": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab021809-1cfa-4693-9ae9-3ac5471a6c96", "node_type": "1", "metadata": {"window": "Real-world super-resolution\nvia kernel estimation and noise injection.  In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. ", "original_text": "A style-based generator architecture for generative adversarial\nnetworks. "}, "hash": "acfa404f99857cd9b87709a46f0ee2056f8b85666c38818b6ec751c24662d634", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "189d22b1-81ba-4ceb-808e-17d04d7e637d", "node_type": "1", "metadata": {"window": "[25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n", "original_text": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. "}, "hash": "00dc9f9178ebe3dfe9d9c0224bc80965823d483d0c19ecf87a6dea8d183dd46d", "class_name": "RelatedNodeInfo"}}, "hash": "de52f4aa5171d66b71d90eba09465469716177e75106d6c7b847a1237124d751", "text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n", "start_char_idx": 41387, "end_char_idx": 41497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "189d22b1-81ba-4ceb-808e-17d04d7e637d": {"__data__": {"id_": "189d22b1-81ba-4ceb-808e-17d04d7e637d", "embedding": null, "metadata": {"window": "[25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n", "original_text": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac64eb8e-e4e9-43eb-ba76-62082fafd7a1", "node_type": "1", "metadata": {"window": "In proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , pages 466\u2013467, 2020.\n [25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n"}, "hash": "de52f4aa5171d66b71d90eba09465469716177e75106d6c7b847a1237124d751", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "027f24ee-13b3-44a4-a4c5-47f60a894c4d", "node_type": "1", "metadata": {"window": "A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. ", "original_text": "Denoising diffusion restoration models.\n"}, "hash": "0cf7143e3fa3a34b95ce3c1a9c3f77d8df6ccb28bfe833e3b8b4bb1dd20251af", "class_name": "RelatedNodeInfo"}}, "hash": "00dc9f9178ebe3dfe9d9c0224bc80965823d483d0c19ecf87a6dea8d183dd46d", "text": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. ", "start_char_idx": 41497, "end_char_idx": 41563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "027f24ee-13b3-44a4-a4c5-47f60a894c4d": {"__data__": {"id_": "027f24ee-13b3-44a4-a4c5-47f60a894c4d", "embedding": null, "metadata": {"window": "A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. ", "original_text": "Denoising diffusion restoration models.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "189d22b1-81ba-4ceb-808e-17d04d7e637d", "node_type": "1", "metadata": {"window": "[25] Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n", "original_text": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. "}, "hash": "00dc9f9178ebe3dfe9d9c0224bc80965823d483d0c19ecf87a6dea8d183dd46d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f71f39dc-0c17-4c6b-8470-0258a599212a", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark. ", "original_text": "arXiv preprint arXiv:2201.11793 , 2022.\n"}, "hash": "c79082b1007bd191bf7994b0a007836f5b4cc86efb75b4517c89fe6f593d9b4d", "class_name": "RelatedNodeInfo"}}, "hash": "0cf7143e3fa3a34b95ce3c1a9c3f77d8df6ccb28bfe833e3b8b4bb1dd20251af", "text": "Denoising diffusion restoration models.\n", "start_char_idx": 41563, "end_char_idx": 41603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f71f39dc-0c17-4c6b-8470-0258a599212a": {"__data__": {"id_": "f71f39dc-0c17-4c6b-8470-0258a599212a", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark. ", "original_text": "arXiv preprint arXiv:2201.11793 , 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "027f24ee-13b3-44a4-a4c5-47f60a894c4d", "node_type": "1", "metadata": {"window": "A style-based generator architecture for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. ", "original_text": "Denoising diffusion restoration models.\n"}, "hash": "0cf7143e3fa3a34b95ce3c1a9c3f77d8df6ccb28bfe833e3b8b4bb1dd20251af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5074b1d9-4bf3-4898-8ee6-1a3be739a40d", "node_type": "1", "metadata": {"window": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n", "original_text": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. "}, "hash": "140c56016e0ab50850a530f221bee49aa8a0479ef7ae1939013218995bca6d4f", "class_name": "RelatedNodeInfo"}}, "hash": "c79082b1007bd191bf7994b0a007836f5b4cc86efb75b4517c89fe6f593d9b4d", "text": "arXiv preprint arXiv:2201.11793 , 2022.\n", "start_char_idx": 41603, "end_char_idx": 41643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5074b1d9-4bf3-4898-8ee6-1a3be739a40d": {"__data__": {"id_": "5074b1d9-4bf3-4898-8ee6-1a3be739a40d", "embedding": null, "metadata": {"window": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n", "original_text": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f71f39dc-0c17-4c6b-8470-0258a599212a", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n4401\u20134410, 2019.\n [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark. ", "original_text": "arXiv preprint arXiv:2201.11793 , 2022.\n"}, "hash": "c79082b1007bd191bf7994b0a007836f5b4cc86efb75b4517c89fe6f593d9b4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d98c37f-f2a5-43a8-ba95-37df1a851078", "node_type": "1", "metadata": {"window": "Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba. ", "original_text": "Progressive face super-resolution via\nattention to facial landmark. "}, "hash": "2e0cef5781cb60d9a77be9a55bc7a10d4796a69285c1cecddbebf0d49ea4f376", "class_name": "RelatedNodeInfo"}}, "hash": "140c56016e0ab50850a530f221bee49aa8a0479ef7ae1939013218995bca6d4f", "text": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. ", "start_char_idx": 41643, "end_char_idx": 41705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d98c37f-f2a5-43a8-ba95-37df1a851078": {"__data__": {"id_": "8d98c37f-f2a5-43a8-ba95-37df1a851078", "embedding": null, "metadata": {"window": "Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba. ", "original_text": "Progressive face super-resolution via\nattention to facial landmark. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5074b1d9-4bf3-4898-8ee6-1a3be739a40d", "node_type": "1", "metadata": {"window": "[26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.  Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n", "original_text": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. "}, "hash": "140c56016e0ab50850a530f221bee49aa8a0479ef7ae1939013218995bca6d4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a74876de-3fb5-462e-8ab8-f0cf22b5d425", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization. ", "original_text": "arXiv preprint arXiv:1908.08239 , 2019.\n"}, "hash": "80876d55a160d4358f3b3bfc4a2e3f6ee6366e880476560c0553c27cee5b4649", "class_name": "RelatedNodeInfo"}}, "hash": "2e0cef5781cb60d9a77be9a55bc7a10d4796a69285c1cecddbebf0d49ea4f376", "text": "Progressive face super-resolution via\nattention to facial landmark. ", "start_char_idx": 41705, "end_char_idx": 41773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a74876de-3fb5-462e-8ab8-f0cf22b5d425": {"__data__": {"id_": "a74876de-3fb5-462e-8ab8-f0cf22b5d425", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization. ", "original_text": "arXiv preprint arXiv:1908.08239 , 2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d98c37f-f2a5-43a8-ba95-37df1a851078", "node_type": "1", "metadata": {"window": "Denoising diffusion restoration models.\n arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba. ", "original_text": "Progressive face super-resolution via\nattention to facial landmark. "}, "hash": "2e0cef5781cb60d9a77be9a55bc7a10d4796a69285c1cecddbebf0d49ea4f376", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e35a3c38-1db8-4c1d-9d8d-fbbf2584602d", "node_type": "1", "metadata": {"window": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n", "original_text": "[28] Diederik P Kingma and Jimmy Ba. "}, "hash": "3bff282d73fdcea0f30292a9fa09592fa8d1d5cc2095ac3f7869c9c5f8105919", "class_name": "RelatedNodeInfo"}}, "hash": "80876d55a160d4358f3b3bfc4a2e3f6ee6366e880476560c0553c27cee5b4649", "text": "arXiv preprint arXiv:1908.08239 , 2019.\n", "start_char_idx": 41773, "end_char_idx": 41813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e35a3c38-1db8-4c1d-9d8d-fbbf2584602d": {"__data__": {"id_": "e35a3c38-1db8-4c1d-9d8d-fbbf2584602d", "embedding": null, "metadata": {"window": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n", "original_text": "[28] Diederik P Kingma and Jimmy Ba. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a74876de-3fb5-462e-8ab8-f0cf22b5d425", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2201.11793 , 2022.\n [27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization. ", "original_text": "arXiv preprint arXiv:1908.08239 , 2019.\n"}, "hash": "80876d55a160d4358f3b3bfc4a2e3f6ee6366e880476560c0553c27cee5b4649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b86fae1-803b-4264-85eb-db5493403373", "node_type": "1", "metadata": {"window": "Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling. ", "original_text": "Adam: A method for stochastic optimization. "}, "hash": "573d09428f2a5e0bb3151fe3793940ec4b67f55902d8bc1fb8f72a4393ead003", "class_name": "RelatedNodeInfo"}}, "hash": "3bff282d73fdcea0f30292a9fa09592fa8d1d5cc2095ac3f7869c9c5f8105919", "text": "[28] Diederik P Kingma and Jimmy Ba. ", "start_char_idx": 41813, "end_char_idx": 41850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b86fae1-803b-4264-85eb-db5493403373": {"__data__": {"id_": "1b86fae1-803b-4264-85eb-db5493403373", "embedding": null, "metadata": {"window": "Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling. ", "original_text": "Adam: A method for stochastic optimization. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e35a3c38-1db8-4c1d-9d8d-fbbf2584602d", "node_type": "1", "metadata": {"window": "[27] Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim.  Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n", "original_text": "[28] Diederik P Kingma and Jimmy Ba. "}, "hash": "3bff282d73fdcea0f30292a9fa09592fa8d1d5cc2095ac3f7869c9c5f8105919", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cd25b7e-25be-4bed-81d7-37dc5c1b8e46", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes. ", "original_text": "arXiv preprint\narXiv:1412.6980 , 2014.\n"}, "hash": "561a386dc708836bb69ca48c4562f8b66d9c833f2b5fe75d52f1f2ea62ad3e46", "class_name": "RelatedNodeInfo"}}, "hash": "573d09428f2a5e0bb3151fe3793940ec4b67f55902d8bc1fb8f72a4393ead003", "text": "Adam: A method for stochastic optimization. ", "start_char_idx": 41850, "end_char_idx": 41894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cd25b7e-25be-4bed-81d7-37dc5c1b8e46": {"__data__": {"id_": "0cd25b7e-25be-4bed-81d7-37dc5c1b8e46", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes. ", "original_text": "arXiv preprint\narXiv:1412.6980 , 2014.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b86fae1-803b-4264-85eb-db5493403373", "node_type": "1", "metadata": {"window": "Progressive face super-resolution via\nattention to facial landmark.  arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling. ", "original_text": "Adam: A method for stochastic optimization. "}, "hash": "573d09428f2a5e0bb3151fe3793940ec4b67f55902d8bc1fb8f72a4393ead003", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4705da8-b72f-4071-845b-bcb6c2e7b989", "node_type": "1", "metadata": {"window": "[28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n", "original_text": "[29] Diederik P Kingma and Max Welling. "}, "hash": "83807abc157e0f2378caf233efe174e03db76f9002bfc7aae49e97b2307d7bc1", "class_name": "RelatedNodeInfo"}}, "hash": "561a386dc708836bb69ca48c4562f8b66d9c833f2b5fe75d52f1f2ea62ad3e46", "text": "arXiv preprint\narXiv:1412.6980 , 2014.\n", "start_char_idx": 41894, "end_char_idx": 41933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4705da8-b72f-4071-845b-bcb6c2e7b989": {"__data__": {"id_": "a4705da8-b72f-4071-845b-bcb6c2e7b989", "embedding": null, "metadata": {"window": "[28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n", "original_text": "[29] Diederik P Kingma and Max Welling. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cd25b7e-25be-4bed-81d7-37dc5c1b8e46", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1908.08239 , 2019.\n [28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes. ", "original_text": "arXiv preprint\narXiv:1412.6980 , 2014.\n"}, "hash": "561a386dc708836bb69ca48c4562f8b66d9c833f2b5fe75d52f1f2ea62ad3e46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e649744-0fd3-4b8a-b617-724d6eb78955", "node_type": "1", "metadata": {"window": "Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling. ", "original_text": "Auto-encoding variational bayes. "}, "hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "class_name": "RelatedNodeInfo"}}, "hash": "83807abc157e0f2378caf233efe174e03db76f9002bfc7aae49e97b2307d7bc1", "text": "[29] Diederik P Kingma and Max Welling. ", "start_char_idx": 41933, "end_char_idx": 41973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e649744-0fd3-4b8a-b617-724d6eb78955": {"__data__": {"id_": "3e649744-0fd3-4b8a-b617-724d6eb78955", "embedding": null, "metadata": {"window": "Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling. ", "original_text": "Auto-encoding variational bayes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4705da8-b72f-4071-845b-bcb6c2e7b989", "node_type": "1", "metadata": {"window": "[28] Diederik P Kingma and Jimmy Ba.  Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n", "original_text": "[29] Diederik P Kingma and Max Welling. "}, "hash": "83807abc157e0f2378caf233efe174e03db76f9002bfc7aae49e97b2307d7bc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d200043-eec5-47e4-b46c-d547bc14ffa0", "node_type": "1", "metadata": {"window": "arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes. ", "original_text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n"}, "hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "class_name": "RelatedNodeInfo"}}, "hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "text": "Auto-encoding variational bayes. ", "start_char_idx": 41973, "end_char_idx": 42006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d200043-eec5-47e4-b46c-d547bc14ffa0": {"__data__": {"id_": "0d200043-eec5-47e4-b46c-d547bc14ffa0", "embedding": null, "metadata": {"window": "arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes. ", "original_text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e649744-0fd3-4b8a-b617-724d6eb78955", "node_type": "1", "metadata": {"window": "Adam: A method for stochastic optimization.  arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling. ", "original_text": "Auto-encoding variational bayes. "}, "hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b839629-7174-4c31-9119-f482194356e7", "node_type": "1", "metadata": {"window": "[29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n", "original_text": "[30] Diederik P Kingma and Max Welling. "}, "hash": "0ce401db8a6c169af70a898a571f404fe7c2554223fc55206b8498f765ec33e8", "class_name": "RelatedNodeInfo"}}, "hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n", "start_char_idx": 42006, "end_char_idx": 42045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b839629-7174-4c31-9119-f482194356e7": {"__data__": {"id_": "2b839629-7174-4c31-9119-f482194356e7", "embedding": null, "metadata": {"window": "[29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n", "original_text": "[30] Diederik P Kingma and Max Welling. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d200043-eec5-47e4-b46c-d547bc14ffa0", "node_type": "1", "metadata": {"window": "arXiv preprint\narXiv:1412.6980 , 2014.\n [29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes. ", "original_text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n"}, "hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b079b4d9-36e0-4a32-8616-7751d682fba4", "node_type": "1", "metadata": {"window": "Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. ", "original_text": "Auto-encoding variational bayes. "}, "hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "class_name": "RelatedNodeInfo"}}, "hash": "0ce401db8a6c169af70a898a571f404fe7c2554223fc55206b8498f765ec33e8", "text": "[30] Diederik P Kingma and Max Welling. ", "start_char_idx": 42045, "end_char_idx": 42085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b079b4d9-36e0-4a32-8616-7751d682fba4": {"__data__": {"id_": "b079b4d9-36e0-4a32-8616-7751d682fba4", "embedding": null, "metadata": {"window": "Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. ", "original_text": "Auto-encoding variational bayes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b839629-7174-4c31-9119-f482194356e7", "node_type": "1", "metadata": {"window": "[29] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n", "original_text": "[30] Diederik P Kingma and Max Welling. "}, "hash": "0ce401db8a6c169af70a898a571f404fe7c2554223fc55206b8498f765ec33e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0081dec6-f249-43f3-bd14-69d5b5235ec7", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network. ", "original_text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n"}, "hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "class_name": "RelatedNodeInfo"}}, "hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "text": "Auto-encoding variational bayes. ", "start_char_idx": 41973, "end_char_idx": 42006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0081dec6-f249-43f3-bd14-69d5b5235ec7": {"__data__": {"id_": "0081dec6-f249-43f3-bd14-69d5b5235ec7", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network. ", "original_text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b079b4d9-36e0-4a32-8616-7751d682fba4", "node_type": "1", "metadata": {"window": "Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. ", "original_text": "Auto-encoding variational bayes. "}, "hash": "a4db1a91064c5bf2f1a0107cc2633efc3f599960c2d176a99d62ca2293e2e20e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f53aa019-93b4-42bb-9f9d-7621c3d2d638", "node_type": "1", "metadata": {"window": "[30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n", "original_text": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. "}, "hash": "176373b5bf59962e081828ba7fccc6d38eeb14bf184675a43b7df7e373bceb0d", "class_name": "RelatedNodeInfo"}}, "hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n", "start_char_idx": 42006, "end_char_idx": 42045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f53aa019-93b4-42bb-9f9d-7621c3d2d638": {"__data__": {"id_": "f53aa019-93b4-42bb-9f9d-7621c3d2d638", "embedding": null, "metadata": {"window": "[30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n", "original_text": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0081dec6-f249-43f3-bd14-69d5b5235ec7", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1312.6114 ,\n2013.\n [30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network. ", "original_text": "arXiv preprint arXiv:1312.6114 ,\n2013.\n"}, "hash": "265c4cbf4ffc70c11f309bf8cb27a9d4a22a4b040f000a198145fe95612e00ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5b6d707-f8d6-437a-b0ee-6e65b22d1d6f", "node_type": "1", "metadata": {"window": "Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. ", "original_text": "Photo-realistic single image super-\nresolution using a generative adversarial network. "}, "hash": "79e9f37405f945f31c222ac231542881438416477937cdc793770b5aeabd9f8a", "class_name": "RelatedNodeInfo"}}, "hash": "176373b5bf59962e081828ba7fccc6d38eeb14bf184675a43b7df7e373bceb0d", "text": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. ", "start_char_idx": 42157, "end_char_idx": 42325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5b6d707-f8d6-437a-b0ee-6e65b22d1d6f": {"__data__": {"id_": "b5b6d707-f8d6-437a-b0ee-6e65b22d1d6f", "embedding": null, "metadata": {"window": "Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. ", "original_text": "Photo-realistic single image super-\nresolution using a generative adversarial network. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f53aa019-93b4-42bb-9f9d-7621c3d2d638", "node_type": "1", "metadata": {"window": "[30] Diederik P Kingma and Max Welling.  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n", "original_text": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. "}, "hash": "176373b5bf59962e081828ba7fccc6d38eeb14bf184675a43b7df7e373bceb0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "605a62f0-1470-48db-84d7-09e73acea979", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries. ", "original_text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n"}, "hash": "c45e4d20037d2c2e32283fd18cf0a66cd4e8dff260c486c0c76ecab971a0a378", "class_name": "RelatedNodeInfo"}}, "hash": "79e9f37405f945f31c222ac231542881438416477937cdc793770b5aeabd9f8a", "text": "Photo-realistic single image super-\nresolution using a generative adversarial network. ", "start_char_idx": 42325, "end_char_idx": 42412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "605a62f0-1470-48db-84d7-09e73acea979": {"__data__": {"id_": "605a62f0-1470-48db-84d7-09e73acea979", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries. ", "original_text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5b6d707-f8d6-437a-b0ee-6e65b22d1d6f", "node_type": "1", "metadata": {"window": "Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. ", "original_text": "Photo-realistic single image super-\nresolution using a generative adversarial network. "}, "hash": "79e9f37405f945f31c222ac231542881438416477937cdc793770b5aeabd9f8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62d9324a-2a48-413c-ad9e-e7c3df34f682", "node_type": "1", "metadata": {"window": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n", "original_text": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. "}, "hash": "bb3bac21f72081b50e4d651915eb6acaa1babd25f2768c2de14b47ef7d54b30a", "class_name": "RelatedNodeInfo"}}, "hash": "c45e4d20037d2c2e32283fd18cf0a66cd4e8dff260c486c0c76ecab971a0a378", "text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n", "start_char_idx": 42412, "end_char_idx": 42518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62d9324a-2a48-413c-ad9e-e7c3df34f682": {"__data__": {"id_": "62d9324a-2a48-413c-ad9e-e7c3df34f682", "embedding": null, "metadata": {"window": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n", "original_text": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "605a62f0-1470-48db-84d7-09e73acea979", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1312.6114 ,\n2013.\n [31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries. ", "original_text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n"}, "hash": "c45e4d20037d2c2e32283fd18cf0a66cd4e8dff260c486c0c76ecab971a0a378", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6254492b-2c25-4944-bddb-3c428054e4f3", "node_type": "1", "metadata": {"window": "Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. ", "original_text": "Blind face\nrestoration via deep multi-scale component dictionaries. "}, "hash": "85969c8288d185d946750aa08b34b5f3bb8816425966bc3b5f62925adf60f659", "class_name": "RelatedNodeInfo"}}, "hash": "bb3bac21f72081b50e4d651915eb6acaa1babd25f2768c2de14b47ef7d54b30a", "text": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. ", "start_char_idx": 42518, "end_char_idx": 42609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6254492b-2c25-4944-bddb-3c428054e4f3": {"__data__": {"id_": "6254492b-2c25-4944-bddb-3c428054e4f3", "embedding": null, "metadata": {"window": "Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. ", "original_text": "Blind face\nrestoration via deep multi-scale component dictionaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62d9324a-2a48-413c-ad9e-e7c3df34f682", "node_type": "1", "metadata": {"window": "[31] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.  Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n", "original_text": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. "}, "hash": "bb3bac21f72081b50e4d651915eb6acaa1babd25f2768c2de14b47ef7d54b30a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18b0fbe9-ee13-41a5-acd7-aebedb0b924a", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. ", "original_text": "Springer International Publishing eBooks , 2020.\n"}, "hash": "6dfd4ee4d2c5615835533070f4f4f2b07e70d3871de95d3926b3c4a608aab992", "class_name": "RelatedNodeInfo"}}, "hash": "85969c8288d185d946750aa08b34b5f3bb8816425966bc3b5f62925adf60f659", "text": "Blind face\nrestoration via deep multi-scale component dictionaries. ", "start_char_idx": 42609, "end_char_idx": 42677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18b0fbe9-ee13-41a5-acd7-aebedb0b924a": {"__data__": {"id_": "18b0fbe9-ee13-41a5-acd7-aebedb0b924a", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. ", "original_text": "Springer International Publishing eBooks , 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6254492b-2c25-4944-bddb-3c428054e4f3", "node_type": "1", "metadata": {"window": "Photo-realistic single image super-\nresolution using a generative adversarial network.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. ", "original_text": "Blind face\nrestoration via deep multi-scale component dictionaries. "}, "hash": "85969c8288d185d946750aa08b34b5f3bb8816425966bc3b5f62925adf60f659", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fe65464-c60c-4a57-b787-af587b328b86", "node_type": "1", "metadata": {"window": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n", "original_text": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. "}, "hash": "aa8cebfbf9ed4e172f79a806ee59cf9753892f0af5b93c9ee8b83dcc8b3892bb", "class_name": "RelatedNodeInfo"}}, "hash": "6dfd4ee4d2c5615835533070f4f4f2b07e70d3871de95d3926b3c4a608aab992", "text": "Springer International Publishing eBooks , 2020.\n", "start_char_idx": 42677, "end_char_idx": 42726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fe65464-c60c-4a57-b787-af587b328b86": {"__data__": {"id_": "0fe65464-c60c-4a57-b787-af587b328b86", "embedding": null, "metadata": {"window": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n", "original_text": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18b0fbe9-ee13-41a5-acd7-aebedb0b924a", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4681\u20134690, 2017.\n [32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. ", "original_text": "Springer International Publishing eBooks , 2020.\n"}, "hash": "6dfd4ee4d2c5615835533070f4f4f2b07e70d3871de95d3926b3c4a608aab992", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f1d8bb1-5078-4933-b4ff-cf884ce36335", "node_type": "1", "metadata": {"window": "Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. ", "original_text": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. "}, "hash": "83e0554efe43c9e0330592e87b602c729f7c08e1bec10f3a9de30b4c464ebdce", "class_name": "RelatedNodeInfo"}}, "hash": "aa8cebfbf9ed4e172f79a806ee59cf9753892f0af5b93c9ee8b83dcc8b3892bb", "text": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. ", "start_char_idx": 42726, "end_char_idx": 42811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f1d8bb1-5078-4933-b4ff-cf884ce36335": {"__data__": {"id_": "2f1d8bb1-5078-4933-b4ff-cf884ce36335", "embedding": null, "metadata": {"window": "Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. ", "original_text": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fe65464-c60c-4a57-b787-af587b328b86", "node_type": "1", "metadata": {"window": "[32] Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang.  Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n", "original_text": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. "}, "hash": "aa8cebfbf9ed4e172f79a806ee59cf9753892f0af5b93c9ee8b83dcc8b3892bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "736f87bb-60b3-4a4e-a220-6bd02b32b2f4", "node_type": "1", "metadata": {"window": "Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration. ", "original_text": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n"}, "hash": "3c75bc336135edfdb69f05a06dc27e62024d69d0036e44dc7e3331dcc638d410", "class_name": "RelatedNodeInfo"}}, "hash": "83e0554efe43c9e0330592e87b602c729f7c08e1bec10f3a9de30b4c464ebdce", "text": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. ", "start_char_idx": 42811, "end_char_idx": 42907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "736f87bb-60b3-4a4e-a220-6bd02b32b2f4": {"__data__": {"id_": "736f87bb-60b3-4a4e-a220-6bd02b32b2f4", "embedding": null, "metadata": {"window": "Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration. ", "original_text": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f1d8bb1-5078-4933-b4ff-cf884ce36335", "node_type": "1", "metadata": {"window": "Blind face\nrestoration via deep multi-scale component dictionaries.  Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. ", "original_text": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion. "}, "hash": "83e0554efe43c9e0330592e87b602c729f7c08e1bec10f3a9de30b4c464ebdce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5a196dd-5e75-4d95-b426-b725e4b42cd7", "node_type": "1", "metadata": {"window": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n", "original_text": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. "}, "hash": "0193657e707aeff44489775183cb2b22943adfcd8cd575a6b9a176a54636c0c8", "class_name": "RelatedNodeInfo"}}, "hash": "3c75bc336135edfdb69f05a06dc27e62024d69d0036e44dc7e3331dcc638d410", "text": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n", "start_char_idx": 42907, "end_char_idx": 43017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5a196dd-5e75-4d95-b426-b725e4b42cd7": {"__data__": {"id_": "c5a196dd-5e75-4d95-b426-b725e4b42cd7", "embedding": null, "metadata": {"window": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n", "original_text": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "736f87bb-60b3-4a4e-a220-6bd02b32b2f4", "node_type": "1", "metadata": {"window": "Springer International Publishing eBooks , 2020.\n [33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration. ", "original_text": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n"}, "hash": "3c75bc336135edfdb69f05a06dc27e62024d69d0036e44dc7e3331dcc638d410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fce034b3-46ea-45aa-b0cf-93c9ae6aad02", "node_type": "1", "metadata": {"window": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. ", "original_text": "Learning warped\nguidance for blind face restoration. "}, "hash": "ff0d4aa0e333b129ebe3fb3766b22ed47d41bcf79e84b1be81fa5f6cc80fd57a", "class_name": "RelatedNodeInfo"}}, "hash": "0193657e707aeff44489775183cb2b22943adfcd8cd575a6b9a176a54636c0c8", "text": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. ", "start_char_idx": 43017, "end_char_idx": 43099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fce034b3-46ea-45aa-b0cf-93c9ae6aad02": {"__data__": {"id_": "fce034b3-46ea-45aa-b0cf-93c9ae6aad02", "embedding": null, "metadata": {"window": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. ", "original_text": "Learning warped\nguidance for blind face restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5a196dd-5e75-4d95-b426-b725e4b42cd7", "node_type": "1", "metadata": {"window": "[33] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo.  Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n", "original_text": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. "}, "hash": "0193657e707aeff44489775183cb2b22943adfcd8cd575a6b9a176a54636c0c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d7e015a-47bc-43a7-8865-ee6edc7459e8", "node_type": "1", "metadata": {"window": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration. ", "original_text": "Cornell University - arXiv , 2018.\n"}, "hash": "18b887820572351789fcff7a227b4201927b58b49a657a96381a436aabf54a89", "class_name": "RelatedNodeInfo"}}, "hash": "ff0d4aa0e333b129ebe3fb3766b22ed47d41bcf79e84b1be81fa5f6cc80fd57a", "text": "Learning warped\nguidance for blind face restoration. ", "start_char_idx": 43099, "end_char_idx": 43152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d7e015a-47bc-43a7-8865-ee6edc7459e8": {"__data__": {"id_": "8d7e015a-47bc-43a7-8865-ee6edc7459e8", "embedding": null, "metadata": {"window": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration. ", "original_text": "Cornell University - arXiv , 2018.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fce034b3-46ea-45aa-b0cf-93c9ae6aad02", "node_type": "1", "metadata": {"window": "Enhanced\nblind face restoration with multi-exemplar images and adaptive spatial feature fusion.  In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. ", "original_text": "Learning warped\nguidance for blind face restoration. "}, "hash": "ff0d4aa0e333b129ebe3fb3766b22ed47d41bcf79e84b1be81fa5f6cc80fd57a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "131b0a41-092a-4429-a258-50c9c0bf3758", "node_type": "1", "metadata": {"window": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n", "original_text": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. "}, "hash": "bed063e09dd5c18e573328dc711905d038b0d0306fb4583314ce8f5f7dbc8252", "class_name": "RelatedNodeInfo"}}, "hash": "18b887820572351789fcff7a227b4201927b58b49a657a96381a436aabf54a89", "text": "Cornell University - arXiv , 2018.\n", "start_char_idx": 43152, "end_char_idx": 43187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "131b0a41-092a-4429-a258-50c9c0bf3758": {"__data__": {"id_": "131b0a41-092a-4429-a258-50c9c0bf3758", "embedding": null, "metadata": {"window": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n", "original_text": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d7e015a-47bc-43a7-8865-ee6edc7459e8", "node_type": "1", "metadata": {"window": "In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2706\u20132715, 2020.\n [34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration. ", "original_text": "Cornell University - arXiv , 2018.\n"}, "hash": "18b887820572351789fcff7a227b4201927b58b49a657a96381a436aabf54a89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dc39263-552f-40cf-a602-34ae1ecd66ea", "node_type": "1", "metadata": {"window": "Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. ", "original_text": "Learning dual memory\ndictionaries for blind face restoration. "}, "hash": "faf45708b5a93e9eb2e6ea46601a925af3f0d780e51092f928ab599358026b18", "class_name": "RelatedNodeInfo"}}, "hash": "bed063e09dd5c18e573328dc711905d038b0d0306fb4583314ce8f5f7dbc8252", "text": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. ", "start_char_idx": 43187, "end_char_idx": 43266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dc39263-552f-40cf-a602-34ae1ecd66ea": {"__data__": {"id_": "8dc39263-552f-40cf-a602-34ae1ecd66ea", "embedding": null, "metadata": {"window": "Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. ", "original_text": "Learning dual memory\ndictionaries for blind face restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "131b0a41-092a-4429-a258-50c9c0bf3758", "node_type": "1", "metadata": {"window": "[34] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang.  Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n", "original_text": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo. "}, "hash": "bed063e09dd5c18e573328dc711905d038b0d0306fb4583314ce8f5f7dbc8252", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab7e104b-a645-4bfb-83c0-d165301336c5", "node_type": "1", "metadata": {"window": "Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer. ", "original_text": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n"}, "hash": "aeaf2d2f9705e3bd87e6bd62ef42714aeafdbe7a0c47656e58b92a8f1aae5bbb", "class_name": "RelatedNodeInfo"}}, "hash": "faf45708b5a93e9eb2e6ea46601a925af3f0d780e51092f928ab599358026b18", "text": "Learning dual memory\ndictionaries for blind face restoration. ", "start_char_idx": 43266, "end_char_idx": 43328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab7e104b-a645-4bfb-83c0-d165301336c5": {"__data__": {"id_": "ab7e104b-a645-4bfb-83c0-d165301336c5", "embedding": null, "metadata": {"window": "Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer. ", "original_text": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dc39263-552f-40cf-a602-34ae1ecd66ea", "node_type": "1", "metadata": {"window": "Learning warped\nguidance for blind face restoration.  Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. ", "original_text": "Learning dual memory\ndictionaries for blind face restoration. "}, "hash": "faf45708b5a93e9eb2e6ea46601a925af3f0d780e51092f928ab599358026b18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7777e25b-0fee-4725-b85d-24b6e116feaa", "node_type": "1", "metadata": {"window": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n", "original_text": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. "}, "hash": "69c2a3c96da41ea9697cca9a3e8d2fba2758cf48e351e68a7b5f46f568d60ed5", "class_name": "RelatedNodeInfo"}}, "hash": "aeaf2d2f9705e3bd87e6bd62ef42714aeafdbe7a0c47656e58b92a8f1aae5bbb", "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n", "start_char_idx": 43328, "end_char_idx": 43399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7777e25b-0fee-4725-b85d-24b6e116feaa": {"__data__": {"id_": "7777e25b-0fee-4725-b85d-24b6e116feaa", "embedding": null, "metadata": {"window": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n", "original_text": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab7e104b-a645-4bfb-83c0-d165301336c5", "node_type": "1", "metadata": {"window": "Cornell University - arXiv , 2018.\n [35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer. ", "original_text": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n"}, "hash": "aeaf2d2f9705e3bd87e6bd62ef42714aeafdbe7a0c47656e58b92a8f1aae5bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d33ab1e-0ad8-4ea8-8206-804e1cae8056", "node_type": "1", "metadata": {"window": "Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. ", "original_text": "Swinir: Image\nrestoration using swin transformer. "}, "hash": "169fe9850095b7734f67d9605eb1677c2da7f69759c43fcd1888b163f86953e7", "class_name": "RelatedNodeInfo"}}, "hash": "69c2a3c96da41ea9697cca9a3e8d2fba2758cf48e351e68a7b5f46f568d60ed5", "text": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. ", "start_char_idx": 43399, "end_char_idx": 43488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d33ab1e-0ad8-4ea8-8206-804e1cae8056": {"__data__": {"id_": "6d33ab1e-0ad8-4ea8-8206-804e1cae8056", "embedding": null, "metadata": {"window": "Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. ", "original_text": "Swinir: Image\nrestoration using swin transformer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7777e25b-0fee-4725-b85d-24b6e116feaa", "node_type": "1", "metadata": {"window": "[35] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang, and Wangmeng Zuo.  Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n", "original_text": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. "}, "hash": "69c2a3c96da41ea9697cca9a3e8d2fba2758cf48e351e68a7b5f46f568d60ed5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1eebe90-13ca-43aa-abd2-1f7b3cd4e43d", "node_type": "1", "metadata": {"window": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond. ", "original_text": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n"}, "hash": "22d09d20d3a69c8321d1240fad93a4a45eea475ea8e575ec1208df7ad89a3678", "class_name": "RelatedNodeInfo"}}, "hash": "169fe9850095b7734f67d9605eb1677c2da7f69759c43fcd1888b163f86953e7", "text": "Swinir: Image\nrestoration using swin transformer. ", "start_char_idx": 43488, "end_char_idx": 43538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1eebe90-13ca-43aa-abd2-1f7b3cd4e43d": {"__data__": {"id_": "a1eebe90-13ca-43aa-abd2-1f7b3cd4e43d", "embedding": null, "metadata": {"window": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond. ", "original_text": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d33ab1e-0ad8-4ea8-8206-804e1cae8056", "node_type": "1", "metadata": {"window": "Learning dual memory\ndictionaries for blind face restoration.  IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. ", "original_text": "Swinir: Image\nrestoration using swin transformer. "}, "hash": "169fe9850095b7734f67d9605eb1677c2da7f69759c43fcd1888b163f86953e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f2325c4-636e-4558-8043-5c0f46755246", "node_type": "1", "metadata": {"window": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n", "original_text": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. "}, "hash": "bdedcb03cc9bbdfdba0a51437853cd953eb6a742502d6686eab9833bf7eb86c5", "class_name": "RelatedNodeInfo"}}, "hash": "22d09d20d3a69c8321d1240fad93a4a45eea475ea8e575ec1208df7ad89a3678", "text": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n", "start_char_idx": 43538, "end_char_idx": 43638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f2325c4-636e-4558-8043-5c0f46755246": {"__data__": {"id_": "5f2325c4-636e-4558-8043-5c0f46755246", "embedding": null, "metadata": {"window": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n", "original_text": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1eebe90-13ca-43aa-abd2-1f7b3cd4e43d", "node_type": "1", "metadata": {"window": "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n2022.\n [36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond. ", "original_text": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n"}, "hash": "22d09d20d3a69c8321d1240fad93a4a45eea475ea8e575ec1208df7ad89a3678", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9862b7b-7fd6-4cde-85ff-8816dc093853", "node_type": "1", "metadata": {"window": "Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. ", "original_text": "Blind image super-resolution: A survey and\nbeyond. "}, "hash": "fa6991820fafdca763f6f0c776fb0c6b0ec351cc00f3b1714aaa2ba485dc4bdf", "class_name": "RelatedNodeInfo"}}, "hash": "bdedcb03cc9bbdfdba0a51437853cd953eb6a742502d6686eab9833bf7eb86c5", "text": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. ", "start_char_idx": 43638, "end_char_idx": 43700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9862b7b-7fd6-4cde-85ff-8816dc093853": {"__data__": {"id_": "b9862b7b-7fd6-4cde-85ff-8816dc093853", "embedding": null, "metadata": {"window": "Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. ", "original_text": "Blind image super-resolution: A survey and\nbeyond. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f2325c4-636e-4558-8043-5c0f46755246", "node_type": "1", "metadata": {"window": "[36] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.  Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n", "original_text": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. "}, "hash": "bdedcb03cc9bbdfdba0a51437853cd953eb6a742502d6686eab9833bf7eb86c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52479432-9e3f-4e5e-88c3-d7c93332aafd", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows. ", "original_text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n"}, "hash": "33ea751838b0e3db451337a86d161a5e39cf99ef004d6f18730f6a851ab1caba", "class_name": "RelatedNodeInfo"}}, "hash": "fa6991820fafdca763f6f0c776fb0c6b0ec351cc00f3b1714aaa2ba485dc4bdf", "text": "Blind image super-resolution: A survey and\nbeyond. ", "start_char_idx": 43700, "end_char_idx": 43751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52479432-9e3f-4e5e-88c3-d7c93332aafd": {"__data__": {"id_": "52479432-9e3f-4e5e-88c3-d7c93332aafd", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows. ", "original_text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9862b7b-7fd6-4cde-85ff-8816dc093853", "node_type": "1", "metadata": {"window": "Swinir: Image\nrestoration using swin transformer.  In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. ", "original_text": "Blind image super-resolution: A survey and\nbeyond. "}, "hash": "fa6991820fafdca763f6f0c776fb0c6b0ec351cc00f3b1714aaa2ba485dc4bdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e85de838-213c-4589-9408-108afb582f89", "node_type": "1", "metadata": {"window": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n", "original_text": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. "}, "hash": "fd0cfffef06217fa27503181ea25e45ff1a4629e14ccb724d543a7b8ce6f94dd", "class_name": "RelatedNodeInfo"}}, "hash": "33ea751838b0e3db451337a86d161a5e39cf99ef004d6f18730f6a851ab1caba", "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n", "start_char_idx": 43751, "end_char_idx": 43822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e85de838-213c-4589-9408-108afb582f89": {"__data__": {"id_": "e85de838-213c-4589-9408-108afb582f89", "embedding": null, "metadata": {"window": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n", "original_text": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52479432-9e3f-4e5e-88c3-d7c93332aafd", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF international conference on computer\nvision , pages 1833\u20131844, 2021.\n [37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows. ", "original_text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n"}, "hash": "33ea751838b0e3db451337a86d161a5e39cf99ef004d6f18730f6a851ab1caba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b04ca275-a026-4749-911a-ec6466ba92b0", "node_type": "1", "metadata": {"window": "Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. ", "original_text": "Swin\ntransformer: Hierarchical vision transformer using shifted windows. "}, "hash": "6ebaed164b70a2af0f60f6d6b0bf96a06f8f08669ddc4b3ecbee88a8620b38cb", "class_name": "RelatedNodeInfo"}}, "hash": "fd0cfffef06217fa27503181ea25e45ff1a4629e14ccb724d543a7b8ce6f94dd", "text": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. ", "start_char_idx": 43822, "end_char_idx": 43919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b04ca275-a026-4749-911a-ec6466ba92b0": {"__data__": {"id_": "b04ca275-a026-4749-911a-ec6466ba92b0", "embedding": null, "metadata": {"window": "Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. ", "original_text": "Swin\ntransformer: Hierarchical vision transformer using shifted windows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e85de838-213c-4589-9408-108afb582f89", "node_type": "1", "metadata": {"window": "[37] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.  Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n", "original_text": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. "}, "hash": "fd0cfffef06217fa27503181ea25e45ff1a4629e14ccb724d543a7b8ce6f94dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e5dd9c1-5d4d-4cd3-b639-5a24f408e42c", "node_type": "1", "metadata": {"window": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild. ", "original_text": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n"}, "hash": "f3cf11ff0ea9bd5d39e011336bd5da3c9a779b76cc0acd548bbeccdfd6e6976f", "class_name": "RelatedNodeInfo"}}, "hash": "6ebaed164b70a2af0f60f6d6b0bf96a06f8f08669ddc4b3ecbee88a8620b38cb", "text": "Swin\ntransformer: Hierarchical vision transformer using shifted windows. ", "start_char_idx": 43919, "end_char_idx": 43992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e5dd9c1-5d4d-4cd3-b639-5a24f408e42c": {"__data__": {"id_": "6e5dd9c1-5d4d-4cd3-b639-5a24f408e42c", "embedding": null, "metadata": {"window": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild. ", "original_text": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b04ca275-a026-4749-911a-ec6466ba92b0", "node_type": "1", "metadata": {"window": "Blind image super-resolution: A survey and\nbeyond.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. ", "original_text": "Swin\ntransformer: Hierarchical vision transformer using shifted windows. "}, "hash": "6ebaed164b70a2af0f60f6d6b0bf96a06f8f08669ddc4b3ecbee88a8620b38cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "192c098d-4183-405d-99e2-f669b6f15d14", "node_type": "1", "metadata": {"window": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n", "original_text": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. "}, "hash": "97187db859e5e08093b64e8a5e6bfe6537a4b8e66171ec9e4c392d93e91acc83", "class_name": "RelatedNodeInfo"}}, "hash": "f3cf11ff0ea9bd5d39e011336bd5da3c9a779b76cc0acd548bbeccdfd6e6976f", "text": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n", "start_char_idx": 43992, "end_char_idx": 44094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "192c098d-4183-405d-99e2-f669b6f15d14": {"__data__": {"id_": "192c098d-4183-405d-99e2-f669b6f15d14", "embedding": null, "metadata": {"window": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n", "original_text": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e5dd9c1-5d4d-4cd3-b639-5a24f408e42c", "node_type": "1", "metadata": {"window": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild. ", "original_text": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n"}, "hash": "f3cf11ff0ea9bd5d39e011336bd5da3c9a779b76cc0acd548bbeccdfd6e6976f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c377869-376e-4f2b-a063-033b930e9b75", "node_type": "1", "metadata": {"window": "Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. ", "original_text": "Deep learning face attributes in the wild. "}, "hash": "a59aee1ded419905f0470e856fd103ffe3e37e2eb9459bd4970de33c6ba07303", "class_name": "RelatedNodeInfo"}}, "hash": "97187db859e5e08093b64e8a5e6bfe6537a4b8e66171ec9e4c392d93e91acc83", "text": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. ", "start_char_idx": 44094, "end_char_idx": 44152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c377869-376e-4f2b-a063-033b930e9b75": {"__data__": {"id_": "0c377869-376e-4f2b-a063-033b930e9b75", "embedding": null, "metadata": {"window": "Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. ", "original_text": "Deep learning face attributes in the wild. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "192c098d-4183-405d-99e2-f669b6f15d14", "node_type": "1", "metadata": {"window": "[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n", "original_text": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. "}, "hash": "97187db859e5e08093b64e8a5e6bfe6537a4b8e66171ec9e4c392d93e91acc83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9870db52-5644-48fd-85ed-43e64d1f2e95", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. ", "original_text": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n"}, "hash": "75f1ea27cd54c2aac7e09d0ec10f166d5da9e1f0dbf39215f61e82a86e966606", "class_name": "RelatedNodeInfo"}}, "hash": "a59aee1ded419905f0470e856fd103ffe3e37e2eb9459bd4970de33c6ba07303", "text": "Deep learning face attributes in the wild. ", "start_char_idx": 44152, "end_char_idx": 44195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9870db52-5644-48fd-85ed-43e64d1f2e95": {"__data__": {"id_": "9870db52-5644-48fd-85ed-43e64d1f2e95", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. ", "original_text": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c377869-376e-4f2b-a063-033b930e9b75", "node_type": "1", "metadata": {"window": "Swin\ntransformer: Hierarchical vision transformer using shifted windows.  In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. ", "original_text": "Deep learning face attributes in the wild. "}, "hash": "a59aee1ded419905f0470e856fd103ffe3e37e2eb9459bd4970de33c6ba07303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72f842bf-576a-454b-b51d-d9be724b0316", "node_type": "1", "metadata": {"window": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n", "original_text": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. "}, "hash": "defbc0a54d329e4002fd4f1a56c7e2b2214ad3edaf47abeda494fca043689b44", "class_name": "RelatedNodeInfo"}}, "hash": "75f1ea27cd54c2aac7e09d0ec10f166d5da9e1f0dbf39215f61e82a86e966606", "text": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n", "start_char_idx": 44195, "end_char_idx": 44281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72f842bf-576a-454b-b51d-d9be724b0316": {"__data__": {"id_": "72f842bf-576a-454b-b51d-d9be724b0316", "embedding": null, "metadata": {"window": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n", "original_text": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9870db52-5644-48fd-85ed-43e64d1f2e95", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022, 2021.\n [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. ", "original_text": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n"}, "hash": "75f1ea27cd54c2aac7e09d0ec10f166d5da9e1f0dbf39215f61e82a86e966606", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c35ea646-50c9-4f6b-ab17-165a8018f3d9", "node_type": "1", "metadata": {"window": "Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. ", "original_text": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. "}, "hash": "b26f0e375f2a077255b1be87ee444326046d1a201ba1b1ec2781c8edd7889b0f", "class_name": "RelatedNodeInfo"}}, "hash": "defbc0a54d329e4002fd4f1a56c7e2b2214ad3edaf47abeda494fca043689b44", "text": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. ", "start_char_idx": 44281, "end_char_idx": 44361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c35ea646-50c9-4f6b-ab17-165a8018f3d9": {"__data__": {"id_": "c35ea646-50c9-4f6b-ab17-165a8018f3d9", "embedding": null, "metadata": {"window": "Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. ", "original_text": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72f842bf-576a-454b-b51d-d9be724b0316", "node_type": "1", "metadata": {"window": "[39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.  Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n", "original_text": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. "}, "hash": "defbc0a54d329e4002fd4f1a56c7e2b2214ad3edaf47abeda494fca043689b44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d165fb2-1322-4ee3-a1e3-9d6028a6c6b2", "node_type": "1", "metadata": {"window": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks. ", "original_text": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n"}, "hash": "85e2df44c0595de04af4a66795fb8cb50f5be76833d804dd6726a4b2eed92e37", "class_name": "RelatedNodeInfo"}}, "hash": "b26f0e375f2a077255b1be87ee444326046d1a201ba1b1ec2781c8edd7889b0f", "text": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. ", "start_char_idx": 44361, "end_char_idx": 44452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d165fb2-1322-4ee3-a1e3-9d6028a6c6b2": {"__data__": {"id_": "9d165fb2-1322-4ee3-a1e3-9d6028a6c6b2", "embedding": null, "metadata": {"window": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks. ", "original_text": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c35ea646-50c9-4f6b-ab17-165a8018f3d9", "node_type": "1", "metadata": {"window": "Deep learning face attributes in the wild.  In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. ", "original_text": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models. "}, "hash": "b26f0e375f2a077255b1be87ee444326046d1a201ba1b1ec2781c8edd7889b0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf8a4d5e-d2ed-479b-840a-e9d7bf0d3415", "node_type": "1", "metadata": {"window": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n", "original_text": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. "}, "hash": "4dc65aaf842dc10bd58b129abacd61ac0cae35f0e7abaaf900276a143aa9603f", "class_name": "RelatedNodeInfo"}}, "hash": "85e2df44c0595de04af4a66795fb8cb50f5be76833d804dd6726a4b2eed92e37", "text": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n", "start_char_idx": 44452, "end_char_idx": 44562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf8a4d5e-d2ed-479b-840a-e9d7bf0d3415": {"__data__": {"id_": "cf8a4d5e-d2ed-479b-840a-e9d7bf0d3415", "embedding": null, "metadata": {"window": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n", "original_text": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d165fb2-1322-4ee3-a1e3-9d6028a6c6b2", "node_type": "1", "metadata": {"window": "In\nProceedings of International Conference on Computer Vision (ICCV) , December 2015.\n [40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks. ", "original_text": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n"}, "hash": "85e2df44c0595de04af4a66795fb8cb50f5be76833d804dd6726a4b2eed92e37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c101d93b-0b90-4c72-9878-fd8a52f005f3", "node_type": "1", "metadata": {"window": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. ", "original_text": "Spectral normalization for\ngenerative adversarial networks. "}, "hash": "129ae0fa18e094b7ac04cf82b1bc1b8383ca3fad3a4c83315962f100494eecbb", "class_name": "RelatedNodeInfo"}}, "hash": "4dc65aaf842dc10bd58b129abacd61ac0cae35f0e7abaaf900276a143aa9603f", "text": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. ", "start_char_idx": 44562, "end_char_idx": 44640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c101d93b-0b90-4c72-9878-fd8a52f005f3": {"__data__": {"id_": "c101d93b-0b90-4c72-9878-fd8a52f005f3", "embedding": null, "metadata": {"window": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. ", "original_text": "Spectral normalization for\ngenerative adversarial networks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf8a4d5e-d2ed-479b-840a-e9d7bf0d3415", "node_type": "1", "metadata": {"window": "[40] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.  Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n", "original_text": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. "}, "hash": "4dc65aaf842dc10bd58b129abacd61ac0cae35f0e7abaaf900276a143aa9603f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd04a337-7655-44f2-be8f-3dc94bffd1da", "node_type": "1", "metadata": {"window": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. ", "original_text": "arXiv preprint arXiv:1802.05957 , 2018.\n"}, "hash": "cb1970dc7f1831ad5e853c223d1ad6df60392f05cda7a69f676f6b81c5bfb3c2", "class_name": "RelatedNodeInfo"}}, "hash": "129ae0fa18e094b7ac04cf82b1bc1b8383ca3fad3a4c83315962f100494eecbb", "text": "Spectral normalization for\ngenerative adversarial networks. ", "start_char_idx": 44640, "end_char_idx": 44700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd04a337-7655-44f2-be8f-3dc94bffd1da": {"__data__": {"id_": "bd04a337-7655-44f2-be8f-3dc94bffd1da", "embedding": null, "metadata": {"window": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. ", "original_text": "arXiv preprint arXiv:1802.05957 , 2018.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c101d93b-0b90-4c72-9878-fd8a52f005f3", "node_type": "1", "metadata": {"window": "Pulse: Self-supervised\nphoto upsampling via latent space exploration of generative models.  In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. ", "original_text": "Spectral normalization for\ngenerative adversarial networks. "}, "hash": "129ae0fa18e094b7ac04cf82b1bc1b8383ca3fad3a4c83315962f100494eecbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d378ed94-168c-4033-aa9a-939898fa877a", "node_type": "1", "metadata": {"window": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n", "original_text": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. "}, "hash": "ebd7eb6dd76000be4e6fa4319c6aed9dced465ce1d97489a526c88d7a59005a9", "class_name": "RelatedNodeInfo"}}, "hash": "cb1970dc7f1831ad5e853c223d1ad6df60392f05cda7a69f676f6b81c5bfb3c2", "text": "arXiv preprint arXiv:1802.05957 , 2018.\n", "start_char_idx": 44700, "end_char_idx": 44740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d378ed94-168c-4033-aa9a-939898fa877a": {"__data__": {"id_": "d378ed94-168c-4033-aa9a-939898fa877a", "embedding": null, "metadata": {"window": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n", "original_text": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd04a337-7655-44f2-be8f-3dc94bffd1da", "node_type": "1", "metadata": {"window": "In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition , pages 2437\u20132445, 2020.\n 13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. ", "original_text": "arXiv preprint arXiv:1802.05957 , 2018.\n"}, "hash": "cb1970dc7f1831ad5e853c223d1ad6df60392f05cda7a69f676f6b81c5bfb3c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6850df06-973b-427e-9580-85fec0c08c77", "node_type": "1", "metadata": {"window": "Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal. ", "original_text": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. "}, "hash": "5eccd2828100eec3e1c552041a70ee9282f6740fd88eb8731cd6a01edc5f59e4", "class_name": "RelatedNodeInfo"}}, "hash": "ebd7eb6dd76000be4e6fa4319c6aed9dced465ce1d97489a526c88d7a59005a9", "text": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. ", "start_char_idx": 44740, "end_char_idx": 44835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6850df06-973b-427e-9580-85fec0c08c77": {"__data__": {"id_": "6850df06-973b-427e-9580-85fec0c08c77", "embedding": null, "metadata": {"window": "Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal. ", "original_text": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d378ed94-168c-4033-aa9a-939898fa877a", "node_type": "1", "metadata": {"window": "13\n\n[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n", "original_text": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. "}, "hash": "ebd7eb6dd76000be4e6fa4319c6aed9dced465ce1d97489a526c88d7a59005a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f871db1-fb35-409c-916f-0d70b15c34b8", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models. ", "original_text": "arXiv\npreprint arXiv:2302.08453 , 2023.\n"}, "hash": "0b8fa98a208eb01c203e53539e583ba9da3d1facf4e80a4911ed4d8a81b62b7b", "class_name": "RelatedNodeInfo"}}, "hash": "5eccd2828100eec3e1c552041a70ee9282f6740fd88eb8731cd6a01edc5f59e4", "text": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. ", "start_char_idx": 44835, "end_char_idx": 44940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f871db1-fb35-409c-916f-0d70b15c34b8": {"__data__": {"id_": "9f871db1-fb35-409c-916f-0d70b15c34b8", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models. ", "original_text": "arXiv\npreprint arXiv:2302.08453 , 2023.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6850df06-973b-427e-9580-85fec0c08c77", "node_type": "1", "metadata": {"window": "Spectral normalization for\ngenerative adversarial networks.  arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal. ", "original_text": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. "}, "hash": "5eccd2828100eec3e1c552041a70ee9282f6740fd88eb8731cd6a01edc5f59e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3c3a124-2959-4465-aed4-6c0f43127a3b", "node_type": "1", "metadata": {"window": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171. ", "original_text": "[43] Alexander Quinn Nichol and Prafulla Dhariwal. "}, "hash": "f35f8021851ae78a2ce2d45a96cf571ab3f803da050ec8da8040a35a729239b7", "class_name": "RelatedNodeInfo"}}, "hash": "0b8fa98a208eb01c203e53539e583ba9da3d1facf4e80a4911ed4d8a81b62b7b", "text": "arXiv\npreprint arXiv:2302.08453 , 2023.\n", "start_char_idx": 44940, "end_char_idx": 44980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3c3a124-2959-4465-aed4-6c0f43127a3b": {"__data__": {"id_": "a3c3a124-2959-4465-aed4-6c0f43127a3b", "embedding": null, "metadata": {"window": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171. ", "original_text": "[43] Alexander Quinn Nichol and Prafulla Dhariwal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f871db1-fb35-409c-916f-0d70b15c34b8", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1802.05957 , 2018.\n [42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models. ", "original_text": "arXiv\npreprint arXiv:2302.08453 , 2023.\n"}, "hash": "0b8fa98a208eb01c203e53539e583ba9da3d1facf4e80a4911ed4d8a81b62b7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9308b161-9f6b-46fe-a1ac-7ede7c1579ca", "node_type": "1", "metadata": {"window": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n", "original_text": "Improved denoising diffusion probabilistic models. "}, "hash": "1d1f9f82268c52585ae55c94f67eaaec229e5694e02afdb3ed5caff4f3cf6297", "class_name": "RelatedNodeInfo"}}, "hash": "f35f8021851ae78a2ce2d45a96cf571ab3f803da050ec8da8040a35a729239b7", "text": "[43] Alexander Quinn Nichol and Prafulla Dhariwal. ", "start_char_idx": 44980, "end_char_idx": 45031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9308b161-9f6b-46fe-a1ac-7ede7c1579ca": {"__data__": {"id_": "9308b161-9f6b-46fe-a1ac-7ede7c1579ca", "embedding": null, "metadata": {"window": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n", "original_text": "Improved denoising diffusion probabilistic models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3c3a124-2959-4465-aed4-6c0f43127a3b", "node_type": "1", "metadata": {"window": "[42] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.  T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171. ", "original_text": "[43] Alexander Quinn Nichol and Prafulla Dhariwal. "}, "hash": "f35f8021851ae78a2ce2d45a96cf571ab3f803da050ec8da8040a35a729239b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a316cf69-a59e-406c-b749-aa68e1821d77", "node_type": "1", "metadata": {"window": "arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. ", "original_text": "In\nInternational Conference on Machine Learning , pages 8162\u20138171. "}, "hash": "569b09f53b4b7fbcf02aced3d338bd84ff072c569530feab5f8de16e626b6c49", "class_name": "RelatedNodeInfo"}}, "hash": "1d1f9f82268c52585ae55c94f67eaaec229e5694e02afdb3ed5caff4f3cf6297", "text": "Improved denoising diffusion probabilistic models. ", "start_char_idx": 45031, "end_char_idx": 45082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a316cf69-a59e-406c-b749-aa68e1821d77": {"__data__": {"id_": "a316cf69-a59e-406c-b749-aa68e1821d77", "embedding": null, "metadata": {"window": "arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. ", "original_text": "In\nInternational Conference on Machine Learning , pages 8162\u20138171. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9308b161-9f6b-46fe-a1ac-7ede7c1579ca", "node_type": "1", "metadata": {"window": "T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.  arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n", "original_text": "Improved denoising diffusion probabilistic models. "}, "hash": "1d1f9f82268c52585ae55c94f67eaaec229e5694e02afdb3ed5caff4f3cf6297", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da993a47-0a10-4936-8d7b-030e9db858e9", "node_type": "1", "metadata": {"window": "[43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation. ", "original_text": "PMLR, 2021.\n"}, "hash": "28df913563e6d4e39d0bf6de89cbe714b2d5c75e0a78d36e1b0811d34c5a8042", "class_name": "RelatedNodeInfo"}}, "hash": "569b09f53b4b7fbcf02aced3d338bd84ff072c569530feab5f8de16e626b6c49", "text": "In\nInternational Conference on Machine Learning , pages 8162\u20138171. ", "start_char_idx": 45082, "end_char_idx": 45149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da993a47-0a10-4936-8d7b-030e9db858e9": {"__data__": {"id_": "da993a47-0a10-4936-8d7b-030e9db858e9", "embedding": null, "metadata": {"window": "[43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation. ", "original_text": "PMLR, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a316cf69-a59e-406c-b749-aa68e1821d77", "node_type": "1", "metadata": {"window": "arXiv\npreprint arXiv:2302.08453 , 2023.\n [43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. ", "original_text": "In\nInternational Conference on Machine Learning , pages 8162\u20138171. "}, "hash": "569b09f53b4b7fbcf02aced3d338bd84ff072c569530feab5f8de16e626b6c49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fc02bbd-23b3-4785-99d6-0f838f790c87", "node_type": "1", "metadata": {"window": "Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n", "original_text": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. "}, "hash": "806a07b19106e35a76fb0e53199f6819a7fcfb0f88fd537d3ecc1d8ebeb1e304", "class_name": "RelatedNodeInfo"}}, "hash": "28df913563e6d4e39d0bf6de89cbe714b2d5c75e0a78d36e1b0811d34c5a8042", "text": "PMLR, 2021.\n", "start_char_idx": 45149, "end_char_idx": 45161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8fc02bbd-23b3-4785-99d6-0f838f790c87": {"__data__": {"id_": "8fc02bbd-23b3-4785-99d6-0f838f790c87", "embedding": null, "metadata": {"window": "Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n", "original_text": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da993a47-0a10-4936-8d7b-030e9db858e9", "node_type": "1", "metadata": {"window": "[43] Alexander Quinn Nichol and Prafulla Dhariwal.  Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation. ", "original_text": "PMLR, 2021.\n"}, "hash": "28df913563e6d4e39d0bf6de89cbe714b2d5c75e0a78d36e1b0811d34c5a8042", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2fdf670-04ab-41bd-9398-bfd0769b5d14", "node_type": "1", "metadata": {"window": "In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. ", "original_text": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation. "}, "hash": "022bb0dc550f57f66cdf62fc696adb27a950b9f7b8d116b55649b4a746f8485a", "class_name": "RelatedNodeInfo"}}, "hash": "806a07b19106e35a76fb0e53199f6819a7fcfb0f88fd537d3ecc1d8ebeb1e304", "text": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. ", "start_char_idx": 45161, "end_char_idx": 45244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2fdf670-04ab-41bd-9398-bfd0769b5d14": {"__data__": {"id_": "e2fdf670-04ab-41bd-9398-bfd0769b5d14", "embedding": null, "metadata": {"window": "In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. ", "original_text": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fc02bbd-23b3-4785-99d6-0f838f790c87", "node_type": "1", "metadata": {"window": "Improved denoising diffusion probabilistic models.  In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n", "original_text": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. "}, "hash": "806a07b19106e35a76fb0e53199f6819a7fcfb0f88fd537d3ecc1d8ebeb1e304", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8a00d1f-9eff-40d8-ab37-5d8e8f5549da", "node_type": "1", "metadata": {"window": "PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents. ", "original_text": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n"}, "hash": "1b619f95e6b56215a8553b9e9d1e8834142f81828f039ec29e58d673a8e9dec5", "class_name": "RelatedNodeInfo"}}, "hash": "022bb0dc550f57f66cdf62fc696adb27a950b9f7b8d116b55649b4a746f8485a", "text": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation. ", "start_char_idx": 45244, "end_char_idx": 45327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8a00d1f-9eff-40d8-ab37-5d8e8f5549da": {"__data__": {"id_": "b8a00d1f-9eff-40d8-ab37-5d8e8f5549da", "embedding": null, "metadata": {"window": "PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents. ", "original_text": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2fdf670-04ab-41bd-9398-bfd0769b5d14", "node_type": "1", "metadata": {"window": "In\nInternational Conference on Machine Learning , pages 8162\u20138171.  PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. ", "original_text": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation. "}, "hash": "022bb0dc550f57f66cdf62fc696adb27a950b9f7b8d116b55649b4a746f8485a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57e1aa90-3eae-4e7a-a625-ab8465ade852", "node_type": "1", "metadata": {"window": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n", "original_text": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. "}, "hash": "8a45d0db2e7c0ed3bbb9a4256ca82f71771cf2e83e8d95d8fd95eaac27a2a738", "class_name": "RelatedNodeInfo"}}, "hash": "1b619f95e6b56215a8553b9e9d1e8834142f81828f039ec29e58d673a8e9dec5", "text": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n", "start_char_idx": 45327, "end_char_idx": 45416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57e1aa90-3eae-4e7a-a625-ab8465ade852": {"__data__": {"id_": "57e1aa90-3eae-4e7a-a625-ab8465ade852", "embedding": null, "metadata": {"window": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n", "original_text": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8a00d1f-9eff-40d8-ab37-5d8e8f5549da", "node_type": "1", "metadata": {"window": "PMLR, 2021.\n [44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents. ", "original_text": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n"}, "hash": "1b619f95e6b56215a8553b9e9d1e8834142f81828f039ec29e58d673a8e9dec5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad8ba3d7-3a80-4b06-b50a-43b85cfca8af", "node_type": "1", "metadata": {"window": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. ", "original_text": "Hierarchical text-conditional\nimage generation with clip latents. "}, "hash": "7ea0ea61655a0dfcd5208208176571aca393893c7532d05a9bad06aab9fa2d0c", "class_name": "RelatedNodeInfo"}}, "hash": "8a45d0db2e7c0ed3bbb9a4256ca82f71771cf2e83e8d95d8fd95eaac27a2a738", "text": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. ", "start_char_idx": 45416, "end_char_idx": 45494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad8ba3d7-3a80-4b06-b50a-43b85cfca8af": {"__data__": {"id_": "ad8ba3d7-3a80-4b06-b50a-43b85cfca8af", "embedding": null, "metadata": {"window": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. ", "original_text": "Hierarchical text-conditional\nimage generation with clip latents. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57e1aa90-3eae-4e7a-a625-ab8465ade852", "node_type": "1", "metadata": {"window": "[44] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo.  Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n", "original_text": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. "}, "hash": "8a45d0db2e7c0ed3bbb9a4256ca82f71771cf2e83e8d95d8fd95eaac27a2a738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8152773-d4e7-49da-846e-ec08e80d78ef", "node_type": "1", "metadata": {"window": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models. ", "original_text": "arXiv preprint arXiv:2204.06125 , 2022.\n"}, "hash": "1aec701cf198015d1c6e3f7fafbfb75a6334b50d6554aedd125d6768fa9a8f32", "class_name": "RelatedNodeInfo"}}, "hash": "7ea0ea61655a0dfcd5208208176571aca393893c7532d05a9bad06aab9fa2d0c", "text": "Hierarchical text-conditional\nimage generation with clip latents. ", "start_char_idx": 45494, "end_char_idx": 45560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8152773-d4e7-49da-846e-ec08e80d78ef": {"__data__": {"id_": "d8152773-d4e7-49da-846e-ec08e80d78ef", "embedding": null, "metadata": {"window": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models. ", "original_text": "arXiv preprint arXiv:2204.06125 , 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad8ba3d7-3a80-4b06-b50a-43b85cfca8af", "node_type": "1", "metadata": {"window": "Exploiting deep\ngenerative prior for versatile image restoration and manipulation.  IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. ", "original_text": "Hierarchical text-conditional\nimage generation with clip latents. "}, "hash": "7ea0ea61655a0dfcd5208208176571aca393893c7532d05a9bad06aab9fa2d0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd91dd22-2003-47cb-a6c0-a0c18dac6199", "node_type": "1", "metadata": {"window": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n", "original_text": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. "}, "hash": "6e6ad1e00104b5509864079778c69bad8140775953b6735dd2421336260e4817", "class_name": "RelatedNodeInfo"}}, "hash": "1aec701cf198015d1c6e3f7fafbfb75a6334b50d6554aedd125d6768fa9a8f32", "text": "arXiv preprint arXiv:2204.06125 , 2022.\n", "start_char_idx": 45560, "end_char_idx": 45600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd91dd22-2003-47cb-a6c0-a0c18dac6199": {"__data__": {"id_": "fd91dd22-2003-47cb-a6c0-a0c18dac6199", "embedding": null, "metadata": {"window": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n", "original_text": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8152773-d4e7-49da-846e-ec08e80d78ef", "node_type": "1", "metadata": {"window": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 44(11):7474\u20137489, 2021.\n [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models. ", "original_text": "arXiv preprint arXiv:2204.06125 , 2022.\n"}, "hash": "1aec701cf198015d1c6e3f7fafbfb75a6334b50d6554aedd125d6768fa9a8f32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a3212bb-80c6-4ff5-a769-3fa5f9478ccb", "node_type": "1", "metadata": {"window": "Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. ", "original_text": "High-resolution\nimage synthesis with latent diffusion models. "}, "hash": "d1cbe82e99a35e089670559d838d30f57159eb96088995b0962a03894ae09b5c", "class_name": "RelatedNodeInfo"}}, "hash": "6e6ad1e00104b5509864079778c69bad8140775953b6735dd2421336260e4817", "text": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. ", "start_char_idx": 45600, "end_char_idx": 45687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a3212bb-80c6-4ff5-a769-3fa5f9478ccb": {"__data__": {"id_": "0a3212bb-80c6-4ff5-a769-3fa5f9478ccb", "embedding": null, "metadata": {"window": "Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. ", "original_text": "High-resolution\nimage synthesis with latent diffusion models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd91dd22-2003-47cb-a6c0-a0c18dac6199", "node_type": "1", "metadata": {"window": "[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.  Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n", "original_text": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. "}, "hash": "6e6ad1e00104b5509864079778c69bad8140775953b6735dd2421336260e4817", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "080c63f0-e347-416f-a942-d7e8f89d2936", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n"}, "hash": "6be2cd846792293b69dddd2018bbb54a6c8d30121faab0ebe2304e53ed3fe533", "class_name": "RelatedNodeInfo"}}, "hash": "d1cbe82e99a35e089670559d838d30f57159eb96088995b0962a03894ae09b5c", "text": "High-resolution\nimage synthesis with latent diffusion models. ", "start_char_idx": 45687, "end_char_idx": 45749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "080c63f0-e347-416f-a942-d7e8f89d2936": {"__data__": {"id_": "080c63f0-e347-416f-a942-d7e8f89d2936", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a3212bb-80c6-4ff5-a769-3fa5f9478ccb", "node_type": "1", "metadata": {"window": "Hierarchical text-conditional\nimage generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. ", "original_text": "High-resolution\nimage synthesis with latent diffusion models. "}, "hash": "d1cbe82e99a35e089670559d838d30f57159eb96088995b0962a03894ae09b5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0055bdba-e0ea-458d-b9f3-38c0d3323a05", "node_type": "1", "metadata": {"window": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. ", "original_text": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. "}, "hash": "eb6d7b1a017d5414e49715687edc9da35df570a2357128a64b34d3fecd963f82", "class_name": "RelatedNodeInfo"}}, "hash": "6be2cd846792293b69dddd2018bbb54a6c8d30121faab0ebe2304e53ed3fe533", "text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n", "start_char_idx": 45749, "end_char_idx": 45861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0055bdba-e0ea-458d-b9f3-38c0d3323a05": {"__data__": {"id_": "0055bdba-e0ea-458d-b9f3-38c0d3323a05", "embedding": null, "metadata": {"window": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. ", "original_text": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "080c63f0-e347-416f-a942-d7e8f89d2936", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2204.06125 , 2022.\n [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n"}, "hash": "6be2cd846792293b69dddd2018bbb54a6c8d30121faab0ebe2304e53ed3fe533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6a1e9fc-51a8-4b57-9b9c-283dd919b0cd", "node_type": "1", "metadata": {"window": "High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n", "original_text": "U-net: Convolutional networks for biomedical\nimage segmentation. "}, "hash": "83d87a002b2347bf70a04a31046e20f074c5cc4eb69a293d7c3b2e8ef191ae82", "class_name": "RelatedNodeInfo"}}, "hash": "eb6d7b1a017d5414e49715687edc9da35df570a2357128a64b34d3fecd963f82", "text": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. ", "start_char_idx": 45861, "end_char_idx": 45918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6a1e9fc-51a8-4b57-9b9c-283dd919b0cd": {"__data__": {"id_": "e6a1e9fc-51a8-4b57-9b9c-283dd919b0cd", "embedding": null, "metadata": {"window": "High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n", "original_text": "U-net: Convolutional networks for biomedical\nimage segmentation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0055bdba-e0ea-458d-b9f3-38c0d3323a05", "node_type": "1", "metadata": {"window": "[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.  High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. ", "original_text": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. "}, "hash": "eb6d7b1a017d5414e49715687edc9da35df570a2357128a64b34d3fecd963f82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3858157-7440-45af-9a1b-007044eec5ce", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. ", "original_text": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. "}, "hash": "cd928a1a4aa92d8b5e331b75212bc175f941657ab954dee96c3ba614a83ae7d0", "class_name": "RelatedNodeInfo"}}, "hash": "83d87a002b2347bf70a04a31046e20f074c5cc4eb69a293d7c3b2e8ef191ae82", "text": "U-net: Convolutional networks for biomedical\nimage segmentation. ", "start_char_idx": 45918, "end_char_idx": 45983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3858157-7440-45af-9a1b-007044eec5ce": {"__data__": {"id_": "c3858157-7440-45af-9a1b-007044eec5ce", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. ", "original_text": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6a1e9fc-51a8-4b57-9b9c-283dd919b0cd", "node_type": "1", "metadata": {"window": "High-resolution\nimage synthesis with latent diffusion models.  In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n", "original_text": "U-net: Convolutional networks for biomedical\nimage segmentation. "}, "hash": "83d87a002b2347bf70a04a31046e20f074c5cc4eb69a293d7c3b2e8ef191ae82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b78e919-db7d-49a2-9d5c-7683134e1068", "node_type": "1", "metadata": {"window": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding. ", "original_text": "Springer, 2015.\n"}, "hash": "d3ee8625ceed3deefb824aefb67a0c6e09313ae8f10b448e1cf9f2de9746a3d9", "class_name": "RelatedNodeInfo"}}, "hash": "cd928a1a4aa92d8b5e331b75212bc175f941657ab954dee96c3ba614a83ae7d0", "text": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. ", "start_char_idx": 45983, "end_char_idx": 46167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b78e919-db7d-49a2-9d5c-7683134e1068": {"__data__": {"id_": "1b78e919-db7d-49a2-9d5c-7683134e1068", "embedding": null, "metadata": {"window": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding. ", "original_text": "Springer, 2015.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3858157-7440-45af-9a1b-007044eec5ce", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10684\u201310695, 2022.\n [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. ", "original_text": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241. "}, "hash": "cd928a1a4aa92d8b5e331b75212bc175f941657ab954dee96c3ba614a83ae7d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f633749-b464-4508-82d3-ec7bf7875ff5", "node_type": "1", "metadata": {"window": "U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n", "original_text": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. "}, "hash": "6ae07422142650b542c81318375972884b940d38fb4de72ddafd1f4fe4d079db", "class_name": "RelatedNodeInfo"}}, "hash": "d3ee8625ceed3deefb824aefb67a0c6e09313ae8f10b448e1cf9f2de9746a3d9", "text": "Springer, 2015.\n", "start_char_idx": 46167, "end_char_idx": 46183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f633749-b464-4508-82d3-ec7bf7875ff5": {"__data__": {"id_": "5f633749-b464-4508-82d3-ec7bf7875ff5", "embedding": null, "metadata": {"window": "U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n", "original_text": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b78e919-db7d-49a2-9d5c-7683134e1068", "node_type": "1", "metadata": {"window": "[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding. ", "original_text": "Springer, 2015.\n"}, "hash": "d3ee8625ceed3deefb824aefb67a0c6e09313ae8f10b448e1cf9f2de9746a3d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f4eb45d-7dc8-4b95-a9ae-41c896bd56ec", "node_type": "1", "metadata": {"window": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. ", "original_text": "Photorealistic text-to-\nimage diffusion models with deep language understanding. "}, "hash": "31c83f84f061e3f09094c050b67cf71c86e46ac1bc18f721de897533edd85c2d", "class_name": "RelatedNodeInfo"}}, "hash": "6ae07422142650b542c81318375972884b940d38fb4de72ddafd1f4fe4d079db", "text": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. ", "start_char_idx": 46183, "end_char_idx": 46355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f4eb45d-7dc8-4b95-a9ae-41c896bd56ec": {"__data__": {"id_": "5f4eb45d-7dc8-4b95-a9ae-41c896bd56ec", "embedding": null, "metadata": {"window": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. ", "original_text": "Photorealistic text-to-\nimage diffusion models with deep language understanding. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f633749-b464-4508-82d3-ec7bf7875ff5", "node_type": "1", "metadata": {"window": "U-net: Convolutional networks for biomedical\nimage segmentation.  In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n", "original_text": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. "}, "hash": "6ae07422142650b542c81318375972884b940d38fb4de72ddafd1f4fe4d079db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d94159e-1a55-417d-a2e2-5d0c0f07fbce", "node_type": "1", "metadata": {"window": "Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks. ", "original_text": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n"}, "hash": "09ee977650ed1bf1be2587195164a64ab32277a6bfad2cfaa6f0c70423af9e0e", "class_name": "RelatedNodeInfo"}}, "hash": "31c83f84f061e3f09094c050b67cf71c86e46ac1bc18f721de897533edd85c2d", "text": "Photorealistic text-to-\nimage diffusion models with deep language understanding. ", "start_char_idx": 46355, "end_char_idx": 46436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d94159e-1a55-417d-a2e2-5d0c0f07fbce": {"__data__": {"id_": "3d94159e-1a55-417d-a2e2-5d0c0f07fbce", "embedding": null, "metadata": {"window": "Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks. ", "original_text": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f4eb45d-7dc8-4b95-a9ae-41c896bd56ec", "node_type": "1", "metadata": {"window": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages\n234\u2013241.  Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. ", "original_text": "Photorealistic text-to-\nimage diffusion models with deep language understanding. "}, "hash": "31c83f84f061e3f09094c050b67cf71c86e46ac1bc18f721de897533edd85c2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b88f44a-3e8b-41eb-9ec1-60cc555c7427", "node_type": "1", "metadata": {"window": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n", "original_text": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. "}, "hash": "f9018aa7e416b12514b9a630e7b7b3046be45e83648101a9bf421c6656024d3c", "class_name": "RelatedNodeInfo"}}, "hash": "09ee977650ed1bf1be2587195164a64ab32277a6bfad2cfaa6f0c70423af9e0e", "text": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n", "start_char_idx": 46436, "end_char_idx": 46510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b88f44a-3e8b-41eb-9ec1-60cc555c7427": {"__data__": {"id_": "4b88f44a-3e8b-41eb-9ec1-60cc555c7427", "embedding": null, "metadata": {"window": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n", "original_text": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d94159e-1a55-417d-a2e2-5d0c0f07fbce", "node_type": "1", "metadata": {"window": "Springer, 2015.\n [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks. ", "original_text": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n"}, "hash": "09ee977650ed1bf1be2587195164a64ab32277a6bfad2cfaa6f0c70423af9e0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8347344-7450-4a17-841e-b421ba968d02", "node_type": "1", "metadata": {"window": "Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. ", "original_text": "A u-net based discriminator for generative adversarial\nnetworks. "}, "hash": "3f87609c5daaa7a17cd13712bdf280af9b267978833f932220044873cc873bd9", "class_name": "RelatedNodeInfo"}}, "hash": "f9018aa7e416b12514b9a630e7b7b3046be45e83648101a9bf421c6656024d3c", "text": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. ", "start_char_idx": 46510, "end_char_idx": 46565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8347344-7450-4a17-841e-b421ba968d02": {"__data__": {"id_": "f8347344-7450-4a17-841e-b421ba968d02", "embedding": null, "metadata": {"window": "Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. ", "original_text": "A u-net based discriminator for generative adversarial\nnetworks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b88f44a-3e8b-41eb-9ec1-60cc555c7427", "node_type": "1", "metadata": {"window": "[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.  Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n", "original_text": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. "}, "hash": "f9018aa7e416b12514b9a630e7b7b3046be45e83648101a9bf421c6656024d3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb035e3f-9fa4-4b81-b289-7d0c7f6156df", "node_type": "1", "metadata": {"window": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n"}, "hash": "274059304f66a5f2f1befb458db58dc36afe132c098c79c26f333aa7b75b2d52", "class_name": "RelatedNodeInfo"}}, "hash": "3f87609c5daaa7a17cd13712bdf280af9b267978833f932220044873cc873bd9", "text": "A u-net based discriminator for generative adversarial\nnetworks. ", "start_char_idx": 46565, "end_char_idx": 46630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb035e3f-9fa4-4b81-b289-7d0c7f6156df": {"__data__": {"id_": "eb035e3f-9fa4-4b81-b289-7d0c7f6156df", "embedding": null, "metadata": {"window": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8347344-7450-4a17-841e-b421ba968d02", "node_type": "1", "metadata": {"window": "Photorealistic text-to-\nimage diffusion models with deep language understanding.  Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. ", "original_text": "A u-net based discriminator for generative adversarial\nnetworks. "}, "hash": "3f87609c5daaa7a17cd13712bdf280af9b267978833f932220044873cc873bd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ed9c19a-011e-4944-b76a-af2d97537b5d", "node_type": "1", "metadata": {"window": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n", "original_text": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. "}, "hash": "f4d5bc3744ccabe1ace2d8943a21b41e861b789c068b8fc63b4ccfdd2a6410f7", "class_name": "RelatedNodeInfo"}}, "hash": "274059304f66a5f2f1befb458db58dc36afe132c098c79c26f333aa7b75b2d52", "text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n", "start_char_idx": 46630, "end_char_idx": 46740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ed9c19a-011e-4944-b76a-af2d97537b5d": {"__data__": {"id_": "1ed9c19a-011e-4944-b76a-af2d97537b5d", "embedding": null, "metadata": {"window": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n", "original_text": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb035e3f-9fa4-4b81-b289-7d0c7f6156df", "node_type": "1", "metadata": {"window": "Advances in Neural Information Processing\nSystems , 35:36479\u201336494, 2022.\n [49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. ", "original_text": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n"}, "hash": "274059304f66a5f2f1befb458db58dc36afe132c098c79c26f333aa7b75b2d52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "659bed95-39d6-40c6-8d6f-0814055a7aa5", "node_type": "1", "metadata": {"window": "A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon. ", "original_text": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. "}, "hash": "82d0ff17d3669c73f6c1e1a7f7cc4283e85c97b1834d1c8f32dccf67349be91b", "class_name": "RelatedNodeInfo"}}, "hash": "f4d5bc3744ccabe1ace2d8943a21b41e861b789c068b8fc63b4ccfdd2a6410f7", "text": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. ", "start_char_idx": 46740, "end_char_idx": 46865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "659bed95-39d6-40c6-8d6f-0814055a7aa5": {"__data__": {"id_": "659bed95-39d6-40c6-8d6f-0814055a7aa5", "embedding": null, "metadata": {"window": "A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon. ", "original_text": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ed9c19a-011e-4944-b76a-af2d97537b5d", "node_type": "1", "metadata": {"window": "[49] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.  A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n", "original_text": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. "}, "hash": "f4d5bc3744ccabe1ace2d8943a21b41e861b789c068b8fc63b4ccfdd2a6410f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b33fe4d-e031-4aec-b819-cd8cef328fc3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution. ", "original_text": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n"}, "hash": "8671dbfde6d3de7489545fded6e8a54dadaa6ac30ee75a245b24216334aaf3a7", "class_name": "RelatedNodeInfo"}}, "hash": "82d0ff17d3669c73f6c1e1a7f7cc4283e85c97b1834d1c8f32dccf67349be91b", "text": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. ", "start_char_idx": 46865, "end_char_idx": 46974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b33fe4d-e031-4aec-b819-cd8cef328fc3": {"__data__": {"id_": "8b33fe4d-e031-4aec-b819-cd8cef328fc3", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution. ", "original_text": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "659bed95-39d6-40c6-8d6f-0814055a7aa5", "node_type": "1", "metadata": {"window": "A u-net based discriminator for generative adversarial\nnetworks.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon. ", "original_text": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network. "}, "hash": "82d0ff17d3669c73f6c1e1a7f7cc4283e85c97b1834d1c8f32dccf67349be91b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c20c283-ddec-46f5-a14c-c580796b950c", "node_type": "1", "metadata": {"window": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. ", "original_text": "[51] Yang Song and Stefano Ermon. "}, "hash": "662d0eb157cde60b54091347313ba2528806c21a8f77df2a8ab2df7eb6ea8e79", "class_name": "RelatedNodeInfo"}}, "hash": "8671dbfde6d3de7489545fded6e8a54dadaa6ac30ee75a245b24216334aaf3a7", "text": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n", "start_char_idx": 46974, "end_char_idx": 47080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c20c283-ddec-46f5-a14c-c580796b950c": {"__data__": {"id_": "1c20c283-ddec-46f5-a14c-c580796b950c", "embedding": null, "metadata": {"window": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. ", "original_text": "[51] Yang Song and Stefano Ermon. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b33fe4d-e031-4aec-b819-cd8cef328fc3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8207\u20138216, 2020.\n [50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution. ", "original_text": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n"}, "hash": "8671dbfde6d3de7489545fded6e8a54dadaa6ac30ee75a245b24216334aaf3a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7131c697-3542-4fa6-bf2a-a525e2158769", "node_type": "1", "metadata": {"window": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n", "original_text": "Generative modeling by estimating gradients of the data distribution. "}, "hash": "e720995ef66d8208f54203e3a9eb17f3d293feb7a5504a1b855007dcd2b4cfaf", "class_name": "RelatedNodeInfo"}}, "hash": "662d0eb157cde60b54091347313ba2528806c21a8f77df2a8ab2df7eb6ea8e79", "text": "[51] Yang Song and Stefano Ermon. ", "start_char_idx": 47080, "end_char_idx": 47114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7131c697-3542-4fa6-bf2a-a525e2158769": {"__data__": {"id_": "7131c697-3542-4fa6-bf2a-a525e2158769", "embedding": null, "metadata": {"window": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n", "original_text": "Generative modeling by estimating gradients of the data distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c20c283-ddec-46f5-a14c-c580796b950c", "node_type": "1", "metadata": {"window": "[50] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.  Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. ", "original_text": "[51] Yang Song and Stefano Ermon. "}, "hash": "662d0eb157cde60b54091347313ba2528806c21a8f77df2a8ab2df7eb6ea8e79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf3553c2-6280-4c33-aab8-9cba666c8a0d", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. ", "original_text": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. "}, "hash": "29c28b2300086ba576e574bb5361fbe40dd2746d2c0c3162f31c3e7dbdd4c91d", "class_name": "RelatedNodeInfo"}}, "hash": "e720995ef66d8208f54203e3a9eb17f3d293feb7a5504a1b855007dcd2b4cfaf", "text": "Generative modeling by estimating gradients of the data distribution. ", "start_char_idx": 47114, "end_char_idx": 47184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf3553c2-6280-4c33-aab8-9cba666c8a0d": {"__data__": {"id_": "cf3553c2-6280-4c33-aab8-9cba666c8a0d", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. ", "original_text": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7131c697-3542-4fa6-bf2a-a525e2158769", "node_type": "1", "metadata": {"window": "Real-time single image and video super-resolution using an efficient sub-pixel\nconvolutional neural network.  In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n", "original_text": "Generative modeling by estimating gradients of the data distribution. "}, "hash": "e720995ef66d8208f54203e3a9eb17f3d293feb7a5504a1b855007dcd2b4cfaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf17e4bc-9056-428f-8799-d509d863a5e3", "node_type": "1", "metadata": {"window": "[51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations. ", "original_text": "Curran Associates, Inc., 2019.\n"}, "hash": "89e14ae916ac6f695b84d37416e0088c4d704bbf6e216bf1a48a686fe478f9e7", "class_name": "RelatedNodeInfo"}}, "hash": "29c28b2300086ba576e574bb5361fbe40dd2746d2c0c3162f31c3e7dbdd4c91d", "text": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. ", "start_char_idx": 47184, "end_char_idx": 47343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf17e4bc-9056-428f-8799-d509d863a5e3": {"__data__": {"id_": "cf17e4bc-9056-428f-8799-d509d863a5e3", "embedding": null, "metadata": {"window": "[51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations. ", "original_text": "Curran Associates, Inc., 2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf3553c2-6280-4c33-aab8-9cba666c8a0d", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1874\u20131883, 2016.\n [51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. ", "original_text": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. "}, "hash": "29c28b2300086ba576e574bb5361fbe40dd2746d2c0c3162f31c3e7dbdd4c91d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "836a49a1-2430-4db6-aa81-5586cc5ad898", "node_type": "1", "metadata": {"window": "Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n", "original_text": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. "}, "hash": "2a3a9b092c9d0b27faf248752157b3f61b74aa53c52560aadd84371df76ed95e", "class_name": "RelatedNodeInfo"}}, "hash": "89e14ae916ac6f695b84d37416e0088c4d704bbf6e216bf1a48a686fe478f9e7", "text": "Curran Associates, Inc., 2019.\n", "start_char_idx": 47343, "end_char_idx": 47374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "836a49a1-2430-4db6-aa81-5586cc5ad898": {"__data__": {"id_": "836a49a1-2430-4db6-aa81-5586cc5ad898", "embedding": null, "metadata": {"window": "Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n", "original_text": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf17e4bc-9056-428f-8799-d509d863a5e3", "node_type": "1", "metadata": {"window": "[51] Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations. ", "original_text": "Curran Associates, Inc., 2019.\n"}, "hash": "89e14ae916ac6f695b84d37416e0088c4d704bbf6e216bf1a48a686fe478f9e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b26f732-ee4e-4960-b538-3876b5175ba0", "node_type": "1", "metadata": {"window": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al. ", "original_text": "Score-based generative modeling through stochastic differential equations. "}, "hash": "f385f79b6712a552d073329e058b38c18c6d2b971f1b1ab8f9f5474e3c0f78e8", "class_name": "RelatedNodeInfo"}}, "hash": "2a3a9b092c9d0b27faf248752157b3f61b74aa53c52560aadd84371df76ed95e", "text": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. ", "start_char_idx": 47374, "end_char_idx": 47478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b26f732-ee4e-4960-b538-3876b5175ba0": {"__data__": {"id_": "7b26f732-ee4e-4960-b538-3876b5175ba0", "embedding": null, "metadata": {"window": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al. ", "original_text": "Score-based generative modeling through stochastic differential equations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "836a49a1-2430-4db6-aa81-5586cc5ad898", "node_type": "1", "metadata": {"window": "Generative modeling by estimating gradients of the data distribution.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n", "original_text": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. "}, "hash": "2a3a9b092c9d0b27faf248752157b3f61b74aa53c52560aadd84371df76ed95e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fe734c2-17ac-4e8a-aa90-986481b27ae5", "node_type": "1", "metadata": {"window": "Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning. ", "original_text": "arXiv preprint\narXiv:2011.13456 , 2020.\n"}, "hash": "a8050f5353943605fd9d4056c7c1f78a76cc1186580ef5969820120a0838fe43", "class_name": "RelatedNodeInfo"}}, "hash": "f385f79b6712a552d073329e058b38c18c6d2b971f1b1ab8f9f5474e3c0f78e8", "text": "Score-based generative modeling through stochastic differential equations. ", "start_char_idx": 47478, "end_char_idx": 47553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fe734c2-17ac-4e8a-aa90-986481b27ae5": {"__data__": {"id_": "7fe734c2-17ac-4e8a-aa90-986481b27ae5", "embedding": null, "metadata": {"window": "Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning. ", "original_text": "arXiv preprint\narXiv:2011.13456 , 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b26f732-ee4e-4960-b538-3876b5175ba0", "node_type": "1", "metadata": {"window": "In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32.  Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al. ", "original_text": "Score-based generative modeling through stochastic differential equations. "}, "hash": "f385f79b6712a552d073329e058b38c18c6d2b971f1b1ab8f9f5474e3c0f78e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04dfd9fe-6fe0-4a41-ae5b-a993ac8d166f", "node_type": "1", "metadata": {"window": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n", "original_text": "[53] Aaron Van Den Oord, Oriol Vinyals, et al. "}, "hash": "b88a822fa3f57248396fc1cfcfb163a58c4540c349d47416ed808feb128a5c4e", "class_name": "RelatedNodeInfo"}}, "hash": "a8050f5353943605fd9d4056c7c1f78a76cc1186580ef5969820120a0838fe43", "text": "arXiv preprint\narXiv:2011.13456 , 2020.\n", "start_char_idx": 47553, "end_char_idx": 47593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04dfd9fe-6fe0-4a41-ae5b-a993ac8d166f": {"__data__": {"id_": "04dfd9fe-6fe0-4a41-ae5b-a993ac8d166f", "embedding": null, "metadata": {"window": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n", "original_text": "[53] Aaron Van Den Oord, Oriol Vinyals, et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fe734c2-17ac-4e8a-aa90-986481b27ae5", "node_type": "1", "metadata": {"window": "Curran Associates, Inc., 2019.\n [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning. ", "original_text": "arXiv preprint\narXiv:2011.13456 , 2020.\n"}, "hash": "a8050f5353943605fd9d4056c7c1f78a76cc1186580ef5969820120a0838fe43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "174d4e98-8027-4ff3-bd3b-487e1259f402", "node_type": "1", "metadata": {"window": "Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. ", "original_text": "Neural discrete representation learning. "}, "hash": "fc2d5e8c94b57f4f6054e95a9ef51b28697fe44c9838d409c716a994d3f61de2", "class_name": "RelatedNodeInfo"}}, "hash": "b88a822fa3f57248396fc1cfcfb163a58c4540c349d47416ed808feb128a5c4e", "text": "[53] Aaron Van Den Oord, Oriol Vinyals, et al. ", "start_char_idx": 47593, "end_char_idx": 47640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "174d4e98-8027-4ff3-bd3b-487e1259f402": {"__data__": {"id_": "174d4e98-8027-4ff3-bd3b-487e1259f402", "embedding": null, "metadata": {"window": "Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. ", "original_text": "Neural discrete representation learning. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04dfd9fe-6fe0-4a41-ae5b-a993ac8d166f", "node_type": "1", "metadata": {"window": "[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.  Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n", "original_text": "[53] Aaron Van Den Oord, Oriol Vinyals, et al. "}, "hash": "b88a822fa3f57248396fc1cfcfb163a58c4540c349d47416ed808feb128a5c4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f26ac318-d67a-4bc4-a247-6d8101dea8d0", "node_type": "1", "metadata": {"window": "arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior. ", "original_text": "Advances in neural\ninformation processing systems , 30, 2017.\n"}, "hash": "ffc2fa3b9e7901daed2c4065d832f48a668a5c523727ad54cf51c7ff6e8b353e", "class_name": "RelatedNodeInfo"}}, "hash": "fc2d5e8c94b57f4f6054e95a9ef51b28697fe44c9838d409c716a994d3f61de2", "text": "Neural discrete representation learning. ", "start_char_idx": 47640, "end_char_idx": 47681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f26ac318-d67a-4bc4-a247-6d8101dea8d0": {"__data__": {"id_": "f26ac318-d67a-4bc4-a247-6d8101dea8d0", "embedding": null, "metadata": {"window": "arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior. ", "original_text": "Advances in neural\ninformation processing systems , 30, 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "174d4e98-8027-4ff3-bd3b-487e1259f402", "node_type": "1", "metadata": {"window": "Score-based generative modeling through stochastic differential equations.  arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. ", "original_text": "Neural discrete representation learning. "}, "hash": "fc2d5e8c94b57f4f6054e95a9ef51b28697fe44c9838d409c716a994d3f61de2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c42344e0-8c9b-4f9b-a511-f69aa4fd3fc8", "node_type": "1", "metadata": {"window": "[53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n", "original_text": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. "}, "hash": "8e480259a0a8f97d11f79f1aae28939c77ecf049955b76f02d2ab0f4a9590f4d", "class_name": "RelatedNodeInfo"}}, "hash": "ffc2fa3b9e7901daed2c4065d832f48a668a5c523727ad54cf51c7ff6e8b353e", "text": "Advances in neural\ninformation processing systems , 30, 2017.\n", "start_char_idx": 47681, "end_char_idx": 47743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c42344e0-8c9b-4f9b-a511-f69aa4fd3fc8": {"__data__": {"id_": "c42344e0-8c9b-4f9b-a511-f69aa4fd3fc8", "embedding": null, "metadata": {"window": "[53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n", "original_text": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f26ac318-d67a-4bc4-a247-6d8101dea8d0", "node_type": "1", "metadata": {"window": "arXiv preprint\narXiv:2011.13456 , 2020.\n [53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior. ", "original_text": "Advances in neural\ninformation processing systems , 30, 2017.\n"}, "hash": "ffc2fa3b9e7901daed2c4065d832f48a668a5c523727ad54cf51c7ff6e8b353e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "379384dd-d943-4316-90a7-b6c326592a0e", "node_type": "1", "metadata": {"window": "Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. ", "original_text": "Towards real-world blind face restoration with\ngenerative facial prior. "}, "hash": "620f44ae972bdc3beecda74d4b08c266fe5567f5db1351f68d9d2b1f43a38f71", "class_name": "RelatedNodeInfo"}}, "hash": "8e480259a0a8f97d11f79f1aae28939c77ecf049955b76f02d2ab0f4a9590f4d", "text": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. ", "start_char_idx": 47743, "end_char_idx": 47798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "379384dd-d943-4316-90a7-b6c326592a0e": {"__data__": {"id_": "379384dd-d943-4316-90a7-b6c326592a0e", "embedding": null, "metadata": {"window": "Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. ", "original_text": "Towards real-world blind face restoration with\ngenerative facial prior. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c42344e0-8c9b-4f9b-a511-f69aa4fd3fc8", "node_type": "1", "metadata": {"window": "[53] Aaron Van Den Oord, Oriol Vinyals, et al.  Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n", "original_text": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. "}, "hash": "8e480259a0a8f97d11f79f1aae28939c77ecf049955b76f02d2ab0f4a9590f4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dd84bf4-e8e8-4a22-8576-dcba0b015eef", "node_type": "1", "metadata": {"window": "Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n"}, "hash": "75410e3c5f1daf3d090eab454b79396a31b67c3063990b3b89d57fe1c3fa96a5", "class_name": "RelatedNodeInfo"}}, "hash": "620f44ae972bdc3beecda74d4b08c266fe5567f5db1351f68d9d2b1f43a38f71", "text": "Towards real-world blind face restoration with\ngenerative facial prior. ", "start_char_idx": 47798, "end_char_idx": 47870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dd84bf4-e8e8-4a22-8576-dcba0b015eef": {"__data__": {"id_": "2dd84bf4-e8e8-4a22-8576-dcba0b015eef", "embedding": null, "metadata": {"window": "Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "379384dd-d943-4316-90a7-b6c326592a0e", "node_type": "1", "metadata": {"window": "Neural discrete representation learning.  Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. ", "original_text": "Towards real-world blind face restoration with\ngenerative facial prior. "}, "hash": "620f44ae972bdc3beecda74d4b08c266fe5567f5db1351f68d9d2b1f43a38f71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7ebcbcb-b285-4385-b848-30a15d11cedb", "node_type": "1", "metadata": {"window": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n", "original_text": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. "}, "hash": "638f49b9bfdae120b80bce95ecc67786e2f42c29d30e88f2c420f3c4d2aebb16", "class_name": "RelatedNodeInfo"}}, "hash": "75410e3c5f1daf3d090eab454b79396a31b67c3063990b3b89d57fe1c3fa96a5", "text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n", "start_char_idx": 47870, "end_char_idx": 47980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7ebcbcb-b285-4385-b848-30a15d11cedb": {"__data__": {"id_": "f7ebcbcb-b285-4385-b848-30a15d11cedb", "embedding": null, "metadata": {"window": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n", "original_text": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dd84bf4-e8e8-4a22-8576-dcba0b015eef", "node_type": "1", "metadata": {"window": "Advances in neural\ninformation processing systems , 30, 2017.\n [54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n"}, "hash": "75410e3c5f1daf3d090eab454b79396a31b67c3063990b3b89d57fe1c3fa96a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d0d3e00-a795-49bf-accf-867e946ca2ea", "node_type": "1", "metadata": {"window": "Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n", "original_text": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. "}, "hash": "2f34227b388d9ec3d470b1ba68a2cc6a4305dce9ac85acb3cb6d6e5e868a5510", "class_name": "RelatedNodeInfo"}}, "hash": "638f49b9bfdae120b80bce95ecc67786e2f42c29d30e88f2c420f3c4d2aebb16", "text": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. ", "start_char_idx": 47980, "end_char_idx": 48038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d0d3e00-a795-49bf-accf-867e946ca2ea": {"__data__": {"id_": "7d0d3e00-a795-49bf-accf-867e946ca2ea", "embedding": null, "metadata": {"window": "Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n", "original_text": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7ebcbcb-b285-4385-b848-30a15d11cedb", "node_type": "1", "metadata": {"window": "[54] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.  Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n", "original_text": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. "}, "hash": "638f49b9bfdae120b80bce95ecc67786e2f42c29d30e88f2c420f3c4d2aebb16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3220e706-be91-46e7-9ad2-62f8347fcff3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks. ", "original_text": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n"}, "hash": "c056c6ac4c6e959e3b27512f64936fa233141145f59c5d0f257cb4bcc3eef658", "class_name": "RelatedNodeInfo"}}, "hash": "2f34227b388d9ec3d470b1ba68a2cc6a4305dce9ac85acb3cb6d6e5e868a5510", "text": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. ", "start_char_idx": 48038, "end_char_idx": 48121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3220e706-be91-46e7-9ad2-62f8347fcff3": {"__data__": {"id_": "3220e706-be91-46e7-9ad2-62f8347fcff3", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks. ", "original_text": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d0d3e00-a795-49bf-accf-867e946ca2ea", "node_type": "1", "metadata": {"window": "Towards real-world blind face restoration with\ngenerative facial prior.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n", "original_text": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data. "}, "hash": "2f34227b388d9ec3d470b1ba68a2cc6a4305dce9ac85acb3cb6d6e5e868a5510", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c7f434c-bc43-492d-9bd9-bb6b3e6f1140", "node_type": "1", "metadata": {"window": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n", "original_text": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n"}, "hash": "7330a26bf25035fca5eb7a518bd66531ab52b5f11308d4012ea899d4f0f00d28", "class_name": "RelatedNodeInfo"}}, "hash": "c056c6ac4c6e959e3b27512f64936fa233141145f59c5d0f257cb4bcc3eef658", "text": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n", "start_char_idx": 48121, "end_char_idx": 48221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c7f434c-bc43-492d-9bd9-bb6b3e6f1140": {"__data__": {"id_": "0c7f434c-bc43-492d-9bd9-bb6b3e6f1140", "embedding": null, "metadata": {"window": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n", "original_text": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3220e706-be91-46e7-9ad2-62f8347fcff3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9168\u20139178, 2021.\n [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks. ", "original_text": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n"}, "hash": "c056c6ac4c6e959e3b27512f64936fa233141145f59c5d0f257cb4bcc3eef658", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "209f8cd2-3d2c-455e-aff7-398206fe0af4", "node_type": "1", "metadata": {"window": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. ", "original_text": "Esrgan: Enhanced super-resolution generative adversarial networks. "}, "hash": "5695947331506d5713c3f70b8b2e3f87d3d6703bdece459c12a1d97e75709ef7", "class_name": "RelatedNodeInfo"}}, "hash": "7330a26bf25035fca5eb7a518bd66531ab52b5f11308d4012ea899d4f0f00d28", "text": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n", "start_char_idx": 48221, "end_char_idx": 48322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "209f8cd2-3d2c-455e-aff7-398206fe0af4": {"__data__": {"id_": "209f8cd2-3d2c-455e-aff7-398206fe0af4", "embedding": null, "metadata": {"window": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. ", "original_text": "Esrgan: Enhanced super-resolution generative adversarial networks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c7f434c-bc43-492d-9bd9-bb6b3e6f1140", "node_type": "1", "metadata": {"window": "[55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.  Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n", "original_text": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n"}, "hash": "7330a26bf25035fca5eb7a518bd66531ab52b5f11308d4012ea899d4f0f00d28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d403329-e1e5-436b-a8da-2c869d37d9d3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel. ", "original_text": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n"}, "hash": "497937b4db145a187a9a4aeaf0310db020d5dc06e0293efb4d85af272b422a08", "class_name": "RelatedNodeInfo"}}, "hash": "5695947331506d5713c3f70b8b2e3f87d3d6703bdece459c12a1d97e75709ef7", "text": "Esrgan: Enhanced super-resolution generative adversarial networks. ", "start_char_idx": 48322, "end_char_idx": 48389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d403329-e1e5-436b-a8da-2c869d37d9d3": {"__data__": {"id_": "9d403329-e1e5-436b-a8da-2c869d37d9d3", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel. ", "original_text": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "209f8cd2-3d2c-455e-aff7-398206fe0af4", "node_type": "1", "metadata": {"window": "Real-esrgan: Training real-world blind super-\nresolution with pure synthetic data.  In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. ", "original_text": "Esrgan: Enhanced super-resolution generative adversarial networks. "}, "hash": "5695947331506d5713c3f70b8b2e3f87d3d6703bdece459c12a1d97e75709ef7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "536ec7fa-6399-486b-a969-d5682a51d1da", "node_type": "1", "metadata": {"window": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n", "original_text": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. "}, "hash": "b378059ee0208552307c78395eba68441ee0638f806cfede6a96ed072c39f78c", "class_name": "RelatedNodeInfo"}}, "hash": "497937b4db145a187a9a4aeaf0310db020d5dc06e0293efb4d85af272b422a08", "text": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n", "start_char_idx": 48389, "end_char_idx": 48486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "536ec7fa-6399-486b-a969-d5682a51d1da": {"__data__": {"id_": "536ec7fa-6399-486b-a969-d5682a51d1da", "embedding": null, "metadata": {"window": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n", "original_text": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d403329-e1e5-436b-a8da-2c869d37d9d3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 1905\u20131914, 2021.\n [56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel. ", "original_text": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n"}, "hash": "497937b4db145a187a9a4aeaf0310db020d5dc06e0293efb4d85af272b422a08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfe660ec-4e3f-464f-abf9-52c3b4d45a4e", "node_type": "1", "metadata": {"window": "Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. ", "original_text": "Zero-shot image restoration using denoising diffusion null-space\nmodel. "}, "hash": "6961159a2da6ca992c82270c0024e6e941891b36fff0b77eeb41ecc094ec6d56", "class_name": "RelatedNodeInfo"}}, "hash": "b378059ee0208552307c78395eba68441ee0638f806cfede6a96ed072c39f78c", "text": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. ", "start_char_idx": 48486, "end_char_idx": 48531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfe660ec-4e3f-464f-abf9-52c3b4d45a4e": {"__data__": {"id_": "bfe660ec-4e3f-464f-abf9-52c3b4d45a4e", "embedding": null, "metadata": {"window": "Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. ", "original_text": "Zero-shot image restoration using denoising diffusion null-space\nmodel. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "536ec7fa-6399-486b-a969-d5682a51d1da", "node_type": "1", "metadata": {"window": "[56] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\n Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n", "original_text": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang. "}, "hash": "b378059ee0208552307c78395eba68441ee0638f806cfede6a96ed072c39f78c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34bbb280-4777-4523-940a-957b29505fbc", "node_type": "1", "metadata": {"window": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration. ", "original_text": "arXiv preprint arXiv:2212.00490 , 2022.\n"}, "hash": "9355a9e0a7621c23b9a4017a76ba6f247f312506b20e3fa98ca4a9dbeedddbc6", "class_name": "RelatedNodeInfo"}}, "hash": "6961159a2da6ca992c82270c0024e6e941891b36fff0b77eeb41ecc094ec6d56", "text": "Zero-shot image restoration using denoising diffusion null-space\nmodel. ", "start_char_idx": 48531, "end_char_idx": 48603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34bbb280-4777-4523-940a-957b29505fbc": {"__data__": {"id_": "34bbb280-4777-4523-940a-957b29505fbc", "embedding": null, "metadata": {"window": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration. ", "original_text": "arXiv preprint arXiv:2212.00490 , 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfe660ec-4e3f-464f-abf9-52c3b4d45a4e", "node_type": "1", "metadata": {"window": "Esrgan: Enhanced super-resolution generative adversarial networks.  In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. ", "original_text": "Zero-shot image restoration using denoising diffusion null-space\nmodel. "}, "hash": "6961159a2da6ca992c82270c0024e6e941891b36fff0b77eeb41ecc094ec6d56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d494b51b-f065-4a27-a49b-1bd145a9b9fb", "node_type": "1", "metadata": {"window": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n", "original_text": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. "}, "hash": "357031d130ce461918df36ae796cbed15ffa2a0067c6b43b28e12d255b157cb4", "class_name": "RelatedNodeInfo"}}, "hash": "9355a9e0a7621c23b9a4017a76ba6f247f312506b20e3fa98ca4a9dbeedddbc6", "text": "arXiv preprint arXiv:2212.00490 , 2022.\n", "start_char_idx": 48603, "end_char_idx": 48643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d494b51b-f065-4a27-a49b-1bd145a9b9fb": {"__data__": {"id_": "d494b51b-f065-4a27-a49b-1bd145a9b9fb", "embedding": null, "metadata": {"window": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n", "original_text": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34bbb280-4777-4523-940a-957b29505fbc", "node_type": "1", "metadata": {"window": "In Proceedings of the European\nconference on computer vision (ECCV) workshops , pages 0\u20130, 2018.\n [57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration. ", "original_text": "arXiv preprint arXiv:2212.00490 , 2022.\n"}, "hash": "9355a9e0a7621c23b9a4017a76ba6f247f312506b20e3fa98ca4a9dbeedddbc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c698be59-2870-414d-b543-4c7ef4a4aca8", "node_type": "1", "metadata": {"window": "Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. ", "original_text": "Uformer:\nA general u-shaped transformer for image restoration. "}, "hash": "dad6ceea546d83b038b600f52547a53bfc78dd483d5e2d6a3ad8115d9a2644c8", "class_name": "RelatedNodeInfo"}}, "hash": "357031d130ce461918df36ae796cbed15ffa2a0067c6b43b28e12d255b157cb4", "text": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. ", "start_char_idx": 48643, "end_char_idx": 48737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c698be59-2870-414d-b543-4c7ef4a4aca8": {"__data__": {"id_": "c698be59-2870-414d-b543-4c7ef4a4aca8", "embedding": null, "metadata": {"window": "Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. ", "original_text": "Uformer:\nA general u-shaped transformer for image restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d494b51b-f065-4a27-a49b-1bd145a9b9fb", "node_type": "1", "metadata": {"window": "[57] Yinhuai Wang, Jiwen Yu, and Jian Zhang.  Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n", "original_text": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. "}, "hash": "357031d130ce461918df36ae796cbed15ffa2a0067c6b43b28e12d255b157cb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30802c63-3d56-4648-a9f8-2f43162e1e77", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. ", "original_text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n"}, "hash": "1c595640f66bc6899b8bd6efbf3c80558f3abe616499fe48ace4076c7413dfe5", "class_name": "RelatedNodeInfo"}}, "hash": "dad6ceea546d83b038b600f52547a53bfc78dd483d5e2d6a3ad8115d9a2644c8", "text": "Uformer:\nA general u-shaped transformer for image restoration. ", "start_char_idx": 48737, "end_char_idx": 48800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30802c63-3d56-4648-a9f8-2f43162e1e77": {"__data__": {"id_": "30802c63-3d56-4648-a9f8-2f43162e1e77", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. ", "original_text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c698be59-2870-414d-b543-4c7ef4a4aca8", "node_type": "1", "metadata": {"window": "Zero-shot image restoration using denoising diffusion null-space\nmodel.  arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. ", "original_text": "Uformer:\nA general u-shaped transformer for image restoration. "}, "hash": "dad6ceea546d83b038b600f52547a53bfc78dd483d5e2d6a3ad8115d9a2644c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c1b3c3d-2c24-4f0e-aa83-a77b273a4df5", "node_type": "1", "metadata": {"window": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n", "original_text": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. "}, "hash": "307efa9fb5929003b045f141567dfde9c905d747996d1dbf8d37a6ca06395832", "class_name": "RelatedNodeInfo"}}, "hash": "1c595640f66bc6899b8bd6efbf3c80558f3abe616499fe48ace4076c7413dfe5", "text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n", "start_char_idx": 48800, "end_char_idx": 48912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c1b3c3d-2c24-4f0e-aa83-a77b273a4df5": {"__data__": {"id_": "6c1b3c3d-2c24-4f0e-aa83-a77b273a4df5", "embedding": null, "metadata": {"window": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n", "original_text": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30802c63-3d56-4648-a9f8-2f43162e1e77", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:2212.00490 , 2022.\n [58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. ", "original_text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n"}, "hash": "1c595640f66bc6899b8bd6efbf3c80558f3abe616499fe48ace4076c7413dfe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68513895-8058-4994-912e-4ff192c37a66", "node_type": "1", "metadata": {"window": "Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. ", "original_text": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. "}, "hash": "14ddadd33fcc12df3190b0224ea7124de18ebefbc335291e3d33b31696a76f6b", "class_name": "RelatedNodeInfo"}}, "hash": "307efa9fb5929003b045f141567dfde9c905d747996d1dbf8d37a6ca06395832", "text": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. ", "start_char_idx": 48912, "end_char_idx": 48991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68513895-8058-4994-912e-4ff192c37a66": {"__data__": {"id_": "68513895-8058-4994-912e-4ff192c37a66", "embedding": null, "metadata": {"window": "Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. ", "original_text": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c1b3c3d-2c24-4f0e-aa83-a77b273a4df5", "node_type": "1", "metadata": {"window": "[58] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.  Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n", "original_text": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo. "}, "hash": "307efa9fb5929003b045f141567dfde9c905d747996d1dbf8d37a6ca06395832", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b55b3907-4bed-4173-836b-7cc10bfbdeb3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment. ", "original_text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n"}, "hash": "5dc753113483b658a147c3ac94b9787d3233f0287e64ea95def39cfb27de76ec", "class_name": "RelatedNodeInfo"}}, "hash": "14ddadd33fcc12df3190b0224ea7124de18ebefbc335291e3d33b31696a76f6b", "text": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. ", "start_char_idx": 48991, "end_char_idx": 49075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b55b3907-4bed-4173-836b-7cc10bfbdeb3": {"__data__": {"id_": "b55b3907-4bed-4173-836b-7cc10bfbdeb3", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment. ", "original_text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68513895-8058-4994-912e-4ff192c37a66", "node_type": "1", "metadata": {"window": "Uformer:\nA general u-shaped transformer for image restoration.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. ", "original_text": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs. "}, "hash": "14ddadd33fcc12df3190b0224ea7124de18ebefbc335291e3d33b31696a76f6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3e5180f-8b19-4255-adf1-b1b7608d0da1", "node_type": "1", "metadata": {"window": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n", "original_text": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. "}, "hash": "b75ae3e2754be582fed515ed3f2a73ff8e0fcd6ff3856473ce6a6748ea29d3fd", "class_name": "RelatedNodeInfo"}}, "hash": "5dc753113483b658a147c3ac94b9787d3233f0287e64ea95def39cfb27de76ec", "text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n", "start_char_idx": 49075, "end_char_idx": 49187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3e5180f-8b19-4255-adf1-b1b7608d0da1": {"__data__": {"id_": "a3e5180f-8b19-4255-adf1-b1b7608d0da1", "embedding": null, "metadata": {"window": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n", "original_text": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b55b3907-4bed-4173-836b-7cc10bfbdeb3", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17683\u201317693, 2022.\n 14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment. ", "original_text": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n"}, "hash": "5dc753113483b658a147c3ac94b9787d3233f0287e64ea95def39cfb27de76ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "414d3841-f7d5-4245-9d84-0d28bec2c214", "node_type": "1", "metadata": {"window": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. ", "original_text": "Maniqa: Multi-dimension attention network for no-reference image quality assessment. "}, "hash": "403c6e7b76cd1f6d5cb3037766c307ae0079f7fdadfbaafaf6be8d2c718e2dab", "class_name": "RelatedNodeInfo"}}, "hash": "b75ae3e2754be582fed515ed3f2a73ff8e0fcd6ff3856473ce6a6748ea29d3fd", "text": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. ", "start_char_idx": 49187, "end_char_idx": 49294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "414d3841-f7d5-4245-9d84-0d28bec2c214": {"__data__": {"id_": "414d3841-f7d5-4245-9d84-0d28bec2c214", "embedding": null, "metadata": {"window": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. ", "original_text": "Maniqa: Multi-dimension attention network for no-reference image quality assessment. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3e5180f-8b19-4255-adf1-b1b7608d0da1", "node_type": "1", "metadata": {"window": "14\n\n[59] Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo.  Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n", "original_text": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang. "}, "hash": "b75ae3e2754be582fed515ed3f2a73ff8e0fcd6ff3856473ce6a6748ea29d3fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93d17f47-b31d-4ceb-b993-fc45d4ebef93", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild. ", "original_text": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n"}, "hash": "e73d5d85c675ef93b9374d8e50ccafecea0144657a7fec234ec04cf2527772e3", "class_name": "RelatedNodeInfo"}}, "hash": "403c6e7b76cd1f6d5cb3037766c307ae0079f7fdadfbaafaf6be8d2c718e2dab", "text": "Maniqa: Multi-dimension attention network for no-reference image quality assessment. ", "start_char_idx": 49294, "end_char_idx": 49379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d17f47-b31d-4ceb-b993-fc45d4ebef93": {"__data__": {"id_": "93d17f47-b31d-4ceb-b993-fc45d4ebef93", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild. ", "original_text": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "414d3841-f7d5-4245-9d84-0d28bec2c214", "node_type": "1", "metadata": {"window": "Restoreformer: High-quality\nblind face restoration from undegraded key-value pairs.  In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. ", "original_text": "Maniqa: Multi-dimension attention network for no-reference image quality assessment. "}, "hash": "403c6e7b76cd1f6d5cb3037766c307ae0079f7fdadfbaafaf6be8d2c718e2dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b6518cd-0e53-4ff8-aea7-9fabfddd93be", "node_type": "1", "metadata": {"window": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n", "original_text": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. "}, "hash": "34f2266acd93ada8b45ab4125aa6c8f85396159355b4433fbf96b412718ccd73", "class_name": "RelatedNodeInfo"}}, "hash": "e73d5d85c675ef93b9374d8e50ccafecea0144657a7fec234ec04cf2527772e3", "text": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n", "start_char_idx": 49379, "end_char_idx": 49489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b6518cd-0e53-4ff8-aea7-9fabfddd93be": {"__data__": {"id_": "4b6518cd-0e53-4ff8-aea7-9fabfddd93be", "embedding": null, "metadata": {"window": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n", "original_text": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93d17f47-b31d-4ceb-b993-fc45d4ebef93", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 17512\u201317521, 2022.\n [60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild. ", "original_text": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n"}, "hash": "e73d5d85c675ef93b9374d8e50ccafecea0144657a7fec234ec04cf2527772e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54fdadc1-6569-421b-9fdc-4828179987e7", "node_type": "1", "metadata": {"window": "Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. ", "original_text": "Gan prior embedded network for blind face\nrestoration in the wild. "}, "hash": "38586132b60657fd86f5406a816101f14b6602ae5c82a2a2a184ccde17bc5389", "class_name": "RelatedNodeInfo"}}, "hash": "34f2266acd93ada8b45ab4125aa6c8f85396159355b4433fbf96b412718ccd73", "text": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. ", "start_char_idx": 49489, "end_char_idx": 49545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54fdadc1-6569-421b-9fdc-4828179987e7": {"__data__": {"id_": "54fdadc1-6569-421b-9fdc-4828179987e7", "embedding": null, "metadata": {"window": "Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. ", "original_text": "Gan prior embedded network for blind face\nrestoration in the wild. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b6518cd-0e53-4ff8-aea7-9fabfddd93be", "node_type": "1", "metadata": {"window": "[60] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and\nYujiu Yang.  Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n", "original_text": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. "}, "hash": "34f2266acd93ada8b45ab4125aa6c8f85396159355b4433fbf96b412718ccd73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adcdae26-5520-4326-9a46-5b9e173a97db", "node_type": "1", "metadata": {"window": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n"}, "hash": "d2615b97a193de38eb4f2105feadb7ebd694baaf4421443a0160c6cf92f73440", "class_name": "RelatedNodeInfo"}}, "hash": "38586132b60657fd86f5406a816101f14b6602ae5c82a2a2a184ccde17bc5389", "text": "Gan prior embedded network for blind face\nrestoration in the wild. ", "start_char_idx": 49545, "end_char_idx": 49612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adcdae26-5520-4326-9a46-5b9e173a97db": {"__data__": {"id_": "adcdae26-5520-4326-9a46-5b9e173a97db", "embedding": null, "metadata": {"window": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54fdadc1-6569-421b-9fdc-4828179987e7", "node_type": "1", "metadata": {"window": "Maniqa: Multi-dimension attention network for no-reference image quality assessment.  In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. ", "original_text": "Gan prior embedded network for blind face\nrestoration in the wild. "}, "hash": "38586132b60657fd86f5406a816101f14b6602ae5c82a2a2a184ccde17bc5389", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "963b129b-5793-40d7-8a6a-6f1ab7f51de4", "node_type": "1", "metadata": {"window": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n", "original_text": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. "}, "hash": "a0786e713122fff96fb34d89d6f29296ca83784807f360d1e3679da9c536e783", "class_name": "RelatedNodeInfo"}}, "hash": "d2615b97a193de38eb4f2105feadb7ebd694baaf4421443a0160c6cf92f73440", "text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n", "start_char_idx": 49612, "end_char_idx": 49720, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "963b129b-5793-40d7-8a6a-6f1ab7f51de4": {"__data__": {"id_": "963b129b-5793-40d7-8a6a-6f1ab7f51de4", "embedding": null, "metadata": {"window": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n", "original_text": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adcdae26-5520-4326-9a46-5b9e173a97db", "node_type": "1", "metadata": {"window": "In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191\u20131200,\n2022.\n [61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps. ", "original_text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n"}, "hash": "d2615b97a193de38eb4f2105feadb7ebd694baaf4421443a0160c6cf92f73440", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12862353-18db-4c6a-9ecc-eb8ae2d74dee", "node_type": "1", "metadata": {"window": "Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. ", "original_text": "Face super-resolution\nguided by facial component heatmaps. "}, "hash": "8c95ded2a329c233da9184fdf335502705fd9f33ff7cf2b0fb38d0917077a3ec", "class_name": "RelatedNodeInfo"}}, "hash": "a0786e713122fff96fb34d89d6f29296ca83784807f360d1e3679da9c536e783", "text": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. ", "start_char_idx": 49720, "end_char_idx": 49802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12862353-18db-4c6a-9ecc-eb8ae2d74dee": {"__data__": {"id_": "12862353-18db-4c6a-9ecc-eb8ae2d74dee", "embedding": null, "metadata": {"window": "Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. ", "original_text": "Face super-resolution\nguided by facial component heatmaps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "963b129b-5793-40d7-8a6a-6f1ab7f51de4", "node_type": "1", "metadata": {"window": "[61] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.  Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n", "original_text": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. "}, "hash": "a0786e713122fff96fb34d89d6f29296ca83784807f360d1e3679da9c536e783", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f513a3d-338c-4677-91fe-8b932d296745", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration. ", "original_text": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n"}, "hash": "744399ac22dc54cf65e10dbb7fbaa583b0d08a1e1143dc62897865899da4ae8a", "class_name": "RelatedNodeInfo"}}, "hash": "8c95ded2a329c233da9184fdf335502705fd9f33ff7cf2b0fb38d0917077a3ec", "text": "Face super-resolution\nguided by facial component heatmaps. ", "start_char_idx": 49802, "end_char_idx": 49861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f513a3d-338c-4677-91fe-8b932d296745": {"__data__": {"id_": "1f513a3d-338c-4677-91fe-8b932d296745", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration. ", "original_text": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12862353-18db-4c6a-9ecc-eb8ae2d74dee", "node_type": "1", "metadata": {"window": "Gan prior embedded network for blind face\nrestoration in the wild.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. ", "original_text": "Face super-resolution\nguided by facial component heatmaps. "}, "hash": "8c95ded2a329c233da9184fdf335502705fd9f33ff7cf2b0fb38d0917077a3ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2650d1bf-380e-4db7-96bf-30a4a4f8f749", "node_type": "1", "metadata": {"window": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n", "original_text": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. "}, "hash": "4675a7475784be281f7e8ef88b7706e21f998f62713815fba3fc73676fadadb3", "class_name": "RelatedNodeInfo"}}, "hash": "744399ac22dc54cf65e10dbb7fbaa583b0d08a1e1143dc62897865899da4ae8a", "text": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n", "start_char_idx": 49861, "end_char_idx": 49952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2650d1bf-380e-4db7-96bf-30a4a4f8f749": {"__data__": {"id_": "2650d1bf-380e-4db7-96bf-30a4a4f8f749", "embedding": null, "metadata": {"window": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n", "original_text": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f513a3d-338c-4677-91fe-8b932d296745", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 672\u2013681, 2021.\n [62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration. ", "original_text": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n"}, "hash": "744399ac22dc54cf65e10dbb7fbaa583b0d08a1e1143dc62897865899da4ae8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bae7834-bdf9-4547-9246-6858bd0d2511", "node_type": "1", "metadata": {"window": "Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. ", "original_text": "Restormer: Efficient transformer for high-resolution image restoration. "}, "hash": "94d595781c9ae005b5882fe03eac96ef0c8b2e1664726f33ef1a8d6fadee301a", "class_name": "RelatedNodeInfo"}}, "hash": "4675a7475784be281f7e8ef88b7706e21f998f62713815fba3fc73676fadadb3", "text": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. ", "start_char_idx": 49952, "end_char_idx": 50058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bae7834-bdf9-4547-9246-6858bd0d2511": {"__data__": {"id_": "4bae7834-bdf9-4547-9246-6858bd0d2511", "embedding": null, "metadata": {"window": "Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. ", "original_text": "Restormer: Efficient transformer for high-resolution image restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2650d1bf-380e-4db7-96bf-30a4a4f8f749", "node_type": "1", "metadata": {"window": "[62] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley.  Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n", "original_text": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. "}, "hash": "4675a7475784be281f7e8ef88b7706e21f998f62713815fba3fc73676fadadb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "beacb639-6fa0-420c-a15b-2bf5f9797def", "node_type": "1", "metadata": {"window": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution. ", "original_text": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n"}, "hash": "467984a3b09e7fa229c6e0820642ed4b357fb3f4308ca8a34ad8258298f9d073", "class_name": "RelatedNodeInfo"}}, "hash": "94d595781c9ae005b5882fe03eac96ef0c8b2e1664726f33ef1a8d6fadee301a", "text": "Restormer: Efficient transformer for high-resolution image restoration. ", "start_char_idx": 50058, "end_char_idx": 50130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "beacb639-6fa0-420c-a15b-2bf5f9797def": {"__data__": {"id_": "beacb639-6fa0-420c-a15b-2bf5f9797def", "embedding": null, "metadata": {"window": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution. ", "original_text": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bae7834-bdf9-4547-9246-6858bd0d2511", "node_type": "1", "metadata": {"window": "Face super-resolution\nguided by facial component heatmaps.  In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. ", "original_text": "Restormer: Efficient transformer for high-resolution image restoration. "}, "hash": "94d595781c9ae005b5882fe03eac96ef0c8b2e1664726f33ef1a8d6fadee301a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8e5ca1a-76ab-40bf-b27d-514ca03b30b3", "node_type": "1", "metadata": {"window": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n", "original_text": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. "}, "hash": "2ee8941d15838235bc6ba0f89f82b902c16481536afd965830fea35acb2fe32e", "class_name": "RelatedNodeInfo"}}, "hash": "467984a3b09e7fa229c6e0820642ed4b357fb3f4308ca8a34ad8258298f9d073", "text": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n", "start_char_idx": 50130, "end_char_idx": 50240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8e5ca1a-76ab-40bf-b27d-514ca03b30b3": {"__data__": {"id_": "a8e5ca1a-76ab-40bf-b27d-514ca03b30b3", "embedding": null, "metadata": {"window": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n", "original_text": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "beacb639-6fa0-420c-a15b-2bf5f9797def", "node_type": "1", "metadata": {"window": "In Proceedings of the European conference on computer vision\n(ECCV) , pages 217\u2013233, 2018.\n [63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution. ", "original_text": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n"}, "hash": "467984a3b09e7fa229c6e0820642ed4b357fb3f4308ca8a34ad8258298f9d073", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a40a62d-fdfd-4aa4-adb6-cc13a1669afd", "node_type": "1", "metadata": {"window": "Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. ", "original_text": "Designing a practical degradation model\nfor deep blind image super-resolution. "}, "hash": "67c2542a12cde9ae779a219bf62184698db46e0c1f915c8ff26f4162bd2bf3a3", "class_name": "RelatedNodeInfo"}}, "hash": "2ee8941d15838235bc6ba0f89f82b902c16481536afd965830fea35acb2fe32e", "text": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. ", "start_char_idx": 50240, "end_char_idx": 50303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a40a62d-fdfd-4aa4-adb6-cc13a1669afd": {"__data__": {"id_": "8a40a62d-fdfd-4aa4-adb6-cc13a1669afd", "embedding": null, "metadata": {"window": "Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. ", "original_text": "Designing a practical degradation model\nfor deep blind image super-resolution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8e5ca1a-76ab-40bf-b27d-514ca03b30b3", "node_type": "1", "metadata": {"window": "[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang.  Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n", "original_text": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. "}, "hash": "2ee8941d15838235bc6ba0f89f82b902c16481536afd965830fea35acb2fe32e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d3912a6-1214-4a18-b74a-266f60e00c59", "node_type": "1", "metadata": {"window": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. ", "original_text": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n"}, "hash": "07ebd086292dc71572da9f2862e7c5a42fda374de22b3e62b2b69acc1268611f", "class_name": "RelatedNodeInfo"}}, "hash": "67c2542a12cde9ae779a219bf62184698db46e0c1f915c8ff26f4162bd2bf3a3", "text": "Designing a practical degradation model\nfor deep blind image super-resolution. ", "start_char_idx": 50303, "end_char_idx": 50382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d3912a6-1214-4a18-b74a-266f60e00c59": {"__data__": {"id_": "8d3912a6-1214-4a18-b74a-266f60e00c59", "embedding": null, "metadata": {"window": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. ", "original_text": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a40a62d-fdfd-4aa4-adb6-cc13a1669afd", "node_type": "1", "metadata": {"window": "Restormer: Efficient transformer for high-resolution image restoration.  In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. ", "original_text": "Designing a practical degradation model\nfor deep blind image super-resolution. "}, "hash": "67c2542a12cde9ae779a219bf62184698db46e0c1f915c8ff26f4162bd2bf3a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68e2e68b-3dab-42b5-bd8b-e284ebaada26", "node_type": "1", "metadata": {"window": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n", "original_text": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. "}, "hash": "d42dd08fe7055c4f653f8f62fea54c5cf4839c543522ee5742458f13f8428881", "class_name": "RelatedNodeInfo"}}, "hash": "07ebd086292dc71572da9f2862e7c5a42fda374de22b3e62b2b69acc1268611f", "text": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n", "start_char_idx": 50382, "end_char_idx": 50482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68e2e68b-3dab-42b5-bd8b-e284ebaada26": {"__data__": {"id_": "68e2e68b-3dab-42b5-bd8b-e284ebaada26", "embedding": null, "metadata": {"window": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n", "original_text": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d3912a6-1214-4a18-b74a-266f60e00c59", "node_type": "1", "metadata": {"window": "In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5728\u20135739, 2022.\n [64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. ", "original_text": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n"}, "hash": "07ebd086292dc71572da9f2862e7c5a42fda374de22b3e62b2b69acc1268611f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bfda55d-e806-4919-95bc-bd7db323597b", "node_type": "1", "metadata": {"window": "Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala. ", "original_text": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. "}, "hash": "f02b3f2e10455d1d65c1b114ebb65601386cf9c6b8d4d33be4401c08cd125bc2", "class_name": "RelatedNodeInfo"}}, "hash": "d42dd08fe7055c4f653f8f62fea54c5cf4839c543522ee5742458f13f8428881", "text": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. ", "start_char_idx": 50482, "end_char_idx": 50551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bfda55d-e806-4919-95bc-bd7db323597b": {"__data__": {"id_": "8bfda55d-e806-4919-95bc-bd7db323597b", "embedding": null, "metadata": {"window": "Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala. ", "original_text": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68e2e68b-3dab-42b5-bd8b-e284ebaada26", "node_type": "1", "metadata": {"window": "[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.  Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n", "original_text": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. "}, "hash": "d42dd08fe7055c4f653f8f62fea54c5cf4839c543522ee5742458f13f8428881", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6ca2304-8539-476f-9881-6aa9b88f47e6", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models. ", "original_text": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n"}, "hash": "2ef984e1eac7d2609a113ebacb88b58a43831521990ce393fe95be82e6937775", "class_name": "RelatedNodeInfo"}}, "hash": "f02b3f2e10455d1d65c1b114ebb65601386cf9c6b8d4d33be4401c08cd125bc2", "text": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. ", "start_char_idx": 50551, "end_char_idx": 50630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6ca2304-8539-476f-9881-6aa9b88f47e6": {"__data__": {"id_": "a6ca2304-8539-476f-9881-6aa9b88f47e6", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models. ", "original_text": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bfda55d-e806-4919-95bc-bd7db323597b", "node_type": "1", "metadata": {"window": "Designing a practical degradation model\nfor deep blind image super-resolution.  In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala. ", "original_text": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising. "}, "hash": "f02b3f2e10455d1d65c1b114ebb65601386cf9c6b8d4d33be4401c08cd125bc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4a69d63-70df-4ec6-949a-1caa11d0fba7", "node_type": "1", "metadata": {"window": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n", "original_text": "[66] Lvmin Zhang and Maneesh Agrawala. "}, "hash": "1a1cac9c8a00089d29beac4a203552bd235566e1d8e6ed78ce10d01b6875394e", "class_name": "RelatedNodeInfo"}}, "hash": "2ef984e1eac7d2609a113ebacb88b58a43831521990ce393fe95be82e6937775", "text": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n", "start_char_idx": 50630, "end_char_idx": 50694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4a69d63-70df-4ec6-949a-1caa11d0fba7": {"__data__": {"id_": "f4a69d63-70df-4ec6-949a-1caa11d0fba7", "embedding": null, "metadata": {"window": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n", "original_text": "[66] Lvmin Zhang and Maneesh Agrawala. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6ca2304-8539-476f-9881-6aa9b88f47e6", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 4791\u20134800, 2021.\n [65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models. ", "original_text": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n"}, "hash": "2ef984e1eac7d2609a113ebacb88b58a43831521990ce393fe95be82e6937775", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75d93b56-bd8c-4edf-b9fd-72b621e6be1e", "node_type": "1", "metadata": {"window": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. ", "original_text": "Adding conditional control to text-to-image diffusion models. "}, "hash": "6b2aff211e28e99a09f2bb7f747c311a1c1d62a11f9da313e4a3a032dee00532", "class_name": "RelatedNodeInfo"}}, "hash": "1a1cac9c8a00089d29beac4a203552bd235566e1d8e6ed78ce10d01b6875394e", "text": "[66] Lvmin Zhang and Maneesh Agrawala. ", "start_char_idx": 50694, "end_char_idx": 50733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75d93b56-bd8c-4edf-b9fd-72b621e6be1e": {"__data__": {"id_": "75d93b56-bd8c-4edf-b9fd-72b621e6be1e", "embedding": null, "metadata": {"window": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. ", "original_text": "Adding conditional control to text-to-image diffusion models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4a69d63-70df-4ec6-949a-1caa11d0fba7", "node_type": "1", "metadata": {"window": "[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.  Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n", "original_text": "[66] Lvmin Zhang and Maneesh Agrawala. "}, "hash": "1a1cac9c8a00089d29beac4a203552bd235566e1d8e6ed78ce10d01b6875394e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8facfbc9-a3d9-49ad-8125-6fa7b4850d13", "node_type": "1", "metadata": {"window": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric. ", "original_text": "arXiv\npreprint arXiv:2302.05543 , 2023.\n"}, "hash": "f09cf18fb8849cb6f733074686689929d6692f93fcb98d818192e46dd07153e0", "class_name": "RelatedNodeInfo"}}, "hash": "6b2aff211e28e99a09f2bb7f747c311a1c1d62a11f9da313e4a3a032dee00532", "text": "Adding conditional control to text-to-image diffusion models. ", "start_char_idx": 50733, "end_char_idx": 50795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8facfbc9-a3d9-49ad-8125-6fa7b4850d13": {"__data__": {"id_": "8facfbc9-a3d9-49ad-8125-6fa7b4850d13", "embedding": null, "metadata": {"window": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric. ", "original_text": "arXiv\npreprint arXiv:2302.05543 , 2023.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75d93b56-bd8c-4edf-b9fd-72b621e6be1e", "node_type": "1", "metadata": {"window": "Beyond a gaussian denoiser:\nResidual learning of deep cnn for image denoising.  IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. ", "original_text": "Adding conditional control to text-to-image diffusion models. "}, "hash": "6b2aff211e28e99a09f2bb7f747c311a1c1d62a11f9da313e4a3a032dee00532", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69940f2a-c138-4b92-8e4a-0d0a79873c94", "node_type": "1", "metadata": {"window": "[66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n", "original_text": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. "}, "hash": "126e3b9fa81e282665900103aed2999059c67d9465631a787a679ad0dd473051", "class_name": "RelatedNodeInfo"}}, "hash": "f09cf18fb8849cb6f733074686689929d6692f93fcb98d818192e46dd07153e0", "text": "arXiv\npreprint arXiv:2302.05543 , 2023.\n", "start_char_idx": 50795, "end_char_idx": 50835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69940f2a-c138-4b92-8e4a-0d0a79873c94": {"__data__": {"id_": "69940f2a-c138-4b92-8e4a-0d0a79873c94", "embedding": null, "metadata": {"window": "[66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n", "original_text": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8facfbc9-a3d9-49ad-8125-6fa7b4850d13", "node_type": "1", "metadata": {"window": "IEEE transactions on image processing , 26(7):3142\u2013\n3155, 2017.\n [66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric. ", "original_text": "arXiv\npreprint arXiv:2302.05543 , 2023.\n"}, "hash": "f09cf18fb8849cb6f733074686689929d6692f93fcb98d818192e46dd07153e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5455a826-68c3-4381-ace5-ff5ff6d91551", "node_type": "1", "metadata": {"window": "Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. ", "original_text": "The unreasonable\neffectiveness of deep features as a perceptual metric. "}, "hash": "e97170b78e6c5be56d44ff559166780ac670ac835ead4f448da419b5f898264b", "class_name": "RelatedNodeInfo"}}, "hash": "126e3b9fa81e282665900103aed2999059c67d9465631a787a679ad0dd473051", "text": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. ", "start_char_idx": 50835, "end_char_idx": 50918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5455a826-68c3-4381-ace5-ff5ff6d91551": {"__data__": {"id_": "5455a826-68c3-4381-ace5-ff5ff6d91551", "embedding": null, "metadata": {"window": "Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. ", "original_text": "The unreasonable\neffectiveness of deep features as a perceptual metric. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69940f2a-c138-4b92-8e4a-0d0a79873c94", "node_type": "1", "metadata": {"window": "[66] Lvmin Zhang and Maneesh Agrawala.  Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n", "original_text": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. "}, "hash": "126e3b9fa81e282665900103aed2999059c67d9465631a787a679ad0dd473051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51d53027-0cd9-4540-a68b-dd48152cf47f", "node_type": "1", "metadata": {"window": "arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer. ", "original_text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n"}, "hash": "c116f331403c7227241b5998007a207ca0c84f7dcc136a6e6ec10f7066af4915", "class_name": "RelatedNodeInfo"}}, "hash": "e97170b78e6c5be56d44ff559166780ac670ac835ead4f448da419b5f898264b", "text": "The unreasonable\neffectiveness of deep features as a perceptual metric. ", "start_char_idx": 50918, "end_char_idx": 50990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51d53027-0cd9-4540-a68b-dd48152cf47f": {"__data__": {"id_": "51d53027-0cd9-4540-a68b-dd48152cf47f", "embedding": null, "metadata": {"window": "arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer. ", "original_text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5455a826-68c3-4381-ace5-ff5ff6d91551", "node_type": "1", "metadata": {"window": "Adding conditional control to text-to-image diffusion models.  arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. ", "original_text": "The unreasonable\neffectiveness of deep features as a perceptual metric. "}, "hash": "e97170b78e6c5be56d44ff559166780ac670ac835ead4f448da419b5f898264b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d198dbd-653a-4d79-9114-e3227cd189a8", "node_type": "1", "metadata": {"window": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n", "original_text": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. "}, "hash": "420f99e22338f7f8f98d47b12568d4b274d08c5d8e44d79d5625c051d942d062", "class_name": "RelatedNodeInfo"}}, "hash": "c116f331403c7227241b5998007a207ca0c84f7dcc136a6e6ec10f7066af4915", "text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n", "start_char_idx": 50990, "end_char_idx": 51094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d198dbd-653a-4d79-9114-e3227cd189a8": {"__data__": {"id_": "4d198dbd-653a-4d79-9114-e3227cd189a8", "embedding": null, "metadata": {"window": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n", "original_text": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51d53027-0cd9-4540-a68b-dd48152cf47f", "node_type": "1", "metadata": {"window": "arXiv\npreprint arXiv:2302.05543 , 2023.\n [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer. ", "original_text": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n"}, "hash": "c116f331403c7227241b5998007a207ca0c84f7dcc136a6e6ec10f7066af4915", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc6cb691-4a50-4238-93f9-aff8f376e093", "node_type": "1", "metadata": {"window": "The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. ", "original_text": "Towards robust blind face restoration\nwith codebook lookup transformer. "}, "hash": "036db61d36ddfe5f9197ec5e7bab29db9fa5ff545eb007940e7926243584ab07", "class_name": "RelatedNodeInfo"}}, "hash": "420f99e22338f7f8f98d47b12568d4b274d08c5d8e44d79d5625c051d942d062", "text": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. ", "start_char_idx": 51094, "end_char_idx": 51161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc6cb691-4a50-4238-93f9-aff8f376e093": {"__data__": {"id_": "fc6cb691-4a50-4238-93f9-aff8f376e093", "embedding": null, "metadata": {"window": "The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. ", "original_text": "Towards robust blind face restoration\nwith codebook lookup transformer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d198dbd-653a-4d79-9114-e3227cd189a8", "node_type": "1", "metadata": {"window": "[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.  The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n", "original_text": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. "}, "hash": "420f99e22338f7f8f98d47b12568d4b274d08c5d8e44d79d5625c051d942d062", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf72cdf9-c1ea-4a82-bccb-eb4fe6fe3a94", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. ", "original_text": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n"}, "hash": "4533edae29e10a0343128ab34b590b11cb52dcf83f9cf33a26722bb4cd62c2ae", "class_name": "RelatedNodeInfo"}}, "hash": "036db61d36ddfe5f9197ec5e7bab29db9fa5ff545eb007940e7926243584ab07", "text": "Towards robust blind face restoration\nwith codebook lookup transformer. ", "start_char_idx": 51161, "end_char_idx": 51233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf72cdf9-c1ea-4a82-bccb-eb4fe6fe3a94": {"__data__": {"id_": "bf72cdf9-c1ea-4a82-bccb-eb4fe6fe3a94", "embedding": null, "metadata": {"window": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. ", "original_text": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc6cb691-4a50-4238-93f9-aff8f376e093", "node_type": "1", "metadata": {"window": "The unreasonable\neffectiveness of deep features as a perceptual metric.  In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. ", "original_text": "Towards robust blind face restoration\nwith codebook lookup transformer. "}, "hash": "036db61d36ddfe5f9197ec5e7bab29db9fa5ff545eb007940e7926243584ab07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d8d7c5e-013b-43b0-865b-184ad235a289", "node_type": "1", "metadata": {"window": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. ", "original_text": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. "}, "hash": "866fa60e89ea137c34c577427162612f82c2eef466b2bf9af24eed7f35f839ee", "class_name": "RelatedNodeInfo"}}, "hash": "4533edae29e10a0343128ab34b590b11cb52dcf83f9cf33a26722bb4cd62c2ae", "text": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n", "start_char_idx": 51233, "end_char_idx": 51307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d8d7c5e-013b-43b0-865b-184ad235a289": {"__data__": {"id_": "4d8d7c5e-013b-43b0-865b-184ad235a289", "embedding": null, "metadata": {"window": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. ", "original_text": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf72cdf9-c1ea-4a82-bccb-eb4fe6fe3a94", "node_type": "1", "metadata": {"window": "In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 586\u2013595, 2018.\n [68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. ", "original_text": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n"}, "hash": "4533edae29e10a0343128ab34b590b11cb52dcf83f9cf33a26722bb4cd62c2ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4c69cf3-a804-4894-a451-a1d60f19afc7", "node_type": "1", "metadata": {"window": "Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. ", "original_text": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. "}, "hash": "cbdf05bd76f004cf3a5ea42e450ef271db95be6af9eed6a4eaae9310883161e8", "class_name": "RelatedNodeInfo"}}, "hash": "866fa60e89ea137c34c577427162612f82c2eef466b2bf9af24eed7f35f839ee", "text": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. ", "start_char_idx": 51307, "end_char_idx": 51416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4c69cf3-a804-4894-a451-a1d60f19afc7": {"__data__": {"id_": "f4c69cf3-a804-4894-a451-a1d60f19afc7", "embedding": null, "metadata": {"window": "Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. ", "original_text": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d8d7c5e-013b-43b0-865b-184ad235a289", "node_type": "1", "metadata": {"window": "[68] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy.  Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. ", "original_text": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section. "}, "hash": "866fa60e89ea137c34c577427162612f82c2eef466b2bf9af24eed7f35f839ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c87dd377-031c-42a0-8d48-cffa4e53ec87", "node_type": "1", "metadata": {"window": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. ", "original_text": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. "}, "hash": "e3037b7a52deaa19bb422da508e595f5c9e27349c239e5411db816d020c11f87", "class_name": "RelatedNodeInfo"}}, "hash": "cbdf05bd76f004cf3a5ea42e450ef271db95be6af9eed6a4eaae9310883161e8", "text": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. ", "start_char_idx": 51416, "end_char_idx": 51557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c87dd377-031c-42a0-8d48-cffa4e53ec87": {"__data__": {"id_": "c87dd377-031c-42a0-8d48-cffa4e53ec87", "embedding": null, "metadata": {"window": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. ", "original_text": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4c69cf3-a804-4894-a451-a1d60f19afc7", "node_type": "1", "metadata": {"window": "Towards robust blind face restoration\nwith codebook lookup transformer.  Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. ", "original_text": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios. "}, "hash": "cbdf05bd76f004cf3a5ea42e450ef271db95be6af9eed6a4eaae9310883161e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81beb354-7c44-4725-8df1-412198d95c1a", "node_type": "1", "metadata": {"window": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize. ", "original_text": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. "}, "hash": "f4b07c06185a258cbb2c997d8135610aa9a040bc1c2cb7482c32f4930695cf58", "class_name": "RelatedNodeInfo"}}, "hash": "e3037b7a52deaa19bb422da508e595f5c9e27349c239e5411db816d020c11f87", "text": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. ", "start_char_idx": 51557, "end_char_idx": 51671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81beb354-7c44-4725-8df1-412198d95c1a": {"__data__": {"id_": "81beb354-7c44-4725-8df1-412198d95c1a", "embedding": null, "metadata": {"window": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize. ", "original_text": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c87dd377-031c-42a0-8d48-cffa4e53ec87", "node_type": "1", "metadata": {"window": "Advances in Neural Information Processing Systems , 35:30599\u201330611,\n2022.\n 15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. ", "original_text": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur. "}, "hash": "e3037b7a52deaa19bb422da508e595f5c9e27349c239e5411db816d020c11f87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57bd4f3f-6e76-4c9f-82cd-e7e331c6a4ff", "node_type": "1", "metadata": {"window": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. ", "original_text": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. "}, "hash": "7d55af2343c9676a6bcebb347b2c1d2d72e3d6517c78103a11d2860f93402c67", "class_name": "RelatedNodeInfo"}}, "hash": "f4b07c06185a258cbb2c997d8135610aa9a040bc1c2cb7482c32f4930695cf58", "text": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. ", "start_char_idx": 51671, "end_char_idx": 51761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57bd4f3f-6e76-4c9f-82cd-e7e331c6a4ff": {"__data__": {"id_": "57bd4f3f-6e76-4c9f-82cd-e7e331c6a4ff", "embedding": null, "metadata": {"window": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. ", "original_text": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81beb354-7c44-4725-8df1-412198d95c1a", "node_type": "1", "metadata": {"window": "15\n\nA Degradation Details\nDegradation settings used for training our DiffBIR are introduced in this section.  Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize. ", "original_text": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities. "}, "hash": "f4b07c06185a258cbb2c997d8135610aa9a040bc1c2cb7482c32f4930695cf58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "130aa38e-1632-40ce-af3c-6ac3747d5427", "node_type": "1", "metadata": {"window": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. ", "original_text": "Resize. "}, "hash": "bae433bb8454ccfc47c478905a3e65da2b73bcefdb09e32db31b77b88b3e525e", "class_name": "RelatedNodeInfo"}}, "hash": "7d55af2343c9676a6bcebb347b2c1d2d72e3d6517c78103a11d2860f93402c67", "text": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. ", "start_char_idx": 51761, "end_char_idx": 51994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "130aa38e-1632-40ce-af3c-6ac3747d5427": {"__data__": {"id_": "130aa38e-1632-40ce-af3c-6ac3747d5427", "embedding": null, "metadata": {"window": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. ", "original_text": "Resize. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57bd4f3f-6e76-4c9f-82cd-e7e331c6a4ff", "node_type": "1", "metadata": {"window": "Following [ 55], we\nemploy the second-order degradation process to enhance the robustness of the restoration module in\nreal-world scenarios.  Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. ", "original_text": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process. "}, "hash": "7d55af2343c9676a6bcebb347b2c1d2d72e3d6517c78103a11d2860f93402c67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a39145fb-bef7-4313-9d92-5fe305c6ea71", "node_type": "1", "metadata": {"window": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise. ", "original_text": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. "}, "hash": "0cc6ad11ae92ae41e72b2703c44db913087ed48f25fe40f06903be9d226ffffe", "class_name": "RelatedNodeInfo"}}, "hash": "bae433bb8454ccfc47c478905a3e65da2b73bcefdb09e32db31b77b88b3e525e", "text": "Resize. ", "start_char_idx": 51994, "end_char_idx": 52002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a39145fb-bef7-4313-9d92-5fe305c6ea71": {"__data__": {"id_": "a39145fb-bef7-4313-9d92-5fe305c6ea71", "embedding": null, "metadata": {"window": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise. ", "original_text": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "130aa38e-1632-40ce-af3c-6ac3747d5427", "node_type": "1", "metadata": {"window": "Specifically, a degradation model in a certain stage consists of three operations:\nblur,resize , and noise .Blur.  We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. ", "original_text": "Resize. "}, "hash": "bae433bb8454ccfc47c478905a3e65da2b73bcefdb09e32db31b77b88b3e525e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06201481-187f-4efe-a93d-73726b3d7320", "node_type": "1", "metadata": {"window": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "original_text": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. "}, "hash": "24195c5c4eb269ff5319edddfc558334febac6d659efc8f0779f115d0d65de16", "class_name": "RelatedNodeInfo"}}, "hash": "0cc6ad11ae92ae41e72b2703c44db913087ed48f25fe40f06903be9d226ffffe", "text": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. ", "start_char_idx": 52002, "end_char_idx": 52108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06201481-187f-4efe-a93d-73726b3d7320": {"__data__": {"id_": "06201481-187f-4efe-a93d-73726b3d7320", "embedding": null, "metadata": {"window": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "original_text": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a39145fb-bef7-4313-9d92-5fe305c6ea71", "node_type": "1", "metadata": {"window": "We utilize isotropic Gaussian blur or anisotropic Gaussian blur with\nequal probabilities.  The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise. ", "original_text": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize. "}, "hash": "0cc6ad11ae92ae41e72b2703c44db913087ed48f25fe40f06903be9d226ffffe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be556533-e83e-42c6-bdda-c3b3c8a2a938", "node_type": "1", "metadata": {"window": "Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. ", "original_text": "Noise. "}, "hash": "a77d596576cedfcb839ff4d7f776776d4b60fe78cd999764f2fe5cec3441fe1b", "class_name": "RelatedNodeInfo"}}, "hash": "24195c5c4eb269ff5319edddfc558334febac6d659efc8f0779f115d0d65de16", "text": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. ", "start_char_idx": 52108, "end_char_idx": 52284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be556533-e83e-42c6-bdda-c3b3c8a2a938": {"__data__": {"id_": "be556533-e83e-42c6-bdda-c3b3c8a2a938", "embedding": null, "metadata": {"window": "Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. ", "original_text": "Noise. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06201481-187f-4efe-a93d-73726b3d7320", "node_type": "1", "metadata": {"window": "The size of the blur kernel follows a uniform distribution ranging from 7 to\n21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process\nand between 0.2 and 1.5 for the second degradation process.  Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "original_text": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process. "}, "hash": "24195c5c4eb269ff5319edddfc558334febac6d659efc8f0779f115d0d65de16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07551196-8f79-41f3-8832-cb5cd682f1d6", "node_type": "1", "metadata": {"window": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. ", "original_text": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. "}, "hash": "ea82ae5d88b6bd21c98f4dacc9b33bd666e0ca30126e906293e33c1b706b3ca4", "class_name": "RelatedNodeInfo"}}, "hash": "a77d596576cedfcb839ff4d7f776776d4b60fe78cd999764f2fe5cec3441fe1b", "text": "Noise. ", "start_char_idx": 52284, "end_char_idx": 52291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07551196-8f79-41f3-8832-cb5cd682f1d6": {"__data__": {"id_": "07551196-8f79-41f3-8832-cb5cd682f1d6", "embedding": null, "metadata": {"window": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. ", "original_text": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be556533-e83e-42c6-bdda-c3b3c8a2a938", "node_type": "1", "metadata": {"window": "Resize.  We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. ", "original_text": "Noise. "}, "hash": "a77d596576cedfcb839ff4d7f776776d4b60fe78cd999764f2fe5cec3441fe1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "313a755d-0a41-40c7-87ee-4076e4016ce3", "node_type": "1", "metadata": {"window": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n", "original_text": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. "}, "hash": "002a9f763c724ac4091509274e98b3aa4aff96bac720bb899d7c360b333dc24b", "class_name": "RelatedNodeInfo"}}, "hash": "ea82ae5d88b6bd21c98f4dacc9b33bd666e0ca30126e906293e33c1b706b3ca4", "text": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. ", "start_char_idx": 52291, "end_char_idx": 52365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "313a755d-0a41-40c7-87ee-4076e4016ce3": {"__data__": {"id_": "313a755d-0a41-40c7-87ee-4076e4016ce3", "embedding": null, "metadata": {"window": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n", "original_text": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07551196-8f79-41f3-8832-cb5cd682f1d6", "node_type": "1", "metadata": {"window": "We consider multiple resize\nalgorithms, including area resize, bilinear interpolation and bicubic resize.  The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. ", "original_text": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise. "}, "hash": "ea82ae5d88b6bd21c98f4dacc9b33bd666e0ca30126e906293e33c1b706b3ca4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0b50cf0-cafe-4f10-8368-54c8d83cd893", "node_type": "1", "metadata": {"window": "Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration. ", "original_text": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. "}, "hash": "1ff388d137f5dd280e31f3d57b5cee579deceee863e3b305f9b0ebca28528929", "class_name": "RelatedNodeInfo"}}, "hash": "002a9f763c724ac4091509274e98b3aa4aff96bac720bb899d7c360b333dc24b", "text": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. ", "start_char_idx": 52365, "end_char_idx": 52520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0b50cf0-cafe-4f10-8368-54c8d83cd893": {"__data__": {"id_": "f0b50cf0-cafe-4f10-8368-54c8d83cd893", "embedding": null, "metadata": {"window": "Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration. ", "original_text": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "313a755d-0a41-40c7-87ee-4076e4016ce3", "node_type": "1", "metadata": {"window": "The scaling factor for\nresize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and\nfrom 0.3 to 1.2 for the second degradation process.  Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n", "original_text": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process. "}, "hash": "002a9f763c724ac4091509274e98b3aa4aff96bac720bb899d7c360b333dc24b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfcb3c74-f321-4320-a674-c6e0b5a2b6e6", "node_type": "1", "metadata": {"window": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. ", "original_text": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n"}, "hash": "e281de46ccfc231397af0ea4305277791f109e8fe513340b37f511cdfb4996ae", "class_name": "RelatedNodeInfo"}}, "hash": "1ff388d137f5dd280e31f3d57b5cee579deceee863e3b305f9b0ebca28528929", "text": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. ", "start_char_idx": 52520, "end_char_idx": 52660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfcb3c74-f321-4320-a674-c6e0b5a2b6e6": {"__data__": {"id_": "dfcb3c74-f321-4320-a674-c6e0b5a2b6e6", "embedding": null, "metadata": {"window": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. ", "original_text": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0b50cf0-cafe-4f10-8368-54c8d83cd893", "node_type": "1", "metadata": {"window": "Noise.  We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration. ", "original_text": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively. "}, "hash": "1ff388d137f5dd280e31f3d57b5cee579deceee863e3b305f9b0ebca28528929", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30596029-e1f0-4743-bf1b-2149adb179ad", "node_type": "1", "metadata": {"window": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n", "original_text": "Moreover, we combine the degradation settings adopted in blind face restoration. "}, "hash": "95b2c195cdac74830e2ecb2cc529dbce74e8d8e7a5c12db07d481e802e5061ac", "class_name": "RelatedNodeInfo"}}, "hash": "e281de46ccfc231397af0ea4305277791f109e8fe513340b37f511cdfb4996ae", "text": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n", "start_char_idx": 52660, "end_char_idx": 52746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30596029-e1f0-4743-bf1b-2149adb179ad": {"__data__": {"id_": "30596029-e1f0-4743-bf1b-2149adb179ad", "embedding": null, "metadata": {"window": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n", "original_text": "Moreover, we combine the degradation settings adopted in blind face restoration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfcb3c74-f321-4320-a674-c6e0b5a2b6e6", "node_type": "1", "metadata": {"window": "We incorporate Gaussian noise, Poisson\nnoise, and JPEG compression noise.  The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. ", "original_text": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n"}, "hash": "e281de46ccfc231397af0ea4305277791f109e8fe513340b37f511cdfb4996ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2b34315-afb7-4d4d-83a9-020d962ead26", "node_type": "1", "metadata": {"window": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset.", "original_text": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. "}, "hash": "4805422eec5f24f2423f769006710b78431847d0cc2106a7de99051f4dcd664b", "class_name": "RelatedNodeInfo"}}, "hash": "95b2c195cdac74830e2ecb2cc529dbce74e8d8e7a5c12db07d481e802e5061ac", "text": "Moreover, we combine the degradation settings adopted in blind face restoration. ", "start_char_idx": 52746, "end_char_idx": 52827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2b34315-afb7-4d4d-83a9-020d962ead26": {"__data__": {"id_": "b2b34315-afb7-4d4d-83a9-020d962ead26", "embedding": null, "metadata": {"window": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset.", "original_text": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30596029-e1f0-4743-bf1b-2149adb179ad", "node_type": "1", "metadata": {"window": "The scale of Gaussian noise is uniformly sampled between 1\nand 30 in the first degradation process and between 1 and 25 in the second degradation process.  The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n", "original_text": "Moreover, we combine the degradation settings adopted in blind face restoration. "}, "hash": "95b2c195cdac74830e2ecb2cc529dbce74e8d8e7a5c12db07d481e802e5061ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dcc1967-78bd-4213-9fda-2f930633742c", "node_type": "1", "metadata": {"window": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset.", "original_text": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n"}, "hash": "b43a664aeacbade3505da4f4d85713db15513a3d2e5c943df74af735f9a65c20", "class_name": "RelatedNodeInfo"}}, "hash": "4805422eec5f24f2423f769006710b78431847d0cc2106a7de99051f4dcd664b", "text": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. ", "start_char_idx": 52827, "end_char_idx": 52949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dcc1967-78bd-4213-9fda-2f930633742c": {"__data__": {"id_": "1dcc1967-78bd-4213-9fda-2f930633742c", "embedding": null, "metadata": {"window": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset.", "original_text": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2b34315-afb7-4d4d-83a9-020d962ead26", "node_type": "1", "metadata": {"window": "The\nscale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second\ndegradation processes, respectively.  The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset.", "original_text": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12]. "}, "hash": "4805422eec5f24f2423f769006710b78431847d0cc2106a7de99051f4dcd664b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e280d40-975b-4328-881e-7161ae5de011", "node_type": "1", "metadata": {"window": "Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset."}, "hash": "67ef770d9dfaa898db560fc76727513fc8e41492340a0ade28c219a3619af59f", "class_name": "RelatedNodeInfo"}}, "hash": "b43a664aeacbade3505da4f4d85713db15513a3d2e5c943df74af735f9a65c20", "text": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n", "start_char_idx": 52949, "end_char_idx": 53047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e280d40-975b-4328-881e-7161ae5de011": {"__data__": {"id_": "7e280d40-975b-4328-881e-7161ae5de011", "embedding": null, "metadata": {"window": "Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dcc1967-78bd-4213-9fda-2f930633742c", "node_type": "1", "metadata": {"window": "The quality of JPEG compression follows a uniform distribution\nranging from 30 to 95.\n Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset.", "original_text": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n"}, "hash": "b43a664aeacbade3505da4f4d85713db15513a3d2e5c943df74af735f9a65c20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f29d3426-e28a-4838-a6ab-6c2802c1e0ac", "node_type": "1", "metadata": {"window": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset."}, "hash": "1b4c758ec90bef24bf92d8f8c8ee162ab4733606a3e4b13e334a70462ce41235", "class_name": "RelatedNodeInfo"}}, "hash": "67ef770d9dfaa898db560fc76727513fc8e41492340a0ade28c219a3619af59f", "text": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset.", "start_char_idx": 53047, "end_char_idx": 53154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f29d3426-e28a-4838-a6ab-6c2802c1e0ac": {"__data__": {"id_": "f29d3426-e28a-4838-a6ab-6c2802c1e0ac", "embedding": null, "metadata": {"window": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e280d40-975b-4328-881e-7161ae5de011", "node_type": "1", "metadata": {"window": "Moreover, we combine the degradation settings adopted in blind face restoration.  Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset."}, "hash": "67ef770d9dfaa898db560fc76727513fc8e41492340a0ade28c219a3619af59f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a2e85cb-59a5-4cfe-97b8-f81bead9645b", "node_type": "1", "metadata": {"window": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "hash": "b191418393281e9fd5e2071e1290cf43c6827a5deb2b862f7ae0b90813f8534f", "class_name": "RelatedNodeInfo"}}, "hash": "1b4c758ec90bef24bf92d8f8c8ee162ab4733606a3e4b13e334a70462ce41235", "text": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset.", "start_char_idx": 53154, "end_char_idx": 53250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a2e85cb-59a5-4cfe-97b8-f81bead9645b": {"__data__": {"id_": "0a2e85cb-59a5-4cfe-97b8-f81bead9645b", "embedding": null, "metadata": {"window": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f29d3426-e28a-4838-a6ab-6c2802c1e0ac", "node_type": "1", "metadata": {"window": "Specifically, we\nconsider a large dowsampling range [1,12], and a large blur kernel range whose sigma is within\n[0.1,12].  In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset."}, "hash": "1b4c758ec90bef24bf92d8f8c8ee162ab4733606a3e4b13e334a70462ce41235", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9e7244a-25e6-4df7-b8f1-f44d53217a3e", "node_type": "1", "metadata": {"window": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "original_text": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "hash": "c77de92e0e545d17d12ec0f2be4790e4c97514843a91b2dd27ef39aa8bf57657", "class_name": "RelatedNodeInfo"}}, "hash": "b191418393281e9fd5e2071e1290cf43c6827a5deb2b862f7ae0b90813f8534f", "text": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset.", "start_char_idx": 53250, "end_char_idx": 53451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9e7244a-25e6-4df7-b8f1-f44d53217a3e": {"__data__": {"id_": "f9e7244a-25e6-4df7-b8f1-f44d53217a3e", "embedding": null, "metadata": {"window": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "original_text": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a2e85cb-59a5-4cfe-97b8-f81bead9645b", "node_type": "1", "metadata": {"window": "In this way, the generation module is trained to remedy the information loss within a wide\nrange.\n B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset.", "original_text": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "hash": "b191418393281e9fd5e2071e1290cf43c6827a5deb2b862f7ae0b90813f8534f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73a7f987-771f-4a99-9025-f18204f71cc6", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "original_text": "( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "hash": "fe909022451f2b8851925619a6dee96fb46c3617829a2e09764e6d09251f8f41", "class_name": "RelatedNodeInfo"}}, "hash": "c77de92e0e545d17d12ec0f2be4790e4c97514843a91b2dd27ef39aa8bf57657", "text": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset.", "start_char_idx": 53451, "end_char_idx": 53547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73a7f987-771f-4a99-9025-f18204f71cc6": {"__data__": {"id_": "73a7f987-771f-4a99-9025-f18204f71cc6", "embedding": null, "metadata": {"window": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "original_text": "( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9e7244a-25e6-4df7-b8f1-f44d53217a3e", "node_type": "1", "metadata": {"window": "B More Qualitative Comparisons For BFR\nFigure 9: More qualitative comparisons for BFR on synthetic dataset. ( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "original_text": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "hash": "c77de92e0e545d17d12ec0f2be4790e4c97514843a91b2dd27ef39aa8bf57657", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9766d90b-0f4a-42c8-95b8-90e6b5b4cf88", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset."}, "hash": "3fdda749345cf077417b9c1ce742477805a2d966a854e822295f7802359d27ed", "class_name": "RelatedNodeInfo"}}, "hash": "fe909022451f2b8851925619a6dee96fb46c3617829a2e09764e6d09251f8f41", "text": "( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset.", "start_char_idx": 53547, "end_char_idx": 53643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9766d90b-0f4a-42c8-95b8-90e6b5b4cf88": {"__data__": {"id_": "9766d90b-0f4a-42c8-95b8-90e6b5b4cf88", "embedding": null, "metadata": {"window": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73a7f987-771f-4a99-9025-f18204f71cc6", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\nFigure 10: More qualitative comparisons for BFR on real-world dataset. ( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "original_text": "( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset."}, "hash": "fe909022451f2b8851925619a6dee96fb46c3617829a2e09764e6d09251f8f41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f769458-ff9f-4215-b3e4-77c0a13bf4b9", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset."}, "hash": "bb58d23f372b8ccf688cf6242f0e15b761d9ae58136f55d3ab70ebd114773b1f", "class_name": "RelatedNodeInfo"}}, "hash": "3fdda749345cf077417b9c1ce742477805a2d966a854e822295f7802359d27ed", "text": "( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "start_char_idx": 53643, "end_char_idx": 53878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f769458-ff9f-4215-b3e4-77c0a13bf4b9": {"__data__": {"id_": "9f769458-ff9f-4215-b3e4-77c0a13bf4b9", "embedding": null, "metadata": {"window": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9766d90b-0f4a-42c8-95b8-90e6b5b4cf88", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n16\n\nC More Qualitative Comparisons For BSR\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\n(b)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset."}, "hash": "3fdda749345cf077417b9c1ce742477805a2d966a854e822295f7802359d27ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05e71140-a1bf-4f9c-ab3b-579ffbde6cac", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n21"}, "hash": "7566faf9b0e740befce7e17d26b796f7da157cf99fd5ecd8e490b55ea6c09c9b", "class_name": "RelatedNodeInfo"}}, "hash": "bb58d23f372b8ccf688cf6242f0e15b761d9ae58136f55d3ab70ebd114773b1f", "text": "( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset.", "start_char_idx": 53878, "end_char_idx": 54114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05e71140-a1bf-4f9c-ab3b-579ffbde6cac": {"__data__": {"id_": "05e71140-a1bf-4f9c-ab3b-579ffbde6cac", "embedding": null, "metadata": {"window": "( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n21"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "887e3cc1-9b34-47b2-888e-bbef4955dd81", "node_type": "4", "metadata": {}, "hash": "84058a9865f75c803c1174d2a9b5b6efb8109bed7084d4425b26851e32852fc0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f769458-ff9f-4215-b3e4-77c0a13bf4b9", "node_type": "1", "metadata": {"window": "( Zoom in for best view )\n17\n\n(c)\n(d)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n18\n\n(e)\n(f)\nFigure 11: More qualitative comparisons on Real47 dataset. ( Zoom in for best view )\n19\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(a)\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)LQ DDNM GDP BSRGAN\n(b)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset. ( Zoom in for best view )\n21", "original_text": "( Zoom in for best view )\n20\n\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(c)\nLQ DDNM GDP BSRGAN\nSwinIR -GAN Real-ESRGAN+ FeMaSR DiffBIR (Ours)\n(d)\nFigure 12: More qualitative comparisons on RealSRSet [24] dataset."}, "hash": "bb58d23f372b8ccf688cf6242f0e15b761d9ae58136f55d3ab70ebd114773b1f", "class_name": "RelatedNodeInfo"}}, "hash": "7566faf9b0e740befce7e17d26b796f7da157cf99fd5ecd8e490b55ea6c09c9b", "text": "( Zoom in for best view )\n21", "start_char_idx": 54114, "end_char_idx": 54142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"887e3cc1-9b34-47b2-888e-bbef4955dd81": {"node_ids": ["e8274ee9-ba00-42ce-bb26-4d8187390e23", "cef4feef-1d4b-4a19-8f2d-eddf42571fc0", "4a160cad-319d-457e-a464-c290c923a7aa", "c5096675-ddbc-4845-b44f-0c7f13dce6af", "ca42b2d9-b8a6-4e5e-88ca-8eee1aede0c0", "f451bfee-e8cc-4a42-ba43-c071a99f1a39", "90935691-b8d0-40a5-a6bf-2d230b2e5984", "1b83f7f3-d247-4137-8680-c85e508db105", "4557d954-490a-4380-8fb3-1c75ff766294", "0d061be7-7b18-46da-af7a-4ef12412709b", "81d44862-df30-410c-95c3-3c7d8cf56f26", "878f74d8-f2eb-4a3e-9200-6047c5dc8e3f", "3c80912f-8ad6-4a0c-9c7f-0bff0150fc32", "b15c81e7-8222-42d7-b86f-9adb2ae16a23", "0ce69935-20c6-448c-bf4e-163ea822990d", "ce98d52d-904e-4928-93e9-a8d03ecbddcd", "97413fd0-d6fd-4277-970d-e5a877e61e2f", "d609659d-5cb6-4ba9-bc98-399080737c58", "d6f1e251-e584-4bda-b3d8-75e38b5a46d3", "9c0b321f-7758-4ecf-9643-0e364cc6e45a", "0ecd85f8-dbaf-4b55-89e9-716765432483", "664f34c4-55ba-4f85-ab87-6bd62c4a68f1", "cbab38fa-d062-4e81-9927-37234979f613", "183b765f-6eed-470b-97f3-9117d6259e52", "abeaa74d-f8f1-4534-a241-1403569a365d", "0a5d1693-cbd7-4bbc-8344-0c99b4422256", "5cde80f3-7689-45fe-a5a2-0f45ac271e0b", "260c04c9-89e8-4ed3-8031-ced614c6c6b3", "c0217ff6-b34e-419b-8cf7-7d8251b47bb7", "254b2434-cdc3-4bb2-8694-b99f3485c28c", "379ad591-952e-4f83-bc9b-07f1c2508045", "daf2519e-01f9-4177-b2c1-eb77cba5c722", "52be0b8c-ee5b-4aef-a516-2ab212d4aa55", "6327bac8-7c0d-4084-83cb-1f71dbfb0d87", "aeee6eeb-44a0-4739-88f2-8e1edef5dc7b", "2773337b-1970-4419-a4a4-41afe0252c8c", "7658497d-6e31-48a6-a62b-f2ec2a7c5ca9", "ac532669-0517-4549-bafb-a9586ffb2de4", "41b6e169-cbbc-47a7-9722-c28ba70d5afa", "3d13f11f-b9cc-46ad-8d01-152018f095f3", "ffb5a522-16c7-4b7a-8769-9d2eafe99a9b", "8eb0c6bd-c0bf-4c08-8d93-c32405ae5c8b", "aa4abd91-d3be-47cf-8d6a-83ac560d7751", "dc4b1c60-0e08-4821-b3ae-31fd4d27191e", "bb327243-c8a4-4c59-b415-25c9ae61b3d9", "7ada15e1-1706-4b99-8de7-bfeb93ac7f24", "e8d4dc0c-3e90-4cf1-8ada-36bab89b747b", "bb707d36-c930-45dc-b610-aeef6228e73a", "51e316c2-47fd-4b28-a986-2a8d1a32f0e8", "8e223b2f-e38c-4ca5-a0fe-e486da202ed4", "790146e6-db42-4ac4-a7c1-7dd8773931fc", "84516289-b954-44cc-92cc-e080c0196b4e", "3df342d2-23b1-492d-97a5-de874e9371ec", "c872629b-d5b5-4cc3-9c87-954a38042f3d", "57a41937-44b4-43dc-ab96-cf11ab655018", "47fe2c58-b375-49d0-ba3d-fad724367224", "c28f0c5f-fad2-4470-860e-27fede132336", "2c2cc220-673d-4eb2-a2fb-327889bce696", "3384bcd0-4a73-42af-bead-9bbfecf42bf3", "642380f3-ed24-47a1-a6ca-8912e4eaf7f1", "6962488e-43aa-44de-8451-7c7ccc73d9e8", "884f7d0a-de8d-43aa-ae8c-7965db26117f", "a6719bd5-f89c-401c-8fbb-3d60bc3ba0e6", "94fad515-7751-4b16-8ed5-d439effc9231", "97dcdd9f-eec4-42bb-b926-8d4dcd350397", "bc0b8576-da5d-432c-8d75-c4f75f1f13f5", "36282957-700b-4bd4-95a3-f76d9c2413f5", "92c1a58f-bbe1-49ad-99da-c5301979a82d", "c0571fe7-563e-409a-a4e4-aabc59474fa7", "7ed9eb4f-7b31-4c86-bc67-1e695dfb0657", "1f02c0ba-3d10-4b27-9769-6e2e4bcb0736", "d08cfaa1-3159-4bcc-8be2-918c79e8ad0c", "7fc2e7cf-ace6-4932-8343-71fc9495ae9b", "0dc339dd-6e81-4547-a747-c52c74c30495", "d79ef500-0789-4b32-a9d5-1e9ed2051e49", "d994b7ba-d5ed-4a88-a96a-bd1150633426", "c28ad670-06c2-4017-8ec2-6f87ecb6bcf4", "ec3aa80a-b79a-4aa5-95a2-a29c72a69324", "f0c841ea-f297-416f-9bf9-4a3d6c4aaf05", "cbf618e2-3e59-4a0b-9ae1-d2bbb16fdf2a", "8dfbe85e-6966-4006-9e76-9e38a662161f", "d54808d6-2445-491a-ae50-ad8c4c63381c", "31d1dbd3-f2e1-4439-ac0a-6111e722609b", "108c69ac-2118-46d1-ab50-b69c77fb67e7", "9f0966f5-5b83-47bc-b176-0d2310b381b0", "58388634-5209-4f2d-a2da-60c4305ecb70", "3198f679-0ea6-438b-b0d6-89935847b6c8", "277bcec3-4472-4755-a59f-b499fd04c445", "18d0b5ac-36f0-43c0-b443-ddf2813c5eec", "3932a3dd-55e3-40f8-ae62-598f2afa79cd", "a5a53aff-a935-4ac9-846a-ead97a8e8e4d", "3f2bd9d5-4ee5-4f73-854b-327cc4dbd9c2", "f729335c-213c-4ece-b08a-7e3c7c3aa559", "168d5bbf-d925-4e19-a6a6-c4a4d1db67fa", "294f88cf-6923-4b09-9a65-414506bbddef", "4fdb4e19-0761-47c8-8649-c916fe765000", "103621d0-09fd-4a91-9824-e920671a949c", "4bb20122-108a-495f-9434-2aca0c87ac3f", "823cd9e1-d006-4be6-b7cb-a14dc1b0006a", "cb8cc6b7-1528-4505-a61d-947cea801c73", "ccb3a3bb-aef2-430f-8d9a-3658fa601db7", "f1e06b3e-05ab-4f2b-adb6-c98cdd3db909", "eb1c5e7c-462d-4d08-927f-acf5f9ab5c19", "a11ee045-9d8e-4996-9140-ecf6810450d8", "4dc50bf4-82de-4080-b678-0e360d6fe086", "3e9f65b0-2963-4492-ba06-2a6f06a5100b", "dca689b3-b3a4-4724-9c69-8a9f5095138b", "27337c73-3249-47b9-a4df-a3c2012f6bf9", "c02b2f67-ed5e-4583-af70-80b6b64084c6", "66de200d-fbee-4b88-946d-7cc5a8489ad9", "13671ef4-4481-46d9-b05a-e3ad81862097", "e5ea6a92-abe3-4cb5-a163-620674cb2398", "2509fc2a-c8f5-45b2-a7f3-887211b78c35", "75f1e5f6-0ec8-439b-ac40-0956e9fc29da", "495e43cd-2597-4f13-adc8-cb97d8c8e7c7", "c3621708-7c95-47a7-8b1b-98581467aa5a", "93723c87-5488-4771-9776-6672db8b8e7a", "a2bc1c73-b4b3-4390-b218-983589b51d27", "02de84cb-85c0-4d9c-85d5-a52d503d661d", "c5a477a2-3f67-46b7-8ee1-fdeff571a1f9", "1a6a2fe9-9a10-4ba6-8d07-9abc3d1f27d0", "46e0dda1-f936-48c3-b22c-ff5212489ded", "42c67bed-fca5-4f55-9b4f-009b25892767", "3806bbf1-78d0-4360-9b31-0a60f8a10a5f", "678fa503-2bce-40f0-ba88-fc0ccf92a4c2", "04ee7a19-562d-43d6-86c1-8548f4b098ee", "83dbe5a6-94fe-4e7d-8c76-b84a51e03d11", "14abff48-9c37-4f8e-aab9-959750f29fd5", "1cd2ea3b-2962-42f6-84d8-f5761861bd91", "913ee0ee-58e5-4826-a7c6-099db479c482", "070c0135-aaa4-4288-b5c9-c893ced04488", "48005c0a-b136-4801-bdf2-5aeecf19e1b7", "6b45a12c-a4d3-4c52-bead-ad68bd0d4db0", "29a8f5b4-53ea-4370-aa21-12ab943fe281", "0520a598-6f1d-438b-8abc-c8030bd3d7de", "0d782296-551d-4f09-b0eb-a8e7ad1d40f7", "f1d93519-ba46-4f0c-8a3b-fd876e1bfc21", "629e172c-a952-439d-be4b-3daa3ce21ad4", "cc2b3f85-e35a-4544-a3ec-0a0258d53062", "fc3c035c-e442-435e-bfca-2cf4c39fc81b", "b53d5e8c-663e-40f8-a993-28bc575b5264", "ba0a66c8-f78a-45af-860b-0d8920f9abcc", "b7fb0487-861c-4b5c-8777-6298ebd7f042", "06723684-d28f-4264-ae93-3c45472e303f", "8f4c2e00-0a66-4bb6-bf3f-bb07d53cfe39", "45579813-f07c-4402-ab9f-7bc300497375", "04449c97-0212-4bbf-a878-131e565c272e", "4b497066-45bc-4644-a27f-b19751a365cc", "f790045d-75cb-4878-9105-e96e28c20c25", "79e81508-68de-4331-9222-ab284c0cb200", "0ab2e8b3-e8b5-48ce-8977-857dde8ac794", "36fd7f88-a727-41f6-8342-439255a48fae", "96b0fc55-79ab-4d65-bd3c-654c8f71edd5", "29d78105-794b-4253-9e0b-e5d2aef31361", "4b0b288b-3e05-4deb-87e9-c98f464b41ab", "3307634e-2960-4b3b-927d-cec61ea06947", "3fe55192-eed2-41a1-a063-324650fe1ea6", "e6c36773-b7fa-4d13-9be8-0340134b6286", "d03dd2ca-e113-4f6d-9980-7518e474b2a3", "651b2efd-5513-42be-a84d-942792bec463", "fac5c673-bd74-487b-8a26-e656f0d28350", "b0f9908e-5a15-4bfe-9a0d-13c03d44ad48", "6b523d44-2250-4857-a871-2bb78ab13fcb", "b56eb80c-01e2-4675-957b-db0cec44214a", "927e2f63-ced6-434e-bfbc-d696d3bfadef", "c618e234-385d-46d5-a104-2aeab6ad1e65", "82bb0c82-4d67-495b-89b8-1d57def2b5bc", "3a1b692b-9874-46d6-938b-65d7b6c83322", "71e4855e-6988-4833-aab9-a789477d5bb3", "7345b916-0dc3-4540-868a-17b763463334", "31e4fa41-4d6d-416a-b1dc-050441f1938a", "9a1019bb-d966-4d86-bc79-9684f1261dbd", "34639589-cbbe-44a8-93d1-aee506a563fa", "e5415ceb-c3d5-4806-8a5b-ecedcec97be2", "345c5d6f-d714-4ff2-8833-002296696fdb", "87934093-e51c-42eb-b07e-51cc0e6a372e", "563511a7-2bf4-40af-9098-6a3009de89c5", "0689e590-e22d-4f37-96a5-9a1f2c4f9d20", "345ea9a5-3f79-4e90-a55e-12994dde510e", "b1d86b3a-cc1c-4032-b806-174802bfc5c2", "da878723-76b7-4dd1-a7ba-960fac34eb88", "4d52bfb7-ed49-460f-856f-2aa1480dbf7f", "95852e4c-a347-4e78-82a1-ccd6af86e20e", "60695295-822d-4ba0-ba1e-6b7cb7a6f1f0", "847dda23-caf9-49ad-b1ad-116461e72413", "d0da254b-15e7-4689-9f45-9ae50beb6c19", "fee909e6-6eb8-4e74-a8f0-2159f2afca77", "ee83116d-3f44-48d6-8687-6638fdeec590", "c1eb57a0-43f8-4ab6-8c84-974e7d9f214b", "69b8b300-58d7-45e2-8f75-4414861b9962", "94e739f6-b404-442b-96b8-cdfa3a38bfca", "0e776fe8-5ab0-4ffa-abdf-c4c8001c8174", "f8551b4d-1563-4d08-8147-89bd31457d05", "fa871314-45b4-471d-ba69-713bcec0b24b", "3e1ad868-4215-40b4-b059-cc80284ee86f", "c2c50208-89c6-4dea-bd2e-df578c65a5c0", "0978c9ea-ffe5-49a1-ba91-57fe0489d955", "cf1be144-1eb0-4032-8f76-aec2553c4733", "64a0fbb3-f32b-4316-96c2-9a17a9f13a06", "ce38a0d4-3458-4ead-8f4a-dbebc017863a", "e4e38081-d462-4a9d-9019-83a009d29477", "60f8db2b-dc9a-4ea8-9f41-143b7c027685", "a6c6399d-9912-4417-9a88-61005b0fef04", "8f5f4da7-7a40-41d6-931e-9ad2ef9062cc", "d50cd03e-a76a-4e3e-a98c-0b1f399ae8f5", "109b8a70-88f3-49bc-a908-26e6ad69c290", "cf9dee5c-8e10-4142-b1e6-3f8509f036dc", "611eb5a8-c788-4d0d-9560-e0886aa41273", "afabe86c-a91b-42e2-837a-055c1fe4f43a", "264a7da6-5672-46cc-a9d9-2883e6cbab59", "0a3c8b2e-333d-4985-84b9-c95d5d75658b", "ae8e34f0-f350-4f1e-ba4e-6e48e8ac8785", "ea68df9c-cf46-496d-abf7-aa2a033ae811", "5e28c622-61bc-4e76-ab50-d6b305eabb25", "fcdc9722-a8b3-4f5e-98f0-7ac9a518358a", "b2bff15d-6780-40f2-941d-b2efe98e3358", "7b4eee06-f1c4-48a2-b404-796a5a74790f", "40b27d67-e31b-4212-b1b7-c835225f8bb3", "94ad96d7-0a9f-4ac1-b0de-1c5eb0c0e9b3", "3d025fa7-b0f7-4787-b39d-ec299f44664d", "8936bab0-ae00-43f7-b24e-b54d3fbbb340", "2201a452-7b9d-4dd8-8491-457e36161411", "fe3b84ff-a1ca-4325-8237-32d9390e302d", "a3191d42-95b2-4a7a-ae67-f149bee5fd76", "22435e08-5926-4e7e-8367-8ca08d528a88", "0673045b-dbd3-442e-b459-5232f9143787", "ea357866-e618-4e1b-bb5f-36d216bba1ca", "43bdd6e7-8794-49be-afc9-a85a9be48750", "5bae9b51-7b4f-48ec-9c18-0de7e4606164", "8afd5ff6-7d40-4bbd-9b77-8f22deff6342", "f0a1e8a3-324d-463c-afeb-c492ed9daa2f", "5e3548b1-1470-47c6-bec4-36389d703f92", "da45dcc2-adc2-42dc-8ceb-e72a331052d1", "b37ddbcd-97ea-4547-b4a6-a68f00ef1057", "dfb5de91-db01-4939-bc33-3616592cd6c2", "2e5cefba-4137-4228-bc24-b083b010f990", "6b281d00-d5f4-4aa5-8ecf-cc1969f2e781", "25c39189-98a2-4e25-8fd1-d516a41830bd", "1a181d98-6a92-4c06-9794-d3230b963753", "3dbda129-b751-4b3a-9fe5-e73b0c20b721", "423c5cf0-a213-4c56-b664-06053a2ebe72", "f76bebaf-e95b-4e88-87ca-1a3c4d44c887", "a865b445-e410-4f5c-9381-4c3ee63d043b", "180a0f06-0e26-46c9-acdc-a6710785befe", "e8c50c35-4031-4542-a6a1-b6986dccffe8", "7a1490e0-96ca-4506-acaf-953269b8d3db", "8c1158d7-90e4-468e-82cb-473f18fe992f", "9fc0df74-0385-4391-b4f9-0eb20b6f1b64", "842d6c2c-8db7-4e4b-aa8e-e1f6c557335b", "57cd9472-f5c5-45a8-9a29-d8d2123ad685", "dd3bda7a-cbff-47b5-8188-eb636b7d35ac", "9c81086e-54a6-412c-9b4c-c1e893ee279a", "1021aa89-6d09-48da-89ed-d9933d98a680", "8dbb3559-30eb-4045-bc5d-b26036e0e7fb", "f894422d-3968-47b3-91b2-e2cb16415f3b", "e78df41f-e7b1-4c05-b561-bfdb4dfc2c50", "058156a7-ba76-4939-8605-49d9089f2432", "57f06cc8-c3b3-4108-8fdb-fef473001cb0", "2a90fac1-9e61-4a8e-9638-e3d03abaa1b6", "e7fa000e-ae3f-4d09-96a6-e19138952711", "63bf8bc5-309e-4e5e-97e5-cb4698f3f0b4", "bbeefa32-1443-4b83-959e-12764f6a48a7", "c5539238-3d99-4cd0-8356-000c20717f18", "b01e6783-bcdd-4cb1-95ab-d81319d2e058", "c3ecce72-200e-4c51-bbb3-72403063afea", "17bd6cbc-9deb-4f49-b774-c62157218fe3", "bf7530db-91fa-4d03-ab84-d2127ebdd1c8", "4691b7b8-6508-4d11-97a9-e132e3bc0359", "f615dd0e-f8e5-4440-8a6e-f0384f4f52d0", "0511131a-c1cc-492d-9989-a243d5878fdf", "1499dfec-f332-4037-8fa4-78e5fd12c2a7", "8746bcd0-ba02-4e73-9775-453b211a040a", "0a8fb800-b1db-4b6c-9952-0ddcd1a246e0", "beacdb7c-03e6-4f71-9462-8d7e86c9cdd4", "43c44ed8-29ca-44e8-ac59-be54635cfe89", "a2520686-96e4-4af7-b4db-e38b9a52308e", "4c87ffe8-6fb5-469c-9add-7192476da0d3", "067a3710-de15-438a-9556-5429e720fc7a", "171df040-b833-491e-bb1b-547e6c62a3ef", "ba348eff-24eb-4092-8f3e-8d1be3f45724", "0b97be60-e83e-402f-9995-b3778d387316", "f0a51584-18b0-4486-8860-71e75ba39ca5", "df40838d-7f0f-43ea-86f7-57120e7e3000", "542995bd-ee2f-447e-b444-68c20e2f84f0", "f78bf349-1623-4609-8018-5d562a67df47", "7f02815e-b849-476c-8aad-e3f34f4a23bb", "4780c862-03a1-4ba5-8121-01b3c23731a1", "86e8c524-ca71-4d9e-b471-d14965af5916", "cfbb1950-5a50-46db-8ff7-aaec2905ea27", "9f0c6007-01fc-4a1f-a45a-0530a6948c5d", "bc6a7f0c-e602-4534-bae7-a17ac229b32a", "2003c2fe-2558-44f8-b4d1-e83e586c839a", "39b7232a-0cdc-4f7d-9ce0-55c058fc8ef6", "481de39b-d027-41c3-b85b-eaadf3dd1940", "3ecd8886-1272-4eec-9501-fb62bd91ef9d", "ab8c725a-40f4-425a-bdaa-db1e62da05d5", "0ce6abe5-93a3-4c94-9970-0f8a1977241e", "9e6842f3-9e9a-4811-9956-01a82c9d9b08", "9ae979fc-107a-42bb-9167-5838cc143aac", "f92ca317-b8bf-4c0d-b273-71978efd65a9", "89e970b2-a1e6-42e1-9052-3f3e9f191d38", "5a88373f-7594-4d54-8723-0b289aadae1e", "dbee968b-8c24-4dbb-9328-b89cc9ba46d6", "d56bbb8c-0194-4dbe-bae4-33558d95dc57", "ab591579-badf-42ad-be19-510df44dc6a4", "29ac1e64-35f1-4c69-a63a-6516ce992690", "75f64e11-631b-468e-9822-0989d5c9c4c5", "752a51c8-af7b-4d36-899f-a86fb53b9158", "f27634f0-b139-425f-8819-6a333b789fd1", "9ad198db-25a9-40e0-a305-0913364c1249", "8c18168d-60e3-4882-b0aa-85c4ecc7728e", "1bca1016-0aa5-4ee2-baa0-9aa22b6d5163", "792ce173-7bf2-4f65-bfb8-73d159878d98", "b0325e13-59cc-4da1-ac86-769fcd2281c6", "e170db10-d971-40ae-a928-50b69e24655c", "a39d37af-c459-43cd-b017-939502f7f398", "c1372150-0ba3-4928-84de-e428a2609731", "806df6a3-c505-45f5-86bd-60333f9cf57e", "285936fd-25dd-494c-b9ee-ed8bf9a98ec0", "1daa56d6-9690-47e3-8e9c-6aa6c1f21d4c", "ac7565be-0c0c-4d04-9342-3b262b91e175", "e1195bcc-cbaf-4627-8892-6f6ee09307c0", "49084dda-ce96-4f45-9014-8719182723e5", "c8abec02-9ded-4377-8c86-3a6a7747c570", "d2ba2529-2602-4843-838f-21477aed9997", "47417187-8f04-4fae-beb3-6ad8b18c9344", "c82d3052-6d33-4a32-8fbf-1fc0bd6ce166", "a2d63ff3-af2b-4bc3-9338-fdd9c5209d41", "91869c96-9f89-44c7-a052-63380c48d539", "25b440f2-b925-42a6-bce5-27beede2d1d5", "816ace9f-2828-449e-a3f3-a6955c8c266b", "66840cf7-148c-45bf-aca9-9a5dbd806432", "2d36b11d-9ee5-4487-bb44-3681b58dc315", "acec621d-59d2-44fb-a813-80db16e8ace8", "dc5c50c7-eda7-4da1-b97e-9e02ccab4aa0", "c38de91a-6c43-452d-826d-867902c36353", "32a357c5-58b1-4a29-8426-03894718d024", "6959b7cd-4fbd-47e2-8de1-f22ccb8251f9", "e15cf3f2-b63a-428a-be0b-8dfb48516116", "8f79ca10-8897-460d-90df-c55c59c37dda", "d587bb29-a55b-437d-a72a-0290fb6ac4e8", "0a7fc208-ce88-4a72-b599-57f8af2280a2", "9d8740bf-f083-424b-9cab-ad8231b15f0c", "d3032879-67a0-4ba1-804d-7fe94803578e", "53dc86a5-ec44-4298-8c23-0d4125cbcd59", "aece8fb3-763e-4fc8-81fd-73234ff42a15", "6cd3b4ca-8188-4fbd-9901-ee552334f981", "fdcde95e-abcb-4a48-add9-0000b9cedc63", "0c5ce62d-9b5f-4375-a25b-31539cbb019e", "fe5dbf49-332b-4c36-8d5c-3f49fcbbeb7e", "ab021809-1cfa-4693-9ae9-3ac5471a6c96", "ac64eb8e-e4e9-43eb-ba76-62082fafd7a1", "189d22b1-81ba-4ceb-808e-17d04d7e637d", "027f24ee-13b3-44a4-a4c5-47f60a894c4d", "f71f39dc-0c17-4c6b-8470-0258a599212a", "5074b1d9-4bf3-4898-8ee6-1a3be739a40d", "8d98c37f-f2a5-43a8-ba95-37df1a851078", "a74876de-3fb5-462e-8ab8-f0cf22b5d425", "e35a3c38-1db8-4c1d-9d8d-fbbf2584602d", "1b86fae1-803b-4264-85eb-db5493403373", "0cd25b7e-25be-4bed-81d7-37dc5c1b8e46", "a4705da8-b72f-4071-845b-bcb6c2e7b989", "3e649744-0fd3-4b8a-b617-724d6eb78955", "0d200043-eec5-47e4-b46c-d547bc14ffa0", "2b839629-7174-4c31-9119-f482194356e7", "b079b4d9-36e0-4a32-8616-7751d682fba4", "0081dec6-f249-43f3-bd14-69d5b5235ec7", "f53aa019-93b4-42bb-9f9d-7621c3d2d638", "b5b6d707-f8d6-437a-b0ee-6e65b22d1d6f", "605a62f0-1470-48db-84d7-09e73acea979", "62d9324a-2a48-413c-ad9e-e7c3df34f682", "6254492b-2c25-4944-bddb-3c428054e4f3", "18b0fbe9-ee13-41a5-acd7-aebedb0b924a", "0fe65464-c60c-4a57-b787-af587b328b86", "2f1d8bb1-5078-4933-b4ff-cf884ce36335", "736f87bb-60b3-4a4e-a220-6bd02b32b2f4", "c5a196dd-5e75-4d95-b426-b725e4b42cd7", "fce034b3-46ea-45aa-b0cf-93c9ae6aad02", "8d7e015a-47bc-43a7-8865-ee6edc7459e8", "131b0a41-092a-4429-a258-50c9c0bf3758", "8dc39263-552f-40cf-a602-34ae1ecd66ea", "ab7e104b-a645-4bfb-83c0-d165301336c5", "7777e25b-0fee-4725-b85d-24b6e116feaa", "6d33ab1e-0ad8-4ea8-8206-804e1cae8056", "a1eebe90-13ca-43aa-abd2-1f7b3cd4e43d", "5f2325c4-636e-4558-8043-5c0f46755246", "b9862b7b-7fd6-4cde-85ff-8816dc093853", "52479432-9e3f-4e5e-88c3-d7c93332aafd", "e85de838-213c-4589-9408-108afb582f89", "b04ca275-a026-4749-911a-ec6466ba92b0", "6e5dd9c1-5d4d-4cd3-b639-5a24f408e42c", "192c098d-4183-405d-99e2-f669b6f15d14", "0c377869-376e-4f2b-a063-033b930e9b75", "9870db52-5644-48fd-85ed-43e64d1f2e95", "72f842bf-576a-454b-b51d-d9be724b0316", "c35ea646-50c9-4f6b-ab17-165a8018f3d9", "9d165fb2-1322-4ee3-a1e3-9d6028a6c6b2", "cf8a4d5e-d2ed-479b-840a-e9d7bf0d3415", "c101d93b-0b90-4c72-9878-fd8a52f005f3", "bd04a337-7655-44f2-be8f-3dc94bffd1da", "d378ed94-168c-4033-aa9a-939898fa877a", "6850df06-973b-427e-9580-85fec0c08c77", "9f871db1-fb35-409c-916f-0d70b15c34b8", "a3c3a124-2959-4465-aed4-6c0f43127a3b", "9308b161-9f6b-46fe-a1ac-7ede7c1579ca", "a316cf69-a59e-406c-b749-aa68e1821d77", "da993a47-0a10-4936-8d7b-030e9db858e9", "8fc02bbd-23b3-4785-99d6-0f838f790c87", "e2fdf670-04ab-41bd-9398-bfd0769b5d14", "b8a00d1f-9eff-40d8-ab37-5d8e8f5549da", "57e1aa90-3eae-4e7a-a625-ab8465ade852", "ad8ba3d7-3a80-4b06-b50a-43b85cfca8af", "d8152773-d4e7-49da-846e-ec08e80d78ef", "fd91dd22-2003-47cb-a6c0-a0c18dac6199", "0a3212bb-80c6-4ff5-a769-3fa5f9478ccb", "080c63f0-e347-416f-a942-d7e8f89d2936", "0055bdba-e0ea-458d-b9f3-38c0d3323a05", "e6a1e9fc-51a8-4b57-9b9c-283dd919b0cd", "c3858157-7440-45af-9a1b-007044eec5ce", "1b78e919-db7d-49a2-9d5c-7683134e1068", "5f633749-b464-4508-82d3-ec7bf7875ff5", "5f4eb45d-7dc8-4b95-a9ae-41c896bd56ec", "3d94159e-1a55-417d-a2e2-5d0c0f07fbce", "4b88f44a-3e8b-41eb-9ec1-60cc555c7427", "f8347344-7450-4a17-841e-b421ba968d02", "eb035e3f-9fa4-4b81-b289-7d0c7f6156df", "1ed9c19a-011e-4944-b76a-af2d97537b5d", "659bed95-39d6-40c6-8d6f-0814055a7aa5", "8b33fe4d-e031-4aec-b819-cd8cef328fc3", "1c20c283-ddec-46f5-a14c-c580796b950c", "7131c697-3542-4fa6-bf2a-a525e2158769", "cf3553c2-6280-4c33-aab8-9cba666c8a0d", "cf17e4bc-9056-428f-8799-d509d863a5e3", "836a49a1-2430-4db6-aa81-5586cc5ad898", "7b26f732-ee4e-4960-b538-3876b5175ba0", "7fe734c2-17ac-4e8a-aa90-986481b27ae5", "04dfd9fe-6fe0-4a41-ae5b-a993ac8d166f", "174d4e98-8027-4ff3-bd3b-487e1259f402", "f26ac318-d67a-4bc4-a247-6d8101dea8d0", "c42344e0-8c9b-4f9b-a511-f69aa4fd3fc8", "379384dd-d943-4316-90a7-b6c326592a0e", "2dd84bf4-e8e8-4a22-8576-dcba0b015eef", "f7ebcbcb-b285-4385-b848-30a15d11cedb", "7d0d3e00-a795-49bf-accf-867e946ca2ea", "3220e706-be91-46e7-9ad2-62f8347fcff3", "0c7f434c-bc43-492d-9bd9-bb6b3e6f1140", "209f8cd2-3d2c-455e-aff7-398206fe0af4", "9d403329-e1e5-436b-a8da-2c869d37d9d3", "536ec7fa-6399-486b-a969-d5682a51d1da", "bfe660ec-4e3f-464f-abf9-52c3b4d45a4e", "34bbb280-4777-4523-940a-957b29505fbc", "d494b51b-f065-4a27-a49b-1bd145a9b9fb", "c698be59-2870-414d-b543-4c7ef4a4aca8", "30802c63-3d56-4648-a9f8-2f43162e1e77", "6c1b3c3d-2c24-4f0e-aa83-a77b273a4df5", "68513895-8058-4994-912e-4ff192c37a66", "b55b3907-4bed-4173-836b-7cc10bfbdeb3", "a3e5180f-8b19-4255-adf1-b1b7608d0da1", "414d3841-f7d5-4245-9d84-0d28bec2c214", "93d17f47-b31d-4ceb-b993-fc45d4ebef93", "4b6518cd-0e53-4ff8-aea7-9fabfddd93be", "54fdadc1-6569-421b-9fdc-4828179987e7", "adcdae26-5520-4326-9a46-5b9e173a97db", "963b129b-5793-40d7-8a6a-6f1ab7f51de4", "12862353-18db-4c6a-9ecc-eb8ae2d74dee", "1f513a3d-338c-4677-91fe-8b932d296745", "2650d1bf-380e-4db7-96bf-30a4a4f8f749", "4bae7834-bdf9-4547-9246-6858bd0d2511", "beacb639-6fa0-420c-a15b-2bf5f9797def", "a8e5ca1a-76ab-40bf-b27d-514ca03b30b3", "8a40a62d-fdfd-4aa4-adb6-cc13a1669afd", "8d3912a6-1214-4a18-b74a-266f60e00c59", "68e2e68b-3dab-42b5-bd8b-e284ebaada26", "8bfda55d-e806-4919-95bc-bd7db323597b", "a6ca2304-8539-476f-9881-6aa9b88f47e6", "f4a69d63-70df-4ec6-949a-1caa11d0fba7", "75d93b56-bd8c-4edf-b9fd-72b621e6be1e", "8facfbc9-a3d9-49ad-8125-6fa7b4850d13", "69940f2a-c138-4b92-8e4a-0d0a79873c94", "5455a826-68c3-4381-ace5-ff5ff6d91551", "51d53027-0cd9-4540-a68b-dd48152cf47f", "4d198dbd-653a-4d79-9114-e3227cd189a8", "fc6cb691-4a50-4238-93f9-aff8f376e093", "bf72cdf9-c1ea-4a82-bccb-eb4fe6fe3a94", "4d8d7c5e-013b-43b0-865b-184ad235a289", "f4c69cf3-a804-4894-a451-a1d60f19afc7", "c87dd377-031c-42a0-8d48-cffa4e53ec87", "81beb354-7c44-4725-8df1-412198d95c1a", "57bd4f3f-6e76-4c9f-82cd-e7e331c6a4ff", "130aa38e-1632-40ce-af3c-6ac3747d5427", "a39145fb-bef7-4313-9d92-5fe305c6ea71", "06201481-187f-4efe-a93d-73726b3d7320", "be556533-e83e-42c6-bdda-c3b3c8a2a938", "07551196-8f79-41f3-8832-cb5cd682f1d6", "313a755d-0a41-40c7-87ee-4076e4016ce3", "f0b50cf0-cafe-4f10-8368-54c8d83cd893", "dfcb3c74-f321-4320-a674-c6e0b5a2b6e6", "30596029-e1f0-4743-bf1b-2149adb179ad", "b2b34315-afb7-4d4d-83a9-020d962ead26", "1dcc1967-78bd-4213-9fda-2f930633742c", "7e280d40-975b-4328-881e-7161ae5de011", "f29d3426-e28a-4838-a6ab-6c2802c1e0ac", "0a2e85cb-59a5-4cfe-97b8-f81bead9645b", "f9e7244a-25e6-4df7-b8f1-f44d53217a3e", "73a7f987-771f-4a99-9025-f18204f71cc6", "9766d90b-0f4a-42c8-95b8-90e6b5b4cf88", "9f769458-ff9f-4215-b3e4-77c0a13bf4b9", "05e71140-a1bf-4f9c-ab3b-579ffbde6cac"], "metadata": {"window": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem.  Our framework adopts a two-stage pipeline.\n In the first stage, we pretrain a restoration module across diversified degradations\nto improve generalization capability in real-world scenarios. ", "original_text": "DiffBIR: Towards Blind Image Restoration with\nGenerative Diffusion Prior\nXinqi Lin1,\u2217Jingwen He2,\u2217Ziyan Chen2Zhaoyang Lyu2Ben Fei2Bo Dai2\nWanli Ouyang2Yu Qiao2Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2Shanghai AI Laboratory\nAbstract\nWe present DiffBIR, which leverages pretrained text-to-image diffusion models\nfor blind image restoration problem. "}}}}